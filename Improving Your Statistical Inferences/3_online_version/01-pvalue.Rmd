---
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
source("D:/New_translate/statistical_inferences-master/statistical_inferences-master/include/globals.R")

# needed to make the chapter (not visible)
library(kableExtra)
library(ggplot2)

```

```{r, include = FALSE}
# Add Type 1 error rate function
add_type1_error <- function(N, 
                            side = "right", 
                            col = rgb(1, 0, 0, 0.5)) {
  mult <- ifelse(side == "right", 1, -1)
  crit_d <- mult * abs(qt(0.05 / 2, (N * 2) - 2)) / sqrt(N / 2)
  
  if (side == "right") {
    y <- seq(crit_d, 10, length = 10000)
  } else {
    y <- seq(-10, crit_d, length = 10000)
  }
  
  # determine upperbounds polygon
  suppressWarnings({
    z <- (dt(y * sqrt(N / 2), df = (N * 2) - 2) * sqrt(N / 2))
  })
  
  if (side == "right") {
    polygon(c(crit_d, y, 10), c(0, z, 0), col = col)
  } else {
    polygon(c(y, crit_d, crit_d), c(z, 0, 0), col = col)
  }
}

# calculate distribution of d based on t-distribution
calc_d_dist <- function(x, N, ncp = 0) {
  suppressWarnings({
    # generates a lot of warnings sometimes
    dt(x * sqrt(N / 2), df = (N * 2) - 2, ncp = ncp) * sqrt(N / 2) 
  })
}
```

\mainmatter

#1使用 *p*值进行假设检验 {#pvalue}

研究者们可以通过收集数据来回答各种研究问题。他们所关注的问题之一是在不同条件下收集到的测量指标是否有所差异。*差异性主张（ordinal claim）*可以为这个问题提供答案,即研究者可以在比较不同条件时提出测量指标的平均值更大、更小或没有差异。研究者可能对这样的假设感兴趣："假设条件A为：学生在学习后接受测试（需要回忆学过的知识），条件B为：学生在学习后不接受测试（将全部时间用于学习）。在收集数据并得出接受测试的学生的平均成绩较高后，研究者可以提出差异性主张，即与条件B相比，条件A的学生成绩*更好*。差异性主张仅能说明条件之间存在一定差异，但不能量化**效应大小**。

为了得出差异性主张，研究者通常采用著名的**假设检验**的方法。假设检验的一个部分为计算***p*****值**，并检验是否存在统计学上的**显著**差异。"显著"意味着某些东西是值得关注的。假设检验用于区分实验数据中的信号（值得关注的）和随机噪声。我们需要区分的是**统计学显著**以及**实际效应显著**，前者仅说明观察到的效应是信号还是噪声，而后者关注效应是否足够大，以便在现实生活中做出有价值的推断。为防止[确认偏差](https://www.youtube.com/watch?v=G1juPBoxBdc)，研究者们会采用这种方法论的程序来决定是否做出差异性主张。William Gosset（或称"Student"，他发明了t检验）在吉尼斯啤酒厂的内部报告中曾应用过上述统计检验方法，他在1904年写道[-@gosset_application_1904]：

另一方面，人们普遍认为让研究者完全拥有拒绝实验结果的自由是危险的，因为研究者可能是带有偏见的。因此，人们提出采用一种评判标准，该标准取决于在给定数量的观测数据中，出现较大误差的概率。

根据自身的期望，研究者们可能倾向于将数据解释为对其假设的支撑，即便数据的结果并不是这样。正如Benjamini（2016）[-@benjamini_its_2016]所提出的，在区分信号和噪声时，*p*值是作为防止被随机因素愚弄的第一道防线。有迹象表明，禁止使用*p*值将会使得研究者更容易做出错误论断。在《基础与应用社会心理学》期刊施行禁止零假设显著性检验之后，人们对后续发表的研究进行了定性分析，@fricker_assessing_2019&nbsp;Fricker（2019）等人认为，相较于使用p\<0.05为阈限的假设检验，当研究者仅用描述性统计时，我们发现他们很可能会过度解释和/或夸大他们的结果。研究者提出差异性主张时，如果假设检验方法使用得当，是可以有效控制研究者自欺欺人的。

##1.1关于*p*值的一些迷思 

在我们厘清*p*值是如何计算之前，更重要的是需要弄清在进行假设检验时，它如何帮助我们得出差异性主张。*p*值的定义是当零假设为真时，观察到当前样本数据或更极端数据的概率。但是这个定义并未告诉我们该如何解释*p*值。

对*p*值的解释方式取决于个人信奉的统计哲学。从[Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher)1925年发表了《研究者统计方法论》一书之后，*p*值的应用开始广泛起来。在Fisherian的理论框架中，*p*值作为一个连续测量，用于描述观察到的数据以及与零假设之间的兼容性[@greenland_statistical_2016]。兼容性的区间在1（完全兼容）到0（完全不兼容）之间，且每个人都可以经过在统计上的"深思熟虑"来解释*p*值。依据Fisher[-@fisher_statistical_1956]所述，*p*值并不意味着真实世界的某个概率，而是以一种合理且完备的形式，来衡量接受期望假设的受阻程度。Fisher试图将他的理念标化为一种"基础推论"，但这并没有得到其他流派的广泛采纳，如决策理论、似然比检验和贝叶斯推理。事实上，Zabell[-@zabell_r_1992]写道："基础推论"是Fisher的一个巨大失败"，尽管其他人表示希望它在未来会发展成一个有用的方法[@schweder_confidence_2016]。Fisherian的p值描述了数据与单一假设的不兼容性，因此又被称为显著性检验。显著性检验受到限制的主要原因是，研究者只指定了一个虚无假设（$H_0$），而没有指定备择假设（$H_1$）。

Neyman和Pearson[-@neyman_on_1933]在William Gosset和Ronald Fisher关于*P值*的见解的基础上，发展了一种叫做*统计假设检验*的方法。与Fisher提出的显著性检验相比，其主要区别是，在统计假设检验中，既指定了一个虚无假设，也规定了一个备择假设。在Neyman-Pearson的理论框架中，统计检验的目标是引导研究者在这两个假设方向上的决策。在不知道假设是否为真的情况下，研究者会根据统计检验的结果，姑且在零假设或备则假设为真的前提下行动。在心理学领域，研究者经常使用Fisherian和Neyman-Pearson二者理论框架的不完美结合版，但根据Dienes[-@dienes_understanding_2008]的说法，Neyman-Pearson的理论方法是"你在心理学学术期刊中看到的所有数据统计的基础逻辑"。

当进行Neyman-Pearson假设检验时，观察到的*p值*只是用于对比是否小于所选的$\alpha$水平，但它小多少并不重要。例如，如果使用0.01的$\alpha$水平，*p*=0.006和*p*=0.000001都会使研究者认为，现实世界就是备择假设所描述的那样。这与Fisherian的*p*值方法不同，Fisherian式的*p*值认为，*p*值越低，研究者在心理上更倾向于拒绝他们所检验的虚无假设。Neyman-Pearson式的假设检验不认为推论的目标是量化兼容性或依据。相反，正如Neyman[-@neyman_inductive_1957]所写的：

> 当我们对行为进行归纳时，应当认识到，每个严肃研究的目的，都是在几个预期的行为中为某一个行为提供依据。

简洁了当点说，人们可能会觉得不应该仅凭一次单一测试的统计结果做出决策行为，这一点经常被提出来作为对Neyman-Pearson式统计推断方法的批评。然而，这种批评中的"行为"与Neyman所说的"行为"，内涵并不一致。诚然，实施一项新政策的决策不应基于单一的研究结果。然而，Neyman认为提出科学主张也是一种"行为"，他曾写道（1957年，第10页），一项研究的结论阶段包括：

> 一种行为意愿或采取某一行动的决策，也许就是对各种现存假设采取特定的态度。

Cox [-@cox_problems_1958] 写道:

> 可以这么说：当进行推断时，我们就是"决定"对总体进行某种类型的陈述，因此，只要不对决定一词进行过于狭义的解释，那么研究的统计决策就包含了推论的过程。其实重点是，统计推断的主要问题在于决定哪些类型的陈述是有意义的，以及它们到底意味着什么。

因此，在Neyman-Pearson的方法中，*p*值是进行声明的基础。在科研中，类似的声明通常是以**辅助假设**的形式来作为大多数新兴实验的基础，或者说，为了使实验按计划进行，必须准确提出一些基础假设[@hempel_philosophy_1966]。例如，在实验计划中，被试能否看到颜色对该实验很重要，我们就假设[Ishihara test](https://en.wikipedia.org/wiki/Ishihara_test)（一种色盲测试）能够成功识别哪些被试是色盲。

##1.2建立一个零模型 

假设一共有两组人，每组10人，我分别问两组人对《指环王》三部曲加长版的喜爱程度。这意味着我们的**总样本量**(*N*)是20，每组的样本量(*n*)为10。第一组的被试都是我的朋友，而第二组的被试是我妻子的朋友。这些朋友对问题进行了1\~10的评分。通过计算朋友们评分的均值，可知我的朋友那组均分是8.7，我妻子朋友那组均值是7.7。我们可以通过查阅原始数据以及绘制相关图表对两组数据进行比较。

```{r friends, echo=F, tab.cap = "Ratings for the Lord of the Rings extended trilogy by two groups of friends."}

friends <- paste0("friend_", 1:10)
friends_daniel <- c(9, 7, 8, 9, 8, 9, 9, 10, 9, 9)
friends_kyra <- c(9, 6, 7, 8, 7, 9, 8, 8, 8, 7)
df <- data.frame(friends, friends_daniel, friends_kyra)
colnames(df) <- c("", "Daniel的朋友", "Kyra的朋友")

kable_styling(kable(df, caption = '两组朋友对指环王加长版的评分', align = c("l", "c", "c"), booktabs = TRUE), bootstrap_options = "striped", full_width = FALSE, position = "left")
```

```{r, echo = FALSE}
# Get data frame in long format for plotting
df_long <- tidyr::gather(df, "Friend Group", rating, "Daniel的朋友":"Kyra的朋友", factor_key = TRUE)

# Plot the data
ggplot(df_long, aes(x = rating, fill = `Friend Group`)) +
  geom_density(alpha = .3) +
  theme_bw() + 
  theme(plot.background = element_rect(fill = backgroundcolor))  + 
  theme(panel.background = element_rect(fill = backgroundcolor))
```

我们可以看到两组人的数据是重叠的，但评分均值却相差1分。所以，我们现在面临的问题如下：
两组之间的差异只是随机误差，还是说我的朋友比我妻子的朋友更喜欢《指环王》三部曲的加长版？

在**零假设的显著性检验**中，我们试图通过计算当前观察到的差异（这时的均值相差1）或更极端差异出现的概率来回答这个问题，并且基于的假设是：我的朋友和我妻子的朋友对《指环王》加长版的喜好程度并没有真正的差异，我们看到的只是随机噪声。这个概率称为*p*值。如果这个概率足够低，那我们就可以说存在差异，如果这个概率不够低，那我们就能够避免做出存在差异的声明。

零假设假定如果分别询问了我无限多的朋友，和我妻子无限多的朋友，这两个海量的群组对LOTR喜爱程度之间的差异为0。然而，从总体中抽取任一样本，通常会出现随机误差，导致两组之间的差异不为0。我们可以建立一个**零模型**，用于衡量出于随机噪声得到的观测数据之间的期望误差，这样我们就可以对，总体之间没有实质差异而产生的组间差异有一个合理的预期。

使用**标准化**分布来建立一个零模型是很便捷的，因为不用考虑测量时数据的单位不同。这样更容易计算某一值的出现概率。有一个衡量差异的零模型是*t*分布，它可以用于描绘从总体抽样时，多大的差异是在期望之中的。这种零模型是对数据存在某种**假设**的。在*t*分布的情况下，它假设分数是正态分布的。然而事实上，这些统计方法下的暗含的数据假设从来不完美匹配现实世界，所以统计学家们会检验违反假设对方法论的影响。当违反假设对统计推断的影响足够小时，统计检验的方法仍然是有用的。

我们可以通过*概率密度函数*来衡量当总体中不存在差异时的预期*t*值分布。下面是一个*自由度*（df）为18的*t*分布的概率密度函数图，它与我们的例子一致，即对应我们从20个朋友那里收集的数据（df=N-2，两个独立组）。对于连续分布来说，概率是由无限多的点定义的，任何一个点的概率（例如，*t*=2.5）总是零。概率是按区间测量的。因此，计算的*p*值并不是所得观测数据（即某一点）的概率，而是所得观测数据及*更极端数据*（即一个区间）的概率。这就形成了一个可以计算面积的概率区间。

##1.3计算*p*值

可以通过样本均值、总体均值、样本的标准差和样本大小来计算*t*值。然后通过计算当前数据及更极端数据所对应*t*值的概率，我们就得到了*p*值。将上述两组朋友的电影评分进行对比，依据双侧*t*检验可得到的*t*值为2.5175，*p*值为0.02151。

```{r}
t.test(df_long$rating ~ df_long$`Friend Group`, var.equal = TRUE)
```

我们可以绘制出*t*分布图（df=18），并高亮了*t*值为2.5175和-2.5175的两个尾部区域。

自由度为18的*t*分布

```{r, tdist, echo = FALSE, out.width='100%', fig.cap = '(ref:tdistlab)'}
par(bg = backgroundcolor)
x <- seq(-5, 5, length = 100)
plot(x, dt(x, df = 18), col = "black", type = "l", xlab = "t值", ylab = "概率密度", main = "t值分布", lwd = 2)
x <- seq(2.5175, 5, length = 100)
z <- (dt(x, df = 18))
polygon(c(2.5175, x, 8), c(0, z, 0), col = rgb(1, 0, 0, 0.5))
x <- seq(-5, -2.5175, length = 100)
z <- (dt(x, df = 18))
polygon(c(-8, x, -2.5175), c(0, z, 0), col = rgb(1, 0, 0, 0.5))
```

##1.4哪个是你想要的p值? 

有一个非常有教学意义的视频叫"[*p*值之舞](https://www.youtube.com/watch?v=5OL1RqHrZQ8)"，Geoff Cumming觉得*p*值因实验的不同而有所差异。然而，这并不是他在视频中提到的"不信任p值"的理由。相反，重要的是要清楚地了解***p*****值的分布**的，以防止误用。因为*p*值是统计中频率学派的部分，我们需要检验的是*从长远来看*能预期得到什么。因为我们无法多次地重复一个相同的实验，在一生中我们只能做有限数量的实验，所以最好的办法就是通过计算机模拟来了解从整体角度来看，我们应该期待什么结果。

请大家花一些时间思考一下两个问题。如果效应真实存在，并且你重复这个实验成千上万次了，你期待得到一个什么样的*p*值？同样，如果效应并不存在，你也同样重复这个实验成千上万次了，你又会得到什么样的*p*值？如果你不知道答案也没有关系，你现在就能学会。但是如果你不知道答案，或许应该值得反思一下，为什么你不了解这个*p*值的重要知识。也许你和我一样，根本没有被教过这方面的内容。但接下来你就会明白，这对巩固理解*p*值该如何解释至关重要。

你能期望的*p*值完全由研究的统计功效决定，或者说由得到显著效应的概率决定（如果效应真实存在）。统计功效的范围在0\~1。我们可以通过模拟独立样本*t*检验来说明这一点。比如说，我们来模拟一群人的智商分数。我们知道智商的标准差是15。现在，我们假设某个样本组的平均智商为100，而另一个组为105。随后，我们来检验这两组人的IQ是否有差异（我们知道正确答案是"是"，因为我们在模拟中就是这样设定的）。

```{r, eval = FALSE, cache = TRUE}
p <- numeric(100000) # store all simulated *p*-values

for (i in 1:100000) { # for each simulated experiment
  x <- rnorm(n = 71, mean = 100, sd = 15) # Simulate data
  y <- rnorm(n = 71, mean = 105, sd = 15) # Simulate data
  p[i] <- t.test(x, y)$p.value # store the *p*-value
}

(sum(p < 0.05) / 100000) # compute power

hist(p, breaks = 20) # plot a histogram
```

在模拟时，我们假设从两组IQ分数分别服从均值(M)等于100或105，标准差等于15的正态分布，并分别从分布中抽取71个样本。然后我们对样本均值进行独立样本*t*检验，求得并储存*p*值，并生成*p*值分布图。

(ref:pdist1lab)统计检验力为50%时的*p*分布

```{r, pdistr1, cache = TRUE, echo=FALSE, fig.cap = '(ref:pdist1lab)'}

#Set number of simulations
nSims <- 100000 # number of simulated experiments
p <- numeric(nSims) # set up empty variable to store all simulated *p*-values
bars <- 20

for (i in 1:nSims) { # for each simulated experiment
  x <- rnorm(n = 71, mean = 100, sd = 15) # Simulate data
  y <- rnorm(n = 71, mean = 105, sd = 15) # Simulate data
  p[i] <- t.test(x, y)$p.value # store the *p*-value
}
  
#Plot figure
par(bg = backgroundcolor)
op <- par(mar = c(5,7,4,4)) #change white-space around graph
hist(p, breaks=bars, xlab="P值", ylab="p值的数量", axes=FALSE,
     main=paste("50%功效下的p值分布"),
     col="grey", xlim=c(0,1), ylim=c(0, nSims))
axis(side=1, at=seq(0,1, 0.1), labels=seq(0,1,0.1))
axis(side=2, at=seq(0,nSims, nSims/4), labels=seq(0,nSims, nSims/4), las=2)
abline(h=nSims/bars, col = "red", lty=3)

```

我们可以看到x轴是20个条形柱分别代表着从0到1的*p*值，Y轴则是这些*p*值出现的频率。水平的红色虚线表示$\alpha$为5%（位于100.000\*0.05=5000的频率），但你现在可以忽略这条线。图表的标题给出了模拟研究的统计功效（假设$\alpha$为0.05时）：
这项研究有50%的功效。

模拟的结果描绘了*p*值的*概率密度函数*。概率密度函数给出了某个随机变量出现特定值的概率（比如在*t*分布中的图1.2)。因为*p*值是一个随机变量，我们可以用它的概率密度函数来绘制*p*值分布图[@hung_behavior_1997;
 
@ulrich_properties_2018]，如图1.3）。在[线上的Shiny应用程序](http://shiny.ieis.tue.nl/d_p_power/)中，你可以改变样本大小、效应量大小和$\alpha$水平，以便观察他们对*p*值分布的影响。增加样本量或效应量将增加*p*值分布的陡峭程度，这意味着观察到小*p*值的概率增加。*p*值分布是统计功效的函数（意思是说统计功效会影响p值分布）。

一个双侧*t*检验对应的*p*值概率密度函数

```{r, pdft, echo = FALSE, fig.cap= '(ref:pdftlab)'}

n <- 71 # sample size per independent group
d <- 0.33 # effect size
se <- sqrt(2 / n) # standard error
ncp <- (d * sqrt(n / 2)) # non-centrality parameter d

pdf2_t <- function(p) { #probability density function
  0.5 * dt(qt(p / 2, 2 * n - 2, 0), 2 * n - 2, ncp) / 
      dt(qt(p / 2, 2 * n - 2, 0), 2 * n - 2, 0) +
    dt(qt(1 - p / 2, 2 * n - 2, 0), 2 * n - 2, ncp) / 
      dt(qt(1 - p / 2, 2 * n - 2, 0), 2 * n - 2, 0)
}

par(bg = backgroundcolor)
plot(-10, xlab = "p值", ylab = "概率密度", axes = FALSE,
     main = "p值分布", xlim = c(0, 1), ylim = c(0, 50), 
     cex.lab = 1.5, cex.main = 1.5, cex.sub = 1
)
curve(pdf2_t, 0, 1, n = 1000, col = "black", lty = 1, lwd = 3, add = TRUE)
axis(side = 1, at = seq(0, 1, 0.05), 
     labels = formatC(seq(0, 1, 0.05), format = "f", digits = 2), cex.axis = 1)
```

当效应不存在时，*p*值是**均匀分布**的。这意味着，当零假设为真时，每一个*p*值都有同样的可能性被观察到。换句话说，当不存在真正的效应时，0.08的*p*值和0.98的*p*值含义一样。我记得当我第一次了解到均匀的*p*值分布时，我觉得这非常反常（在我完成了我的博士项目之后）。但是如果保证零假设为真，*p*值永远大于我们所设定的$\alpha$水平时，那么*p*值为均匀分布似乎又是合理的。如果我们把$\alpha$水平设为0.01，观察到的1%的*p*值小于0.01，如果我们把$\alpha$水平设置为0.12，观察到的12%的*p*值小于0.12。但是，这只有在零假设（$H_0$）为真时，*p*值是均匀分布的情况下才会发生这种情况。

 零假设为真时的*p*值分布

```{r pdistr2, cache = TRUE, echo=FALSE, fig.cap = '(ref:pdist2lab)'}

#Set number of simulations
nSims <- 100000 #number of simulated experiments
p <-numeric(nSims) #set up empty variable to store all simulated *p*-values

for (i in 1:nSims) { # for each simulated experiment
  x <- rnorm(n = 71, mean = 100, sd = 15) # Simulate data
  y <- rnorm(n = 71, mean = 100, sd = 15) # Simulate data
  p[i] <- t.test(x, y)$p.value # store the *p*-value
}
  
bars<-20
#Plot figure
op <- par(mar = c(5,7,4,4)) #change white-space around graph
par(bg = backgroundcolor)
hist(p, breaks=bars, xlab="p值", ylab="p值的数量", axes=FALSE,
     main=paste("当零假设为真时的p值分布"),
     col="grey", xlim=c(0,1), ylim=c(0, nSims))
axis(side=1, at=seq(0,1, 0.1), labels=seq(0,1,0.1))
axis(side=2, at=seq(0,nSims, nSims/4), labels=seq(0,nSims, nSims/4), las=2)
abline(h=nSims/bars, col = "red", lty=3)

```

##1.5林德利悖论 Lindley's paradox 

随着统计功效的提高，一些稍微低于0.05的*p*值（如*p*=0.04），相比于*有*效应，它更可能在*没有*效应的情况下出现。这就是著名的林德利悖论[@lindley_statistical_1957]，或称杰弗里-林德利悖论 [@spanos_who_2013]。因为*p*值的分布是统计功效的函数 [@cumming_replication_2008]，功效越高，*p*值的分布就越右偏（即越有可能观察到小的*p*值）。当效应不存在时，*p*值是均匀分布的，1%的概率可以观察到*p*值在0.04和0.05之间。当统计功效极高时，不仅大多数*p*值会低于0.05，甚至会低于0.01。通过图1.5我们可以看到，在高功效下，当效应真实存在时，非常小的*p*值（例如0.001）就比无效应时更容易被观察到（例如，当*p*值为0.01时，黑色虚线代表统计功效为99%时的*p*值分布，灰色水平线代表零假设为真时*p*值的均匀分布，黑色虚线落在灰色水平线之上）。

然而惊讶的是，相比于备择假设为真且存在非常高的统计功效时，在虚无假设为真的情况下我们更可能观察到*p*值为0.04，正如图1.5中所示，当*p*等于0.04时，零假设为真时，*p*值分布的密度要比统计功效为99%时要高。所以，林德利悖论说明，0.04的*p*值可能在具有统计显著性，但同时也为零假设提供了一定证据。从Neyman-Pearson的方法来看，我们的主张最大错误率为5%，但从似然比或贝叶斯的方法来看，我们可以推断：相对于备择假设，我们的数据信息提供了有利于零假设的证据。林德利悖论说明了不同的统计学派何时会得出不同的结论，以及在不考虑检验统计功效的情况下，为什么*p*值不能被视为直接证据。虽然这并不是必要的，但研究者可能希望避免以下情况：那就是当实验数据倾向于零假设而不是备择假设，频率学派的学者们却因为p\<0.05拒绝零假设。这可以通过降低$\alpha$水平来实现，将$\alpha$水平作为样本量的函数[@leamer_specification_1978；@maier_justify_2022；@good_bayesnon-bayes_1992]，这一点将在"[误差的控制](#errorcontrol)"一章节进行解释。

*p*值分布（灰色水平线为0，50%统计功效（黑色实线），99%统计功效（黑色虚线））相比于$H_1$为真，当$H_0$为真时更可能出现*p*略微小于0.05的值

```{r, paradox, echo=FALSE, fig.cap= '(ref:paradoxlab)'}
# Lindley plot

n <- 150
p <- 0.05
ymax <- 25 # Maximum value y-scale (only for p-curve)

# Calculations

# p-value function
pdf2_t <- function(p) 0.5 * dt(qt(p / 2, 2 * n - 2, 0), 2 * n - 2, ncp) / dt(qt(p / 2, 2 * n - 2, 0), 2 * n - 2, 0) + dt(qt(1 - p / 2, 2 * n - 2, 0), 2 * n - 2, ncp) / dt(qt(1 - p / 2, 2 * n - 2, 0), 2 * n - 2, 0)

par(bg = backgroundcolor)
plot(-10,
  xlab = "p值", ylab = "概率密度", axes = FALSE,
  main = "d = 0, 50%和99%统计功效下的p值分布", xlim = c(0, 1), ylim = c(0, ymax), cex.lab = 1.2, cex.main = 1.2, cex.sub = 1
)
axis(side = 1, at = seq(0, 1, 0.05), labels = formatC(seq(0, 1, 0.05), format = "f", digits = 2), cex.axis = 1)
# Draw null line
ncp <- (0 * sqrt(n / 2)) # Calculate non-centrality parameter d
curve(pdf2_t, 0, 1, n = 1000, col = "grey", lty = 1, lwd = 2, add = TRUE)
# Draw 50% low power line
n <- 146
d <- 0.23
se <- sqrt(2 / n) # standard error
ncp <- (d * sqrt(n / 2)) # Calculate non-centrality parameter d
curve(pdf2_t, 0, 1, n = 1000, col = "black", lwd = 3, add = TRUE)
# Draw 99% power line
n <- 150
d <- 0.5
se <- sqrt(2 / n) # standard error
ncp <- (d * sqrt(n / 2)) # Calculate non-centrality parameter d
curve(pdf2_t, 0, 1, n = 1000, col = "black", lwd = 3, lty = 3, add = TRUE)


```

##1.6正确地解释和报告*p*值

虽然从严格的Neyman-Pearson方法的角度来看，报告*p*\<$\alpha$或*p*\>$\alpha$就足够了，但研究者还是应当报告确切的*p*值。这便于后续再次分析结果时使用[@appelbaum_journal_2018]，也便于并其他研究者将*p*值与他们希望使用的$\alpha$水平进行比较[@lehmann_testing_2005]。因为是在允许最大误差率的情况下做出的陈述，*p*值永远不允许你完全肯定所得到的陈述。即便我们把$\alpha$水平设定为0.000001，结论也有可能是错误的，Fisher[-@fisher_design_1935]提醒我们，即使是"百万分之一的几率"仍旧会发生，既不低于也不高于其某个恰当的概率，但只要发生在我们身上就是0和1的概率，纵使我们会非常惊讶为什么如此小的概率仍会发生在我们身上"。这就是*重复研究*在科学领域很重要的原因。任何单一的发现都可能是一种侥幸，但如果几个重复的研究都能得到相同的结果，那么这种概率很快就会变得非常小。这种不确定性有时并没有反映在学术写作中，因为可以看到研究者会使用"证明"、"显示"或"已知"等字眼。在做完假设检验之后，有个稍长但更准确的说法是：

> 我们宣称了一个有/没有意义的效应，并且我们同时承认，如果学者们在这套方法论下提出主张，从长远来看，他们最多只有$\alpha$%或$\beta$%的几率被误导，我们认为这是可以接受的。在可预见的未来，在出现新的数据或信息证明我们是错误的之前，我们将假定这一说法是正确的。

请记住，在Neyman-Pearson的理论框架下，研究者提出某种说法，但是这些说法不一定是真实的。例如，OPERA合作组织在2011年报告说，他们所得到的观测数据似乎揭示了中微子的速度超过了光速。这一说法的Ⅰ类错误率是百万分之0.2，*假定误差纯粹是由随机噪声造成的*。然而，实际上并没有研究者相信这一说法是真的，因为理论上中微子的运动速度不可能超过光速。事实上，后来发现是由于设备故障所导致的数据异常：一根光纤的连接不当，还有一个时钟振荡器的走的过快。然而，这一主张在提出的同时也公开邀请学界提供新的数据或信息，来证明这一说法有误。

当学者们在Neyman-Pearson统计推断方法下"接受"或"拒绝"一个假设时，他们并没有表明任何关于假设的信念或结论。相反，他们是基于预设规则（即观察到的数据反映了世界的某种状态）做出了一种波普尔式**基本声明(basic statement)**。这些基本声明描绘的是已经进行的观察（例如，"我观察到了一只黑天鹅"）或已经发生的事件（例如，"接受间隔练习训练的学生，比不接受训练的学生在考试中表现得更好"）。

这些主张与我们已经观测到的数据有关，但与我们用于预测的理论无关。某个主张与观测数据有关，因为它是一种统计推断，但它与理论无关，因为理论需要理论推断。数据永远无法"证明"一个理论是对还是错。一个基本声明可以**证实**一个从理论推导出来的预测，也可能无法证实。如果从一个理论中推导出的许多预测都得到了证实，我们就会越来越相信这个理论是接近真理的。这种理论的"接近真理性"叫做**逼真性(verisimilitude)** [@niiniluoto_verisimilitude_1998;
 
@popper_logic_2002]。报告假设检验时会使用的更短的陈述是"p=.xx，即我们的预测在y%的$\alpha$水平下得到证实，或者"p=.xx，在y%统计功效以及对应的效应量下，预测没有被证实"。$\alpha$水平或者统计检验力往往只在文章的实验设计部分被提及，但是在结果部分重复他们能够提醒读者注意与你的主张有关的错误率。

甚至当我们做出正确的陈述时，它的底层理论也可能是错误的。Popper [-@popper_logic_2002] 提醒我们"客观科学的实证基础并不是绝对的"。他争辩科学并不建立在坚固的基石之上，而是在沼泽中打下的木桩上，并指出"所以就目前而言，当确信桩的牢固程度足以承载结构后，我们就会停下来"。如Hacking [-@hacking_logic_1965] 写道："拒绝不是驳倒。多数拒绝必须只是试探性的"。所以当拒绝零模型时，我们是试探性的，并且知道有犯错的可能性，而不是一定要相信零模型是错的，以及我们用来进行预测的理论是对的。对Neyman [-@neyman_inductive_1957] 而言，推断行为是一种"根据实验结果的情况，在将来以特定方式行动（直到被新的实验所迭代）的意愿。所有的科学知识都是暂时的。

一些统计学家建议把p值解释成对证据的度量。例如，Bland [-@bland_introduction_2015] 认为*p*值可以被解释成一种对证据强度的"粗略且方便"的指南，*p*\>0.1表示"很少或没有证据"，0.01\<*p*\<0.05表示"有证据"，*p*\<0.001表示"证据很强"。从前面关于林德利悖论和*p*值均匀分布的讨论中可以看出这是不正确的 [@johansson_hail_2011;
 
@lakens_why_2022]。如果你想量化证据，请参阅[似然性](#likelihoods)或[贝叶斯统计](#bayes)章节。

##1.7避免*p*值的常见误解 

*p*值是在假设零假设为真的情况下，观察到观测数据或者更极端数据的概率。为了理解*p*值能说明什么，我们尤其要注意*p*值不能说明什么。首先，我们需要知道"零假设为真"是什么，以及当零假设为真时，数据的分布形态是怎样的。尽管零假设可以被设定为任意数值，在这里我们规定零假设就是两组均值差异为0。例如，计算因变量在实验条件下和控制条件下的差异。

区分零假设（假设在总体中平均值差异正好为0）和零模型（当零假设为真时，我们应该观察得到的数据模型）非常重要。零假设是一个位于0的点，而零模型则是一个分布。如下图所示，教科书或者统计分析软件使用图片将其可视化，横轴代表*t*值，同时临界*t*值介于1.96-2.00之间（取决于样本量）。之所以用*t*值，是因为比较两组差异的统计检验是基于*t*分布的，当实际得到的*t*值大于临界*t*值时，我们就认为*p*值在统计上显著。

我个人认为，如果绘制的零模型是均值差的而非*t*值的，解释起来会更加清楚。所以在下方，你会看见当比较样本量为50的两组数据时，均值差的零模型，假设两组的真实差异为0，每组的标准差均为1。因为它们的标准差为1，你也可以把均值差异解释为Cohen's *d*效应量。所以，对样本量为50的两组进行独立样本t检验，其对应的Cohen's *d*为0时的分布也如图所示。


在每组收集50个观察值的独立样本t检验中，所得Cohen's *d*效应量的分布。

```{r, fig131, echo = FALSE, fig.cap = '(ref:fig131lab)'}
# Figure 1 & 2 (set to N <- 5000 for Figure 2)
# Set x-axis upper and lower scale points
low_x <- -1
high_x <- 1
y_max <- 2

# Set sample size per group and effect size d (assumes equal sample sizes per group)
N <- 50 # sample size per group for independent t-test
d <- 0.5 # please enter positive d only
# Calculate non-centrality parameter - equals t-value from sample
ncp <- d * sqrt(N / 2)

# # or Cumming, page 305
# ncp <- d / (sqrt((1 / N) + (1 / N)))

# calc d-distribution
x <- seq(low_x, high_x, length = 10000) # create x values
d_dist <- calc_d_dist(x, N, ncp)

# Set max Y
y_max <- max(d_dist) + 0.5

# create plot
par(bg = backgroundcolor)
plot(-10, xlim = c(low_x, high_x), ylim = c(0, y_max), xlab = "差异", ylab = "", main = paste("N = 50 的零假设"))

d_dist <- dt(x * sqrt(N / 2), df = (N * 2) - 2, ncp = 0) * sqrt(N / 2)
lines(x, d_dist, col = "black", type = "l", lwd = 2)

# Add type 1 error rate
add_type1_error(N, "right")
add_type1_error(N, "left")

```

首先要注意到的是，我们期望的零模型的平均值是0。观察X轴，我们可以看到绘制的分布是以0为中心的。但是，即使总体的均值差是0，这也不意味着每个从总体中抽出的样本的均值差都为0。样本的值会围绕总体的值变化，是一个标准差和样本量的函数。

图中Y轴是概率密度，它代表的是连续分布中某一特定值出现的可能性。我们可以看到，最有可能出现的均值差是总体的真实值0，离0越远的值得到的可能性就越小。图中有两个区域标红了，这些区域代表分布左尾部2.5%以及分布右尾部2.5%的极端值。当真实的均值差恰好为0，样本量为50时，它们共同构成了抽样分布的5%的极端值区域。当测量的值落在这个区域，相应的统计检验就认为两组的差异在5%的$\alpha$水平上达到显著。换句话说，因为比5%还小的均值差离0足够远，当出现时会被认为是小概率事件。若零假设为真，且测量到这种小概率的平均值差异（即红色区域），则被认为是犯了Ⅰ类错误。

让我们假设上图中的零模型是正确的，并且我们测量得到两组之间的平均值差异为0.5。这种测量得到的差异落在分布右尾的红色区域。这意味着，在真实平均差值为0的假设下，相对来说，测量得到的均差值是小概率事件。如果真实的均差值为0，概率密度函数表明0.5的平均差值不应该在测量当中经常出现。如果我们计算这个观测值的*p*值，它将低于5%。观察到的平均差异距离0远至0.5(当我们做双尾检验时，观察到的值要么在均值的左边，要么在均值的右边)的概率小于5%。

我更喜欢用原始分数而不是t值来绘制零模型的另一个原因是，当样本量增加时，我们可以看到零模型是如何变化的。当我们收集5000个而不是50个观测值时，我们看到零模型仍然以0为中心---但在现在的零模型中，我们预计大多数值将非常接近0。

当d=0时，在每组收集5000个观察值的独立样本t检验中，所得Cohen's *d*效应量的分布。

```{r, fig132, echo = FALSE, fig.cap= '(ref:fig132lab)'}
low_x <- -1
high_x <- 1
y_max <- 2

# Set sample size per group and effect size d (assumes equal sample sizes per group)
N <- 5000 # sample size per group for independent t-test
d <- 0.5 # please enter positive d only
# Calculate non-centrality parameter - equals t-value from sample
ncp <- d * sqrt(N / 2)

# calc d-distribution
x <- seq(low_x, high_x, length = 10000) # create x values
d_dist <- calc_d_dist(x, N, ncp)

# Set max Y
y_max <- max(d_dist) + 0.5

# create plot
par(bg = backgroundcolor)
plot(-10, xlim = c(low_x, high_x), ylim = c(0, y_max), xlab = "差异", ylab = "", main = paste("N = 5000的零假设"))

d_dist <- calc_d_dist(x, N, 0)
lines(x, d_dist, col = "black", type = "l", lwd = 2)

# Add type 1 error rate
add_type1_error(N, "right")
add_type1_error(N, "left")

```

由于均值差的分布是基于均值差的标准误，所以5000样本量的分布要窄得多。这个值是根据标准差和样本量计算的，如下所示:

$$\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}$$

这个公式表明，均值差的标准误是每个组的标准差（σ）的平方除以该组的样本量，加在一起，然后取平方根。样本量越大，除以的数字就越大，因此均值差的标准误就越小。在n=50的例子中，有标准差:

$$\sqrt{\frac{1^{2}}{50}+\frac{1^{2}}{50}}$$
因此，当n=50时，两组均值差的标准误为0.2；当n=5000时，均值差的标准误为0.02。假设抽样分布为正态分布，95%的观测值落在1.96个标准误之间。因此，对于样本量为50的样本，均值差应该在-1.96\*0.2=-0.392和+1.96\*0.2=0.392之间，我们可以看到，当n=50时，红色区域大约从-0.392到0.392。对于样本量为5000的样本，均值差应在-1.96\*0.02和+1.96\*0.02之间，也就是说，应该介于-0.0392到0.0392之间。由于样本量较大（n=5000），观测得到的均值差应该比较小样本（n=50）中观测得到的均值差更接近0.

我们要清楚，抽出n=5000的样本并观测到0.5均值差的概率比起收集50个观测值发现0.5的均值差的要更小。我们现在几乎已经准备好介绍关于p值的常见误解，但在此之前，我们需要引入一个当零假设不为真时的数据模型。如果我们不是从一个真实均值差为0的模型中抽样，那么得到的备择模型会是什么样呢？一些软件（比如G\*power,见图1.8))会同时显示出零模型（红色曲线）和备择模型（蓝色曲线）:

G\*Power软件截图显示了零模型（红色分布）和备择模型（蓝色分布）以及区分显著和非显著结果的临界*t*值（1.66055）。

```{r gpowerscreenshot, echo = FALSE, fig.cap = '(ref:gpowerscreenshotlab)'}
knitr::include_graphics("D:/New_translate/statistical_inferences-master/statistical_inferences-master/images/1.3.3.png")
```

我们做一项研究时，我们不知道真正的均值差是多少（如果我们已经知道了，为什么还要做这项研究?）但是，让我们假设有一个全知的存在，按照Paul Meehl的说法，我们称呼它为"无所不知的琼斯"。在我们从总体里收集50个观测值之前，"无所不知的琼斯"就已经知道了总体的真实均值差为0.5。那么在备择模型中，平均差值就应该在0.5周围变化。下图显示了零假设成立时的数据模式(用灰线表示)，和备择模型(用黑线表示)，即假设两个总体间真实存在0.5均值差的模型。


在*d*= 0，每组收集50个观测值的独立样本*t*检验中，Cohen's *d*效应量的分布。

```{r fig134, echo = FALSE, fig.cap = '(ref:fig134lab)'}
# Figure 3 & 4 (set d <- 1.5 and high_x <- 1.5 for figure 4)-----
low_x <- -1
high_x <- 3
y_max <- 2

# Set sample size per group and effect size d (assumes equal sample sizes per group)
N <- 50 # sample size per group for independent t-test
d <- 0.5 # please enter positive d only
# Calculate non-centrality parameter - equals t-value from sample
ncp <- d * sqrt(N / 2)

# or Cumming, page 305
ncp <- d / (sqrt((1 / N) + (1 / N)))

# calc d-distribution
x <- seq(low_x, high_x, length = 10000) # create x values
d_dist <- calc_d_dist(x, N, ncp)

# Set max Y
y_max <- max(d_dist) + 0.5

par(bg = backgroundcolor)
plot(-10, xlim = c(low_x, high_x), ylim = c(0, y_max), xlab = "差异", ylab = "", main = paste("N = 50 的零假设和备择假设"))
# abline(v = seq(low_x,high_x,0.1), h = seq(0,0.5,0.1), col = "lightgray", lty = 1)
lines(x, d_dist, col = "black", type = "l", lwd = 2)
# add d = 0 line
d_dist <- calc_d_dist(x, N, ncp=0)
lines(x, d_dist, col = "darkgrey", type = "l", lwd = 2)

# Add type 1 error rate
add_type1_error(N, "right")
add_type1_error(N, "left")

```

但是"无所不知的琼斯"也可以说真实均值差是一个更大的值。假设在另一个研究中，在我们抽样之前，琼斯说真实均值差是1.5。那么此时零假设模型不变，备择假设模型会向右移动。


在*d*=0和*d*=0.5，每组收集50个观察值的独立样本*t*检验中，Cohen's *d*效应量的分布。

```{r fig135, echo = FALSE, include = FALSE, fig.cap = '(ref:fig135lab)'}
low_x <- -1
high_x <- 1.5
y_max <- 2

# Set sample size per group and effect size d (assumes equal sample sizes per group)
N <- 50 # sample size per group for independent t-test
d <- 1.5 # please enter positive d only
# Calculate non-centrality parameter - equals t-value from sample
ncp <- d * sqrt(N / 2)

# # or Cumming, page 305
# ncp <- d / (sqrt((1 / N) + (1 / N)))

# calc d-distribution
x <- seq(low_x, high_x, length = 10000) # create x values
d_dist <- calc_d_dist(x, N, ncp)

# Set max Y
y_max <- max(d_dist) + 0.5

par(bg = backgroundcolor)
plot(-10, xlim = c(low_x, high_x), ylim = c(0, y_max), xlab = "差异", ylab = "", main = paste("N = 50 的零假设和备择假设"))
lines(x, d_dist, col = "black", type = "l", lwd = 2)
# add d = 0 line
d_dist <- calc_d_dist(x, N, ncp=0)
lines(x, d_dist, col = "darkgrey", type = "l", lwd = 2)

# Add type 1 error rate
add_type1_error(N, "right")
add_type1_error(N, "left")

```

您可以在此应用程序上在线调整备择模型和零模型：<http://shiny.ieis.tue.nl/d_p_power/>.该应用程序允许你设定独立*t*检验中每组的样本量（从2到无穷大）、均值差（从0到2）和$\alpha$水平。在图中，红色区域表示Ⅰ类错误，蓝色区域表示Ⅱ类错误（具体我们将在之后讨论）。该应用程序也会显示临界值：会有一条垂直线（样本量为50时，落在均值差为0.4处）和一句提示语------大于0.4代表效应统计学显著。这个应用程序展示了双侧独立样本*t*检验的情况,虽然没有提示语，我们应该知道此时小于-0.4也是显著的。

你可以看到，在表示临界均值差的竖线的左边有一个蓝色区域，它是备择模型的一部分。这就是犯Ⅱ类错误的概率（或者代表1-统计检验力）。如果一个研究的统计功效是80%，代表我们能观测到的80%的均值差都会落在代表临界值的那条线的右边。如果备择假设为真，但是我们又得到了小于临界值的效应，那么即使真的存在效应，此时的*p*值大于0.05。你可以在应用程序中验证，当样本量越大，整个备择模型就越靠右，统计力也就越大。同时你也可以看到，样本量越大，分布越窄，低于临界值的分布越少（只要真实总体均值大于临界值）。最终，$\alpha$水平越大，临界值代表的均值差越向左移，低于临界值的备择分布的面积就越小。

该应用程序还绘制了3个图，是不同$\alpha$水平、不同样本量和不同真均值差下的检验功效曲线函数。用户可以在应用程序中调整这三个值，了解每个变量如何影响零模型和备择模型，看到达到统计学显著的均值差的大小和犯Ⅰ类以和Ⅱ类错误的概率。

到目前为止，零模型的几个方面应该已经很清楚了。首先，传统零假设中的总体均值差为0，但在你抽取的任何样本中，观测的均值差都落在一个以0为中心的分布中，通常会略大于或略小于0。其次，该分布的宽度取决于样本量和标准差。研究的样本量越大，分布就越集中于0。最后，当观测到的均值差落在零模型尾端时，结果被认为是小概率事件。离0越远，这个结果就越令人惊讶。但是，当零模型为真时，这些令人惊讶的结果发生的概率为$\alpha$水平（也被称为Ⅰ类错误）。请记住，当研究者得出总体存在差异，但实际的均值差为0时，就发生了Ⅰ类错误。

现在我们终于可以讨论一些关于p值的常见误解了。让我们来回顾一下科学文献中报告过的一系列常见误解。其中一些也许听起来只是语言表达问题。乍一看，人们很容易认为这句话传达了正确的想法，即使在书面形式上并不正确。然而，当一个陈述在形式上不正确时，它就是错误的。正是因为人们会经常误解p值，所以在形式上正确解释p值是非常重要的。

### 误解 1：*p*值不显著意味着零假设为真。

此误解的一个常见版本如句子：'因为*p*\>0.05，我们可以推断效应不存在'中；这类句子的另一个版本是'差异不存在（*p*\>0.05）'。

在谈论更多细节之前，我想提醒你一个简单的事实，它会帮助你辨明许多关于*p*值的误解：*p*值是对数据概率的描述，而非对假设或理论概率的描述。不论何时当你看到*p*值被解释成一个假设或理论的概率，你需要知道这是不对的。与假设相关的例子有：'零假设为真'或'备择假设为真'，它们都是指零模型或者备择模型为真的概率是100%。更微妙的表述有'观测到的差异不是出于偶然'。当零假设为真时，观测到的差异只是'由于偶然'（而不是由于真实差异的存在），与之前一样，这句话是指零假设100%为真。

当你得出'效应不存在'或者'不存在差异'的结论时，你也就是在说零假设100%为真。但是由于*p*值是有关数据概率的解释，你应该避免仅基于*p*值去解释理论存在的概率。*p*值旨在帮助你从嘈杂的数据生成过程（即真实世界）中识别出意料之外的结果。它并不能量化一个假设为真的概率。

让我们举一个具体的例子来说明为什么不显著的结果并不意味着零假设就是正确的。在下图中，无所不知的琼斯告诉我们，真均值差为0.5。我们可以从图中看出，当备择假设为真时，期望的均值差的可视化分布以0.5为中心。我们观测到的均值差是0.35。这个值并没有极端到足以在统计学上与0有显著差异。同样从图中可以看出，这个值没有落在零模型的红色区域（因此，*p*值并不小于我们规定的$\alpha$水平）。

然而，鉴于真均值差是0.5，观测均值差为0.35不仅很可能发生，而且更可能是在备择模型而非是零模型上观测到。你可以看到这一点，因为在零模型中，均值差为0.35的概率密度曲线的高度约为0.5，在备择模型中概率密度曲线的高度则接近1.5。详情请参见[似然性](#likettest)一章。

(ref:fig136lab)
在*d*=0.35时，对*d*=0和*d*=0.5的情况下分别进行每组50个观测值的独立样本t检验，所得Cohen's *d*效应量的分布。

```{r fig136, echo = FALSE, fig.cap = '(ref:fig136lab)'}
low_x <- -1
high_x <- 1.5
y_max <- 2

# Set sample size per group and effect size d (assumes equal sample sizes per group)
N <- 50 # sample size per group for independent t-test
d <- 0.5 # please enter positive d only
# Calculate non-centrality parameter - equals t-value from sample
ncp <- d * sqrt(N / 2)

# #or Cumming, page 305
# ncp<-d/(sqrt((1/N)+(1/N)))

# calc d-distribution
x <- seq(low_x, high_x, length = 10000) # create x values
d_dist <- calc_d_dist(x, N, ncp)

# Set max Y
y_max <- max(d_dist) + 0.5

par(bg = backgroundcolor)
plot(-10, xlim = c(low_x, high_x), ylim = c(0, y_max), xlab = "差异", ylab = "", main = paste("N = 50 的零假设和备择假设"))
lines(x, d_dist, col = "black", type = "l", lwd = 2)
# add d = 0 line
d_dist <- calc_d_dist(x, N, ncp = 0)
lines(x, d_dist, col = "darkgrey", type = "l", lwd = 2)

# Add type 1 error rate
add_type1_error(N, "right")
add_type1_error(N, "left")

segments(0.35, 0, 0.35, 2.2, col = "black", lwd = 2)
text(0.35, 2.4, paste("所得均值差异"), cex = 1)

```

如果假定零假设为真，所有p值都表明均值差为0.35并不那么出人意料。这有很多可能的原因。在真实世界中，并没有无所不知的琼斯告诉我们真均值差，所以即使真均值差存在，如上图所示的情况还是可能出现。

那么我们应该如何陈述呢？解决办法很微妙，但也很重要。让我们再看看之前提出的两个错误陈述的例子。首先，"因为*p*\>0.05，我们可以推断效应不存在"这个陈述是错误的，因为效应很有可能是存在的（请记住*p*值是有关数据的解释而不是有效应或无效应的概率）。费希尔对于*p*值的解释是，我们可以得出一个小概率事件的发生或者零假设是错误的（他的原话是："要么是发生了及其罕见的事件，要么是随机分布的原假设不正确"）。这可能听起来像是对理论的概率的解释，但实际只是描述了当*p*值很小时，两种可能出现的场景（犯Ⅰ型错误或者备择假设为真）。真阳性和假阳性都是可能的，而且我们不会量化两种可能性出现的概率（例如，我们并不会说零假设为假的概率是95%）。从内曼-皮尔逊的角度来看，*p*\>.05意味着我们不能拒绝零假设，因为我们有着5%的预期错误率。

如果你对推断效应是否缺失感兴趣，零假设检验并不是你需要的工具。零假设检验回答的问题是'在预期错误率下，我可以拒绝零假设吗？'。当*p*\>0.05，零假设无法被拒绝时，零假设的真假无法仅靠*p*值推断得出（就像'无'的概念：既不是真也不是假）。幸运地是，一些统计方法能够回答效应缺失的问题，例如[等价检验](#equivalencetest),贝叶斯因子和贝叶斯估计(相关概述请参见
@harms_making_2018)。

第二个错误的说法是'没有差异'。这一论述更正起来要容易些。你可以写'在统计学上没有显著差异'。当然，两者有点重复，因为它们基本上是用两种不同的方式表明*p*值大于$\alpha$水平，但至少这一说法在形式上是正确的。'没有差异'和'没有统计学上的显著差异'听起来可能只是语义上的区别，但实际前者在说'差异为0'，而后者在说'差异不足以使*p*\<.05'。虽然我从未见过有人这样做，但更完整的说法应该是'鉴于样本量是每组50，$\alpha$水平是0.05，观测差异只有大于0.4才能达到统计学上的显著水平，但由于我们的观测差异是0.35，因此我们不能拒绝零假设'。如果感觉这是一个非常令人不满的结论，请记住，零假设检验的目的并不是为了得出关于效应缺失的有趣结论---你需要了解等价检验，来得到与零效应相关的更好答案。

### 误解 2：*p*值显著意味着零假设为假。

这是上一个误解的另一面。基于这种误解的错误陈述有'*p*\<.05，因此效应存在'，或'两组之间存在差异，*p*\<.05'。像之前一样，这些陈述都在暗指零假设为假的概率是100%，而且备择假设为真。

举一个简单的例子来说明为什么这些极端说法是不正确的。假设我们通过下面的命令在R中生成一系列数字：

```{r}
rnorm(n = 50, mean = 0, sd = 1)
```

该命令会从一个平均值为0、标准差为1的分布中随机生成50个观察值（从理论上来说---每个生成样本的均值和标准差会有所不同）。如果我们运行这个命令一次，得到一个均值为0.5的分布。下图画出了这个分布。如果我们对它和0进行单样本t检验，得到*p*\<.05，这个检验告诉我们，我们观测到的数据与0有显著差异，但此时R函数中随机数生成器是按照指令运行的，生成数据的真实均值为0。

(ref:fig137lab)
在真实*d*=0的情况下，在进行每组50个观测值的独立样本*t*检验所得的Cohen's *d*效应量分布中观察到*d*=0.5

```{r fig137, echo = FALSE, fig.cap= '(ref:fig137lab)'}
# Set x-axis upper and lower scalepoint
low_x <- -1
high_x <- 1
y_max <- 2.5

# Set sample size per group and effect size d (assumes equal sample sizes per group)
N <- 50 # sample size per group for independent t-test
d <- 0.0 # please enter positive d only
# Calculate non-centrality parameter - equals t-value from sample
ncp <- d * sqrt(N / 2)
# # or Cumming, page 305
# ncp <- d / (sqrt((1 / N) + (1 / N)))

# calc d-distribution
x <- seq(low_x, high_x, length = 10000) # create x values
d_dist <- calc_d_dist(x, N, ncp)

par(bg = backgroundcolor)
plot(-10, xlim = c(low_x, high_x), ylim = c(0, y_max), xlab = "差异", ylab = "", main = paste("N = 50 的零假设和备择假设"))
d_dist <- calc_d_dist(x, N, ncp = 0)
lines(x, d_dist, col = "black", type = "l", lwd = 2)

# Add type 1 error rate
add_type1_error(N, "right")
add_type1_error(N, "left")

segments(0.5, 0, 0.5, 2.2, col = "black", lwd = 2)
text(0.5, 2.4, paste("所得均值差异"), cex = 1)

```

*p*值显著并不能让我们得出零假设（"随机数生成器照常运行"）为假的结论。诚然，我们生成的50个样本的平均值非常极端。但是较低的*p*值只是告诉我们这个观测结果是出人意料的。当零假设为真时，我们只会在很低的概率下得到这个令人惊讶的观测结果---但它仍有可能发生。因此，显著的结果并不意味着备择假设就是对的---也可能是源于Ⅰ类错误，就如同上面的例子，无所不知的琼斯知道它属于这种情况。

让我们重新审视这个错误陈述'*p*\<.05，因此效应存在'。为了正确理解显著的*p*值，我们需要承认显著结果是Ⅰ类错误的可能性。请记住，费希尔会得出的结论是"要么发生了及其罕见的事件，要么随机分布的原假设不正确"。基于内曼-皮尔逊统计得出的正确解释是："我们可以把零假设当作假的，且从理论来看，我们出错的几率不会超过5%"。注意我使用了'当作'这个词，这并不说明着这个假设是真还是假，而只是说明，任何时候当*p*\<$\alpha$，我们就把零假设当作假的，那我们犯错误的几率不会超过$\alpha$%。

这两种正式的陈述都有一点冗长。在科学文章中，我们经常读到简短点的陈述，比如："我们可以拒绝零假设"，或者"我们可以接受备择假设"。这些陈述可能是假定读者会自己加上"从理论上来说，会有5%的错误概率"这句话。但是，至少在第一次陈述时，可以加上"从理论上来说，有5%的错误率"这个声明来提醒读者。

在上面的例子中，我们有一个非常强的主观先验概率，即R中的随机数生成器可以运行。其他可以纳入分析这种主观先验概率的统计方法有[贝叶斯统计](#bayes)或者[假阳报告概率](#ppv)。在频率主义的统计学中，你需要多次重复你的研究。你会时不时地观测到Ⅰ类错误，但不太可能连续三次都观测到。或者，你也可以在单次研究中降低犯Ⅰ类错误的概率。

### 误解 3：*p*值显著意味着发现了一个实际重要的效应。

在解释*p*值时，一个常见的问题是："显著"在日常语言中意味着"重要"，因此，"显著"效应会被解释为"重要"效应。然而，一个效应是否重要与它是否和零有差异，或效应有多大是无关的。不是所有效应都有实际影响。效应越小，能被人注意到的可能性越小，但这种效应仍然可能在社会层面上产生很大的影响。因此，一般来说，统计学显著性并不能回答一个效应在实践中是否重要的问题。要了解一个效应是否重要，你需要进行成本效益分析。

这个实际意义的问题经常出现在样本量非常大的研究中。正如我们之前所看到的，随着样本量增加，零值周围的概率密度分布变得越来越窄，这被认为是p值无限接近于0。

如果为一个非常大的样本量（例如每组n=10000）绘制零模型，我们会发现即使非常小的平均差异（比0.04的平均差异更极端的差异）也会被认为是"出人意料的"。这意味着若群体之间真的没有差异，从理论上看，你只有低于5%的几率观察到大于0.04的平均差异，同时95%的观测差异将小于0.04的平均差异。但论证这种效应的实际意义就变得更为困难。想象有一项干预措施能成功改变人们的消费行为，在实施这种干预措施时，人们每年可以节省12美分。很难论证这种效应如何使人变得更加幸福。然而，如果这些钱加在一起，将超过200万，这些钱可以用于在发展中国家治疗疾病，从而产生真正的影响。如果目标是使人们更加快乐，那么干预的成本会被认为太高，但如果目标是为慈善事业筹集200万，这可能会被认为是值得的。

在心理学中，并不是所有的效应都可以相加（我们不能将幸福感增加0.04个刻度点的效应合并或转移），所以要论证主观感受中弱效应的重要性往往比较困难[@anvari_not_2021]。成本效益分析可能表明弱效应很重要，但是否真的如此，我们无法从*p*值推断出来。

请注意，这与对p值本身的解释并没有关系：如果零假设为真，*p*\<0.05仍然正确表明我们观测到的数据是出人意料的。然而，数据出人意料并不意味着我们就需要关心它。在这里造成困惑的主要是语言标签"显著"---"显著"应该理解为是"出乎意料的"效应，但不一定是"重要的"效应，这样想可能会减少困惑。

### 误解 4：如果你得到了显著结果，你犯Ⅰ类错误（假阳性）的概率是5%。

此误解是对*p*值是"偶然观察到显著结果的概率"这一错误说法的一种可能解释。试设想，我们收集了20个观测值，并且无所不知的琼斯告诉我们零假设为真（就像上面的例子里，我们在R中生成随机数一样）。这意味着我们正从下图的分布中进行抽样。

(ref:fig138lab)
*d*=0，在每组20个观测值的独立样本*t*检验中，Cohen's *d*效应量的分布。

```{r fig138, echo = FALSE, fig.cap= '(ref:fig138lab)'}
# Set x-axis upper and lower scale points
low_x <- -1
high_x <- 1
y_max <- 2.5

# Set sample size per group and effect size d (assumes equal sample sizes per group)
N <- 20 # sample size per group for independent t-test
d <- 0.0 # please enter positive d only
# Calculate non-centrality parameter - equals t-value from sample
ncp <- d * sqrt(N / 2)
# # or Cumming, page 305
# ncp <- d / (sqrt((1 / N) + (1 / N)))

# calc d-distribution
x <- seq(low_x, high_x, length = 10000) # create x values
d_dist <- calc_d_dist(x, N, ncp)

par(bg = backgroundcolor)
plot(-10, xlim = c(low_x, high_x), ylim = c(0, y_max), xlab = "差异", ylab = "", main = paste("N = 20的零假设和备择假设"))
d_dist <- calc_d_dist(x, N, ncp = 0)
lines(x, d_dist, col = "black", type = "l", lwd = 2)

# Add type 1 error rate
add_type1_error(N, "right")
add_type1_error(N, "left")

segments(0.5, 0, 0.5, 2.2, col = "black", lwd = 2)
text(0.5, 2.4, paste("所得均值差异"), cex = 1)

```

如果这是真实情况，那意味着100%的时间里发现的显著结果，都是假阳性（或者犯了Ⅰ型错误）。因此，100%的显著结果都是Ⅰ型错误导致的。

区分在数据收集和结果分析之前和之后的概率是非常重要的。犯Ⅰ类错误的概率是指，在未来可能完成的所有零假设为真的研究中，低于5%的观测均值差会落到分布的红色尾部区域。但是当观测的结果落到了尾部区域，即*p*\<$\alpha$，我们又知道零假设为真时，那么这些显著结果就是Ⅰ类错误导致的。如果阅读得足够仔细，你就会发现这个误解实际是设问方式不同导致的。"如果我发现*p*\<.05，零假设为真的概率是多少？"和"如果零假设为真，观察到显著结果的概率是多少？"是两个完全不同的问题。*p*值只回答后一个问题。在数据收集前，你需要主观判断零假设为真的概率才能回答第一个问题。

### 误解 5：1减去*p*值是重复实验能得到相同效应的概率。

我们不可能仅基于*p*值计算出一种效应重复出现的概率，主要原因是我们不知道真实的均值差。如果我们是无所不知的琼斯，知道真均值差的值（例如，两组之间的差异为0.5个标度），我们就能知道这个检验的统计功效。统计功效是指当备择假设为真时（即真实效应存在），我们能发现显著结果的概率。例如，阅读应用程序中左侧栏里的文本，我们可以发现每组的样本量为50，真均值差为0.5，发现显著结果（或称统计功效）的概率为69.69%。如果我们在这种情况下观察到显著的效应（例如，*p*=0.03），并不意味着我们重复该次研究（使用相同的样本量）有97%的可能会得到显著结果。重复研究得到显著结果的概率取决于统计功效，而非之前研究的*p*值。

我们可以从最后一个误解中得知，显著结果复现的概率取决于真实的效应是否存在。换句话说，像上面的例子一样，如果存在一个真实的效应，统计检验力的水平就代表着能够重复观察到显著结果的概率（例如，统计检验力为80%意味着我们有80%的时间能够观察到显著的结果）。另一方面，如果零假设为真（例如，效应为0），那么显著的结果仅仅会在接近我们选择的$\alpha$水平的概率上被观察到（例如，如果我们选择的$\alpha$水平为0.05，就会有5%的可能犯Ⅰ类错误）。因此，如果原始研究中正确地观测到了一个效应，在重复实验中观察到显著结果的概率取决于统计功效；如果原始研究中正确地观测到了零效应，在重复实验中观察到显著结果的概率则取决于$\alpha$水平。

##1.8自我测试

###你能想到的有关p值的问题

将下面的代码复制到R并运行代码。您可以单击代码部分右上角的"剪贴板"图标，将所有代码复制到剪贴板，这样您就可以轻松地将其粘贴到R
中。

```{r, q1, echo = TRUE}

nsims <- 100000 # number of simulations
# 模拟的次数

m <- 106 # mean sample
# 样本均值
n <- 26 # set sample size
# 设置样本量大小
sd <- 15 # SD of the simulated data
#模拟数据的标准差

p <- numeric(nsims) # set up empty vector
# 设置一个空向量
bars <- 20

for (i in 1:nsims) { # for each simulated experiment
  # 对于每次模拟实验的循环
  x <- rnorm(n = n, mean = m, sd = sd)
  z <- t.test(x, mu = 100) # perform the t-test  # 进行T检验
  p[i] <- z$p.value # get the p-value  #得到p值
}
power <- round((sum(p < 0.05) / nsims), 2) # power # 统计功效

# Plot figure # 画图
hist(p,
  breaks = bars, xlab = "P-values", ylab = "number of p-values\n", 
  axes = FALSE, main = paste("P-value Distribution with", 
                             round(power * 100, digits = 1), "% Power"),
  col = "grey", xlim = c(0, 1), ylim = c(0, nsims))
axis(side = 1, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1))
axis(side = 2, at = seq(0, nsims, nsims / 4), 
     labels = seq(0, nsims, nsims / 4), las = 2)
abline(h = nsims / bars, col = "red", lty = 3)

```

我们可以从条形图的x轴上看到从0到1的*p*值，在y轴上，代表观察到这些*p*值的频率。其中水平红色虚线表示$\alpha$为5%（位于频率100.000\*0.05=5000）---但您现在可以忽略这条线。在标题中，给出了在模拟研究中达到的统计功效（假设$\alpha$为0.05）：研究具有50%的统计功效（每次模拟都有细微的差异）。

**问题1:**由于统计功效是观察到结果在统计上显著的概率，如果效应为真，我们从该图上哪里以看到统计功效本身？
A)我们可以计算出大于0.5的*p*值的数量，并将它们除以模拟次数。
B)我们可以计算第一个条形中的*p*值数量（其中包含从0.00到0.05的所有"显著"*p*值），并将该条形中的*p* 值除以模拟总数。
C)我们可以计算高于0.5的*p*值减去低于0.5的*p*值之间的差值，并将该数字除以模拟总数。
D)我们可以计算高于0.5的*p*值减去低于0.05的*p*值之间的差值，并将该数字除以模拟次数。

**问题2:**将代码中第4行的样本大小从n\<-26更改为n\<-51。通过选择所有行并按CTRL+Enter来运行模拟。现在我们已将样本量从26人增加到51人，模拟的功效如何？请记住，模拟有时会产生略有不同的答案，因此请选择最接近模拟结果的答案选项。
A) 55% 
B) 60% 
C) 80% 
D) 95%

**问题3:**如果你观察*p*值的分布，你会发现什么？ 
A)*p*值分布与50%功效时完全相同 
B) *p*值分布比50%功效时陡峭得多 
C)*p*值分布比50%功效时平坦得多 
D) *p*值分布比50% 功效时更符合正态分布

请随意增加和减少样本量，看看运行后会发生什么。完成这些探索后，请确保第4行代码中的样本量仍然为n\<-51。

**问题4:**当我们的模拟样本与平均IQ分数之间没有真正差异时会发生什么？在这种情况下，我们没有观察到任何效应，因此您可能会认为统计功效为"0"。事实上，当真正的效应不存在时，统计功效无法被定义。但是，我们可以因此将其称为"零功效"。将样本中的均值更改为100（将m\<-106设置为m\<-100），现在样本中的均值与我们在单样本t检验中测试的总体值之间没有差异。请再次运行脚本，您发现了什么？

A) *p*值分布与 50% 功效完全相同 
B) *p*值分布比 50% 功效陡峭得多 
C)*p*值分布基本上是完全平坦的（忽略了由于模拟中的随机噪声引起的一些微小变化）
D) *p*值分布呈正态（即钟形）分布

下面的问题建立在上面的数据模拟之上，其中各组之间没有真正的区别。

**问题5:**查看为Q4生成的图中最左边的条柱，并查看该条中*p*值的频率。这个条柱的正式名称应该是什么？
A)统计功效（或真阳性） 
B)真阴性 
C)Ⅰ类错误（或假阳性）
D)Ⅱ类错误（或假阴性）

让我们只看一下低于0.05的*p*值，请耐心进行接下来的几个步骤。在第8行的语句bars\<-20中找到决定有多少条柱的变量。将其更改为bars\<-100。我们现在将获得0到0.01之间的*p*值的1个柱，*p*值在0.01和0.02之间的1个柱，总共100个柱。红色虚线现在将指示原假设为真时*p*值的频率，其中每个条柱包含*p*值总数的1%。我们只想查看低于0.05的*p*值，我们将在0.05处截断该图。将xlim=c(0,1)更改为xlim=c(0,0.05)。我们不会看到0到1之间的所有*p*值，而只会看到0到0.05之间的*p*值。重新运行模拟（仍然是m\<-100）。我们将看到相同的均匀分布，但现在每个条柱都包含1%的*p*值，因此*p*值分布非常平坦，几乎看不到（稍后我们将在y轴上放大此分布）。假设零假设为真，红线现在清楚地给出了每个条柱的频率。

将第9行模拟中的平均值更改为m\<-107（记住n仍然是51）。重新运行模拟。很明显，我们拥有非常大的统计功效。大多数*p*值位于最左侧的条柱中，其中包含0.00和0.01之间的所有*p*值。

**问题6:**上次模拟的图告诉我们有大约90.5%的功效（注意：由于随机变化，您模拟中的数字可能会略有不同），这就是我们使用5%的$\alpha$时的功效。但我们也可以使用1%的$\alpha$。看看图表，当我们使用1%的$\alpha$时，我们在模拟研究中的统计功效是多少？从您的模拟中选择最接近答案的答案。请注意，您还可以通过将第15行中的*p*\<0.05更改为*p*\<0.01来计算$\alpha$为0.01的统计功效，只需确保在继续下一步探索之前将其设置回0.05。

A) \~90% 
B) \~75% 
C) \~50% 
D) \~5%

为了能够查看0.03和0.04附近的*p*值，我们还将放大y轴。在绘制绘图的代码部分，将ylim=c(0,nSims)更改为ylim=c(0,10000)。重新运行脚本。

将样本中的平均值更改为108（m\<-108)，并将样本大小保留为51。运行模拟。与上图相比，看看分布发生了怎样的变化？

查看从左边数第五个条柱。此条柱现在包含0.04到0.05之间的所有*p*值。你可能会发现一些奇怪的现象。请记住，假设零假设为真，红色虚线代表每个条柱的频率。查看*p*值介于0.04和0.05之间的条形如何低于红线。我们有96%功效的模拟研究。当功效非常高时，*p*值介于0.04和0.05之间的情况非常罕见---它们出现的概率不到1%（大多数*p*值小于0.01）。当零假设为真时，0.04和0.05之间的*p*值恰好出现1%的几率（因为*p*值是均匀分布的）。现在问问自己：当您的统计功效非常高并且观察到*p*值介于0.04和0.05之间时，零假设更可能为真，还是备择假设更可能为真？鉴于当零假设为真时您更有可能观察到0.04和0.05之间的*p*值，您应该将$\alpha$为0.05的*p*值解释为更有可能在零假设为真时出现，而不是备择假设为真。

在我们的模拟中，我们知道是否存在真实的效应，但在现实世界中，我们往往不知道。当您具有非常高的统计功效时，使用0.05的$\alpha$水平，并得到*p*=.045，若零假设为真，这将是个令人吃惊的结果。但若备择假设为真，该结果更令人吃惊。这表明显著的*p*值并不总是备择假设的证据。

**问题7:**当你关心的最小效应量具有非常高（例如98%）的统计功效，并且你观察到*p*值为
0.045 时，正确的结论是什么？ 
A)效应显著，为备择假设提供了强有力的支持。
B)效应显著，但毫无疑问属于第一类错误。 
C)对于高统计功效，应该使用小于0.05 的 $\alpha$水平，因此，该效应不能被认为是显著的。
D)效应显著，但相比于备择假设，数据更可能在零假设下出现。

**问题8:**通过改变样本量(n)和平均值(m)，从而改变模拟研究中的统计功效。查看包含介于0.04和0.05之间的*p*值的条形的模拟结果。红线表示如果零假设为真（并且始终为1%），将在此条柱中找到多少*p*值。在最好的情况下，0.04和0.05之间的*p*值来自效应真实存在时*p*值分布的可能性比来自效应不存在时*p*值分布的可能性大多少？你可以通过查看0.04和0.05之间的*p*值条柱的高度来回答这个问题。如果模拟中的条形图最多是红线处的五倍高（因此条形图显示5%的*p*值最终介于0.04和0.05之间，而红线保持在1%），那么*p*值在0.04和0.05之间时，有真实效应的可能性是不存在真实效应的五倍。

A)*p*值介于0.04和0.05之间的概率在备择假设和零假设下相同。 
B)在备择假设下，*p*值介于0.04和0.05之间的概率大约是零假设下的4倍。
C)在备择假设下，*p*值介于0.04和0.05之间的概率是零假设下的\~10倍左右。 
D)在备择假设下，*p*值介于0.04和0.05之间的概率最多是零假设下的\~30 倍。

出于这个原因，统计学家会发出警告：略低于0.05的*p*值（例如，0.04和0.05之间）是对备择假设的最弱支持。如果你发现*p*值在此范围内，请考虑重复该研究，就算不能重复研究，至少要谨慎地解释结果。当然，你也可以根据Neyman-Pearson的方法，提出最多有5%的可能发生Ⅰ类错误的声明。因此，林德利悖论很好地说明了统计推断的不同哲学方法之间的差异。

### 针对p值概念的误解

**问题1:**当独立样本t检验中每组的样本量为50个观察值时（见图1.6），下面哪种说法是正确的？

A)两组之间观察到平均差异总是0。 
B)两组之间的平均差异一般不可能为0。
C)假设零假设为真，能观察到+0.5或-0.5的平均差异几乎不太可能
D)假设零假设为真，能观察到+0.1或-0.1的平均差异几乎不太可能

**问题2:**图1.6和图1.7中的零模型在什么方面上是相似的，在什么方面又是不同的？

A)在这两种情况下，分布都以零为中心，临界*t*值在1.96和2之间（对于双侧检验来说，取决于样本量）。但是，样本量越大，平均差异越接近于0的结果更容易"令人惊讶"。
B)在这两种情况下，*t*值为0是最有可能的结果，但对于n=50来说临界*t*值大约是0.4，而n=5000时，大约是0.05。
C)在这两种情况下，平均值都在0附近变化，但n=5000时的犯第一类错误的概率比n=50时小得多。
D)因为n=50的标准误差比n=5000的标准误差大得多，所以n=50时的零假设更可能是真的。

**问题3:**您可以在这个在线app中调整备择模型和零模型：<http://shiny.ieis.tue.nl/d_p_power/>。该应用程序允许你指定独立样本*t*检验中每组的样本量（从2到无穷大），平均差异（从0到2），以及α水平。在该图中，红色区域显示了Ⅰ类错误。蓝色区域直观地显示了Ⅱ类错误。该app还会告诉你临界值：有一条垂直线（在n=50的情况下，这条线落在0.4的平均差上）和一个标签，写着："大于0.4的效应将具有统计学意义"。请注意，对于小于-0.4的效应也是如此，尽管那里没有第二个标签，但该app显示的是双侧独立样本*t*检验的情况。

您可以看到，在表示临界均值差的垂直线左侧，有一个蓝色区域是备择假设模型的一部分。这是犯Ⅱ类错误的几率（或表示1减去该研究的统计功效）。如果一项研究具有80%的功效，那么我们将观察到的80%的平均误差应该落在该线指示的临界值的右侧。如果备择假设的模型为真，但我们观察到的效应小于临界值，即使存在真实效应，那么我们观察到的*p*值也将大于0.05，您可以在app中查看，效应越大，整个备择假设所代表的模型分布越靠右，因此统计功效越高。您还可以看到，样本量越大，分布越窄，低于临界值的分布将越少（只要真正的总体均值大于临界值）。最后，$\alpha$水平越大，临界均值差向左移动得越远，低于临界值的备择假设所代表的分布区域就越小。

该app还绘制了3张图表，来说明不同α水平、样本量或真实平均差对应的统计功效曲线函数。通过改变数值，在app中进行探索。感受一下每个变量是如何影响零模型和备择模型的，以及如何影响平均差的统计学显著性、Ⅰ类和Ⅱ类错误率的。

打开app，并确保它为默认设置，即保持样本量为50，α水平为0.05。看一下零模型的分布。然后将样本大小设为2，再将样本大小设为5000并观察分布。在该app中，你无法绘制"组"样本量大小为1的数据。但在n=2的情况下，你将得到真实效应为0时单个观察值（n=1）的期望值范围。根据你在改变不同参数时对app的体验，下面哪句话是正确的？

A)当零假设为真且标准差为1时，如果你从每组中随机抽取1个观察值并计算差异得分，那么在你抽取的所有观察对中，其中95%的差异将落在-0.4和0.4之间。
B)当零假设为真且标准差为1时，每组样本量n=50，从理论上讲，95%的研究，均值差落在-0.4和0.4之间。
C)在任意每组样本量为n=50的研究中，即使标准差未知，并且也不知道零假设是否为真，你应该很少能观察到比-0.4或0.4更极端的均值差异。
D)随着样本量的增加，对于零模型来说，均值的期望分布会变窄，但对于备择模型来说则不会。

**问题4:**使用默认设置再次打开应用程序。将$\alpha$水平设置为0.01（同时将平均差保持在0.5，样本大小保持在50）与$\alpha$=0.05时的临界值相比，以下哪个说法是正确的？

A)与0.05的α值相比，当使用0.01的α值时，*更不极端*的数值会被认为是难以置信的，而且只有大于0.53（或小于-0.53）的差异才具备统计学意义。
B)与0.05的α值相比，当使用0.01的α值时，*更不极端*的数值会被认为是难以置信的，而且只有大于0.33（或小于-0.33）的差异才具备统计学意义。
C)与0.05的α值相比，当使用0.01的α值时，*更极端*的数值会被认为是难以置信的，而且只有大于0.53（或小于-0.53）的差异才具备统计学意义。
D)与0.05的α值相比，当使用0.01的α值时，*更极端*的数值会被认为是难以置信的，而且只有大于0.33（或小于-0.33）的差异才具备统计学意义。

**问题5:**当您观察到统计上不显著的*p*值(p\>$\alpha$)时，为什么不能得出零假设一定为真的结论？

A) 在计算*p*值时，需要考虑先验概率。 
B)你需要考虑出现Ⅰ类错误的概率。
C)零假设永远不会是真的。 
D)你需要考虑出现Ⅱ类错误的概率。

**问题6：**当你观察到一个具有统计学意义的*p*值（p\<$\alpha$）时，为什么不能得出备择假设一定为真的结论？

A) 在计算*p*值时，你需要考虑先验概率。 
B)你需要考虑出现Ⅰ类错误的概率。
C) 备择假设从不为真。 
D)你需要考虑出现Ⅱ类错误的概率。

**问题7:**在解释*p*值时，一个常见的问题是，"显著"在正常语言中意味着"重要"，因此，"显著"效应被解释为"重要"效应。然而，一个效应是否重要的问题与它是否不等于零，甚至效应有多大的问题是完全独立的。一方面来说，不是所有的效应在实际生活中都能产生影响。另外一方面来说，效应越小，越不可能被人注意到，但这种效应仍然可能在社会层面上产生很大的影响。因此，一般来说，统计学意义并不能回答一个效应在实践中是否重要，或是否"实际重要"的问题。要回答一个效应是否重要的问题，你需要做成本效益分析。

转到app：[http://shiny.ieis.tue.nl/d_p\\\_power/](http://shiny.ieis.tue.nl/d_p_power/){.uri}。设置样本量为50000，平均差异为0.5，α水平为0.05,并观察以下哪种效应会与0存在统计学差异？

A)比-0.01和0.01更极端的效应 
B)比-0.04和0.04更极端的效应
C)比-0.05和0.05更极端的效应 
D)比-0.12和0.12更极端的效应

如果我们为一个非常大的样本量（例如，每组n=10000）绘制零模型，我们会发现，即使是非常小的平均差异（比0.04的平均差异更极端的差异）也会被认为是"令人惊喜的"。这仍然意味着，如果在整个人群中真的没有差异，你将有5%的几率观察到大于0.04的平均差异，而剩下95%的几率则观察到小于0.04的平均差异。但要论证这种效应的实际意义就变得更加困难。想象一下，一项具体的干预措施在改变人们的消费行为方面是成功的，当实施某种干预措施时，人们每年可以节省12美分。很难论证这种效应如何使人更加幸福。然而，如果这些钱加在一起，将产生200多万，这些钱可以用于治疗发展中国家的疾病，在那里会产生真正的影响。而如果干预的目标是使个人更快乐，干预的成本可能被认为太高，但如果目标是为慈善事业筹集200万，可能会被认为是值得的。

在心理学中，并不是所有的效应都是相加的（我们不能将幸福感增加0.04个尺度点的效应合并或转换），所以要论证主观感受中的小效应的重要性往往比较困难。成本效益分析可能显示小效应很重要，但是否真的如此，我们不能从p值中推断出来。相反，您需要报告并解释效应量。

**问题8:**我们使用R语言中的随机数生成器，输入rnorm(n=50,mean=0,sd=1)生成50个观察值，这些观察值的平均值是0.5，在对效应为0的单样本*t*检验中得到的*p*值是0.03，小于α水平（我们设定为0.05）。我们观察到显著差异（p\<α）只是偶然的概率有多少？

A)  3%
B)  5%
C)  95%
D)  100%

**问题9:**以下哪一个说法是正确的？

A) 重复研究产生显著结果的概率为1-*p*。
B)重复研究产生显著结果的概率是1-*p*乘以零假设为真的概率。
C)重复研究产生显著结果的概率等于重复研究的统计功效（如果存在真实效应的话）或α水平（如果不存在真实效应）。
D)重复研究产生显著结果的概率等于重复研究的统计功效加上α水平。

这个问题在概念上与Tversky和Kahneman（1971）在"相信小数法则"一文中提出的问题非常相似：

(ref:smallnumberslab) Tversky和Kahneman 1971年的文章中第一段的截图

```{r smallnumbers, echo=FALSE, fig.cap="(ref:smallnumberslab)"}
knitr::include_graphics("D:/New_translate/statistical_inferences-master/statistical_inferences-master/images/belieflawsmallnumers.png")
```

> 假设你对20名被试进行了实验，并得到了一个能证实你理论的重要结果（*z*=2.23，*p*\<0.05，双尾）。你现在再做一组10人的实验，你认为这一组的结果通过单尾检验并具有统计学意义的概率是多少？

Tversky和Kahneman认为合理的答案是48%，但唯一的正确答案与问题9的正确答案相同，确切的概率无法得知[@miller_what_2009]。

**问题10:** 一个不显著的*p*值（例如*p*=0.65）是否意味着零假设为真？

A)不是，该结果可能是II类错误，或假阴性。
B)是的，因为该结果是一个真阴性。
C)是的，如果*p*值大于α水平，则零假设为真。
D)不是，因为你需要至少两个不显著的*p*值才能得出零假设是真的结论。

**问题11:**
以下哪一个是采用了正确的方式来表达*p*值不显著（例如在独立t检验中使用0.05的α水平，*p*=
0.34）？

A) 零假设被证实，*p* \> 0.05 
B)两个条件之间没有差异，*p* \> 0.05 
C)观察到的差异在统计学上与0没有差异 
D) 零假设为真。

**问题12:** 观察到一个显著的p值（p \<0.05）是否意味着零假设是假的？

A)不，因为*p*\<0.05只意味着备择假设是真的，而不是说零假设是错的。
B)不是，因为*p*值从来不是关于假设或理论的概率的声明。
C)是的，因为一个异常罕见的事件已经发生。
D)是的，因为差异在统计学上是显著的。

**问题13:** 在统计学上有意义的效应是否总是意味着该效应在实践中也很重要？

A)不是，因为在超大样本中，极小的效应也可以有统计学意义，而小的效应在实际生活中从来就没有重要性。
B)不，因为理论上alpha水平可以设定为0.20，在这种情况下，显著的效应在实际生活中并不重要。
C)不是，因为一个效应有多重要取决于成本效益分析，而不是取决于在零假设下数据有多令人惊讶。
D)以上都是对的。

**问题14:** 以下关于p值定义哪一个是正确的？

A)*p*值是指零假设为真的概率，意味着给定的数据与您观察到的数据一样极端或更极端。
B)*p*值是备择假设为真的概率，意味着给定的数据与您观察到的数据一样极端或更极端。
C)假设备择假设为真，*p*值是指观察到的数据与您观察到的数据一样极端或更极端的概率。
D)假设零假设为真，*p*值是指观察到的数据与您观察到的数据一样极端或更极端的概率。

### 开放性问题

1.什么决定了*p*值分布的形状？

2.当存在真实效应并且样本量增加时，p 值分布的形状如何变化？

3.什么是林德利悖论？

4.当真实效应不存在时，*p*值如何分布？

5.*p*值的正确定义是什么？

6.为什么不显著的*p*值意味着零假设为真是不正确的？

7.为什么显著的*p*值意味着零假设为假是不正确的？

8.为什么显著的*p*值意味着发现了在实际生活中重要的效应是不正确的？

9.如果您观察到重要发现，您犯第1类错误（假阳性）的概率是5%，为什么这种观点是不正确的？

10.为什么1--*p*（例如
1--0.05=0.95）并不是指在重复研究时效应也能被重复的概率？

11.Fisherian和Neyman-Pearson的理论对*p*值的解释有什么不同？

12.在零假设显著性检验中，零模型或者说零假设代表了什么？

13.我们不能通过零假设显著性检验得出效应不存在的结论。那么我们可以通过哪些统计方法来确定真实效应不存在？
