```{r, include = FALSE}
source("~/Desktop/statistical_inferences-master/statistical_inferences-master/include/globals.R")

# needed to make the chapter (not visible)
library(ggplot2)
library(gridExtra)
library(BEST)

# for students
library(TOSTER)
library(pwr)
```

# Equivalence Testing and Interval Hypotheses {#equivalencetest}

Most scientific studies are designed to test the prediction that an effect or a difference exists. Does a new intervention work? Is there a relationship between two variables? These studies are commonly analyzed with a null hypothesis significance test. When a statistically significant *p*-value is observed, the null hypothesis can be rejected, and researchers can claim that the intervention works, or that there is a relationship between two variables, with a maximum error rate. But if the *p*-value is not statistically significant, researchers very often draw a logically incorrect conclusion: They conclude there is no effect based on *p* > 0.05. 

大多数科学研究旨在检验一个效应或差异存在的假设。新的干预措施有效吗？两个变量之间有关系吗？这些研究通常采用零假设显著性检验进行分析。当观察到具有统计学意义上显著的*p*值时，这个零假设便可被拒绝。同时，研究人员可以在承认最大错误率的前提下声称干预有效，或两个变量之间存在关联。但是，如果*p*值在统计意义上不显著，研究人员往往会得出一个逻辑上不正确的结论：他们基于*p* ＞ 0.05的结果得出结论，研究中的效应不存在。

Open a result section of an article you are writing, or the result section of an article you have recently read. Search for "*p* > 0.05", and look carefully at what you or the scientists concluded (in the results section, but also check which claim they make in the discussion section). If you see the conclusion that there was 'no effect' or there was 'no association between variables', you have found an example where researchers forgot that *absence of evidence is not evidence of absence* [@altman_statistics_1995]. A non-significant result in itself only tells us that we cannot reject the null hypothesis. It is tempting to ask after *p* > 0.05 'so, is the true effect zero'? But the *p*-value from a null hypothesis significance test cannot answer that question. It might be useful to think of the answer to the question whether an effect is absent after observing *p* > 0.05 as 無 ([mu](https://en.wikipedia.org/wiki/Mu_(negative)#Non-dualistic_meaning)), used as a non-dualistic answer, neither yes nor no, or 'unasking the question'. It is simply not possible to answer the question whether a meaningful effect is absent based on *p* > 0.05. 

打开你正在写的一篇论文的结果部分，或者你最近读过的一篇论文的结果部分。搜索“*p* >0.05”，仔细查看你或这位科学家得出的结论（在结果部分，但也要检查他/她们在讨论部分的说法）。如果你看到“没有效应”或“变量之间没有关联”的结论，那么你就发现了一个例子，研究人员忘记了*缺乏证据不等于没有证据*[@altman_statistics_1995]。一个不显著的结果本身只是告诉我们，我们不能拒绝零假设。在观察到*p* ＞0.05之后，“所以真正的效应是零吗？”会是一个让人很想了解的问题。但来自零假设显著性检验的*p* 值不能回答这个问题。在观察到p＞0.05后，将是否存在效应的问题的答案视为“無”([mu](https://en.wikipedia.org/wiki/Mu_(negative)#Non-dualistic_meaning))，用作非二元答案，既不是“是”也不是“否”，或者“未提出问题”，这可能更有用。仅基于*p*＞0.05，无法回答一个有意义的效应存在与否的问题。

There should be many situations where researchers are interested in examining whether a meaningful effect is absent. For example, it can be important to show two groups do not differ on factors that might be a confound in the experimental design (e.g., examining whether a manipulation intended to increase fatigue did not affect the mood of the participants, by showing that positive and negative affect did not differ between the groups). Researchers might want to know if two interventions work equally well, especially when the newer intervention costs less or requires less effort (e.g., is online therapy just as efficient as in person therapy?). And other times we might be interested to demonstrate the absence of an effect because a theoretical model predicts there is no effect, or because we believe a previously published study was a false positive, and we expect to show the absence of an effect in a replication study [@dienes_using_2014]. And yet, when you ask researchers if they have ever designed a study where the goal was to show that there was no effect, for example by predicting that there would be no difference between two conditions, many people say they have never designed a study where their main prediction was that the effect size was 0. Researchers almost always predict there is a difference. One reason might be that many researchers would not even know how to statistically support a prediction of an effect size of 0, because they were not trained in the use of equivalence testing.

在许多情况下，研究人员都应该有兴趣检验一个有意义的效应是否不存在。例如，证明可能成为实验设计的混杂因素在两个组别内没有差异是很重要的（例如，通过证明两组之间的积极和消极情绪没有差异来检验增加疲劳是否不会影响被试的情绪）。研究人员可能想知道两种干预措施是否同样有效，尤其是当新的干预措施成本更低或花费的成本更少（例如，线上治疗和面对面治疗一样有效吗？）。并且在其他时候，例如当理论模型预测没有效应，或我们认为之前发表的研究是假阳性，或我们希望在重复研究中证明一个效应不存在时，我们可能会有兴趣证明一个效应不存在[@dienes_using_2014]。然而，当你问研究人员，他们是否设计过一项旨在证明没有效应的研究，例如预测两种条件之间没有差异时，许多人会说，他们从未设计过一个主要预测是效应大小为0的研究。研究人员几乎总是预测会有差异。其中一个原因可能是许多研究人员甚至不知道如何在统计上支持一个效应大小为0的预测，因为他们没有接受过使用等价检验的训练。

It is never possible to show an effect is *exactly* 0. Even if you collected data from every person in the world, the effect in any single study will randomly vary around the true effect size of 0 - you might end up with a mean difference that is very close to, but not exactly, zero, in any finite sample. @hodges_testing_1954 were the first to discuss the statistical problem of testing whether two populations have the same mean. They suggest (p. 264) to: “test that their means do not differ by more than an amount specified to represent the smallest difference of practical interest”. @nunnally_place_1960 similarly proposed a ‘fixed-increment’ hypothesis where researchers compare an observed effect against a range of values that is deemed too small to be meaningful. Defining a range of values considered practically equivalent to the absence of an effect is known as an **equivalence range** [@bauer_unifying_1996] or a **region of practical equivalence** [@kruschke_bayesian_2013]. The equivalence range should be specified in advance, and requires careful consideration of the smallest effect size of interest. 

证明一个效应大小“正好”是零是永远不可能的。即使你从世界上每个人那里收集到数据，任何一项研究中的效应都会在真实效应量0左右随机变化——在任何有限的样本中，你最终可能都会得到非常接近但不完全为0的平均数差异。@hodges_testing_1954是第一个讨论检验两个群体是否具有相同平均值的统计问题的人。他们建议（第264页）：“检验两组中的均值的差异不超过指定的数额，以代表实际上的最小差异”。@nunnally_place_1960同样提出了一个“固定增量”假设，研究人员将观察到的效应与一个被认为太小而没有意义的值的范围进行比较。定义一个范围，范围内的值在实际上代表着没有效应，此范围就被称为**等效范围**[@bauer_unifying_1996]或**实际等效区域**[@kruschke_bayesian_2013]。等效范围应提前规定，并需要仔细考虑最小目标效应量。

Although researchers have repeatedly attempted to introduce tests against an equivalence range in the social sciences [@cribbie_recommendations_2004; @levine_communication_2008; @hoenig_abuse_2001; @rogers_using_1993; @quertemont_how_2011], this statistical approach has only recently become popular. During the replication crisis, researchers searched for tools to interpret null results when performing replication studies. Researchers wanted to be able to publish informative null results when replicating findings in the literature that they suspected were false positives. One notable example were the studies on pre-cognition by Daryl Bem, which ostensibly showed that participants were able to predict the future [@bem_feeling_2011]. Equivalence tests were proposed as a statistical approach to answer the question whether an observed effect is small enough to conclude that a previous study could not be replicated [@anderson_theres_2016; @lakens_equivalence_2017; @simonsohn_small_2015]. Researchers specify a smallest effect size of interest (for example an effect of 0.5, so for a two-sided test any value outside a range from -0.5 to 0.5) and test whether effects more extreme than this range can be rejected. If so, they can reject the presence of effects that are deemed large enough to be meaningful.

尽管研究人员一再试图在社会科学中引入针对等效范围的检验 [@cribbie_recommendations_2004; @levine_communication_2008; @hoenig_abuse_2001; @rogers_using_1993; @quertemont_how_2011], 但这种统计方法直到最近才流行起来。在可重复性危机期间，研究人员在进行重复研究时寻找解释无效结果的工具。研究人员希望在重复他们怀疑是假阳性的文献中的发现时，能够发布信息丰富的无效结果。一个值得注意的例子是Daryl Bem对前认知的研究，该研究表面上表明被试能够预测未来[@bem_feeling_2011]。等价检验被提议作为一种统计方法，以回答观察到的效应是否小到足以得出先前研究无法重复的结论的问题[@anderson_theres_2016; @lakens_equivalence_2017; @simonsohn_small_2015]。研究人员指定了最小目标效应量（例如0.5的效应，因此对于双侧检验来说，是在-0.5到0.5范围之外的任何值），并检验是否可以拒绝比这个范围更极端的效应。如果是这样，他们可以拒绝那些被认为足够大而有意义的效应的存在。

One can distinguish a **nil null hypothesis**, where the null hypothesis is an effect of 0, from a **non-nil null hypothesis**, where the null hypothesis is any other effect than 0, for example effects more extreme than the smallest effect size of interest [@nickerson_null_2000]. As Nickerson writes: 

人们可以将**效应量为0的虚无假设**与**效应量非0的虚无假设**区分开来，其中0零假设是效应为0，非0零假设是除0之外的任何其他效应，例如比最小目标效应量更极端的效应[@nickerson_null_2000]。正如尼克森所写：

>The distinction is an important one, especially relative to the controversy regarding the merits or shortcomings of NHST inasmuch as criticisms that may be valid when applied to nil hypothesis testing are not necessarily valid when directed at null hypothesis testing in the more general sense. 

>这是一个很重要的区别，特别是当涉及到NHST优缺点的争议，有些批评针对效应量为0的虚无假设很有用，但在效应量非0的虚无假设的语境下，由于后者更为宽泛，这些批评并不那么具有建设性。

Equivalence tests are a specific implementation of **interval hypothesis tests**, where instead of testing against a null hypothesis of no effect (that is, an effect size of 0; **nil null hypothesis**), an effect is tested against a null hypothesis that represents a range of non-zero effect sizes (**non-nil null hypothesis**). Indeed, one of the most widely suggested improvements that mitigates the most important limitations of null hypothesis significance testing is to replace the nil null hypothesis with the test of a range prediction (by specifying a non-nil null hypothesis) in an interval hypothesis test [@lakens_practical_2021]. To illustrate the difference, Panel A in Figure \@ref(fig:intervaltest) visualizes the results that are predicted in a two-sided null hypothesis test with a nil hypothesis, where the test examines whether an effect of 0 can be rejected. Panel B shows an interval hypothesis where an effect between 0.5 and 2.5 is predicted, where the non-nill null hypothesis consists of values smaller than 0.5 or larger than 2.5, and the interval hypothesis test examines whether values in these ranges can be rejected. Panel C illustrates an equivalence test, which is basically identical to an interval hypothesis test, but the predicted effects are located in a range around 0, and contain effects that are deemed too small to be meaningful. 

等价检验是**区间假设检验**的一种具体实现，等价检验针对的虚无假设，其效应并不为0。就零假设显著性检验的局限性而提出的改进建议中，最广泛出现的是使用区间假设检验中的范围预测（通过指定**效应量非0的虚无假设**）来代替**效应量为0的虚无假设**[@lakens_practical_2021]。为了说明这种差异，\@ref(fig:intervaltest) 中的面板A显示了双侧0零假设检验的预测结果，检验效应为0的假设是否能被拒绝。面板B为区间预测，预测的效应落在0.5-2.5之间，检验能否拒绝小于0.5或大于2.5的值。面板C为等价检验，它基本上与区间假设检验相同，但预测的效应落在0左右的范围内，其中的值被认为是太小而没有意义的效应。

(ref:intervaltestlab) Two-sided null hypothesis test (A), interval hypothesis test (B), equivalence test (C) and minimum effect test (D).

```{r, intervaltest, echo = FALSE, fig.height = 8, fig.cap="(ref:intervaltestlab)"}
plotheight <- 0.9
lowerbound <- -1
upperbound <- 1
df <- data.frame()
####  Base plot ------------------------------------
baseplot <-   ggplot(df) +
  scale_y_continuous(limits = c(0,plotheight+0.02), breaks=NULL) + # no y-axis will be displayed
  theme_classic() + 
  theme(plot.background = element_rect(fill = backgroundcolor))  + 
  theme(panel.background = element_rect(fill = backgroundcolor)) +
  theme(plot.title = element_text(size = rel(1), face = "bold"), #font size & appearance for plot titles
        axis.title.y = element_blank(), #remove title of y-axis
        axis.line.y= element_blank(),
        axis.title.x = element_text(size=rel(1), lineheight = 0.5), #font size for x-axis label
        plot.margin=unit(c(0.5,0.8,0.5,0.8),"cm")) #add padding around each plot to make them look nicer when combined; margin order: top, right, bottom, left

#NHST plot
NHSTplot <- baseplot +
  scale_x_continuous(limits = c(-5, 5), breaks=c(-4, -3, -2, -1, 0, 1, 2, 3, 4),
                     name = "observed difference") +
  ggtitle("A: Two-sided NHST") +
  annotate("segment", x = 0, xend = 0, y = plotheight-plotheight/2, yend = -Inf) + #vertical line at x=0 (H0)
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = 0.9, fill = "red", alpha = .2, color = NA) + #shading for H0 area
  annotate("text", size = rel(3.5), x=0, y = plotheight-plotheight/20, parse=TRUE, label="H0", hjust = 0.6) + #label for point null (H0)
  annotate("segment", x = 0, xend = 0, y = plotheight-plotheight/6, yend=plotheight-plotheight/2.3,
           arrow = arrow(type = "closed", length=unit(2, "mm"))) + #arrow pointing from H0 label to H0 line
  annotate("text", size = rel(3.5), x=-2.8, y=plotheight/3, parse=TRUE, label="H1") + #label for lower area (H1)
  annotate("text", size = rel(3.5), x = 2.8, y=plotheight/3, parse=TRUE, label="H1", hjust = 0.7) #label for upper area (H1)

rangeplot <- baseplot +
  scale_x_continuous(limits = c(-5, 5), breaks=c(-4, -3, -2, -1, 0, 1, 2, 3, 4),
                     name = "observed difference") +
  ggtitle("B: Interval Hypothesis Test") +
  annotate("segment", x = 0.5, xend = 0.5, y = plotheight, yend = -Inf, linetype = "dashed") + #dashed line for lower bound
  annotate("segment", x = 2.5, xend = 2.5, y = plotheight, yend = -Inf, linetype = "dashed") + #dashed line for upper bound
  annotate("rect", xmin = 0.5, xmax = 2.5, ymin = -Inf, ymax = 0.9, fill = "red", alpha = .2, color = NA) + #shading for H0 area
  annotate("text", size = rel(3.5), x=-0.8, y=plotheight/2.5, parse=TRUE, label= "H0") + #label for lower area (H1)
  annotate("text", size = rel(3.5), x=3.8, y=plotheight/2.5, parse=TRUE, label="H0") + #label for upper area (H1)
  annotate("text", size = rel(3.5), x=1.6, y=plotheight/2.5, parse=TRUE, label="H1", hjust = 0.7) #label for minimal effects area (H1)

equivalenceplot <- baseplot +
  scale_x_continuous(limits = c(-5, 5), breaks=c(-4, -3, -2, -1, 0, 1, 2, 3, 4),
                     name = "observed difference") +
  ggtitle("C: Equivalence Test") +
  annotate("segment", x = -1, xend = -1, y = plotheight, yend = -Inf, linetype = "dashed") + #dashed line for lower bound
  annotate("segment", x = 1, xend = 1, y = plotheight, yend = -Inf, linetype = "dashed") + #dashed line for upper bound
  annotate("rect", xmin = -1, xmax = 1, ymin = -Inf, ymax = 0.9, fill = "red", alpha = .2, color = NA) + #shading for H0 area
  annotate("text", size = rel(3.5), x=-2.8, y=plotheight/2.5, parse=TRUE, label= "H0") + #label for lower area (H1)
  annotate("text", size = rel(3.5), x=2.8, y=plotheight/2.5, parse=TRUE, label="H0") + #label for upper area (H1)
  annotate("text", size = rel(3.5), x=0, y=plotheight/2.5, parse=TRUE, label="H1", hjust = 0.6) #label for minimal effects area (H1)

mineffectplot <- baseplot +
  scale_x_continuous(limits = c(-5, 5), breaks=c(-4, -3, -2, -1, 0, 1, 2, 3, 4),
                     name = "observed difference") +
  ggtitle("D: Minimum Effect Test") +
  annotate("segment", x = -1, xend = -1, y = plotheight, yend = -Inf, linetype = "dashed") + #dashed line for lower bound
  annotate("segment", x = 1, xend = 1, y = plotheight, yend = -Inf, linetype = "dashed") + #dashed line for upper bound
  annotate("rect", xmin = -Inf, xmax = -1, ymin = -Inf, ymax = 0.9, fill = "red", alpha = .2, color = NA) + #shading for H0 area
  annotate("rect", xmin = 1, xmax = Inf, ymin = -Inf, ymax = 0.9, fill = "red", alpha = .2, color = NA) + #shading for H0 area
  annotate("text", size = rel(3.5), x=-2.8, y=plotheight/2.5, parse=TRUE, label= "H1") + #label for lower area (H1)
  annotate("text", size = rel(3.5), x=2.8, y=plotheight/2.5, parse=TRUE, label="H1") + #label for upper area (H1)
  annotate("text", size = rel(3.5), x=0, y=plotheight/2.5, parse=TRUE, label="H0", hjust = 0.6) + #label for minimal effects area (H1)
  annotate("segment", x = 0.1, xend = 1, y = plotheight/2.5, yend=plotheight/2.5,
           arrow = arrow(type = "closed", length=unit(2, "mm"))) + #arrow pointing from H0 label to upper H0 dashed line
  annotate("segment", x = -0.1, xend = -1, y = plotheight/2.5, yend=plotheight/2.5,
           arrow = arrow(type = "closed", length=unit(2, "mm")))  #arrow pointing from H0 label to lower H0 dashed line         



gridExtra::grid.arrange(NHSTplot, rangeplot, equivalenceplot, mineffectplot, ncol = 1) #combine plots in one column (all stacked)
```

When an equivalence test is reversed, a researcher designs a study to reject effects less extreme than a smallest effect size of interest (see Panel D in Figure \@ref(fig:intervaltest)), it is called a **minimum effect test** [@murphy_testing_1999]. A researcher might not just be interested in rejecting an effect of 0 (as in a null hypothesis significance test) but in rejecting a range of effects that are too small to be meaningful. All else equal, a study designed to have high power for a minimum effect requires more observations than if the goal had been to reject an effect of zero. As the confidence interval needs to reject a value that is closer to the observed effect size (e.g., 0.1 instead of 0) it needs to be more narrow, which requires more observations. 

将等价检验进行翻转，意味着研究者设计了一项研究来拒绝比最小关注效应量还小的那些值，这被称为**最小效应检验**[@murphy_testing_1999]。研究人员可能不仅想要拒绝一个效应为0的假设（如零假设显著性检验），而且也想拒绝那些太小而没有意义的效应范围。在其他条件相同的情况下，相比于那些想拒绝0效应假设的研究，旨在拒绝最小效应并具有高统计检验力的研究需要更多的观测值，因为后者的置信区间需要拒绝的值更接近观测到的效应量（例如，0.1而不是0），置信区间会变得更窄需要更加收缩，这需要更多的观测。

One benefit of a minimum effect test compared to a null hypothesis test is that there is no distinction between statistical significance and practical significance. As the test value is chosen to represent the minimum effect of interest, whenever it is rejected, the effect is both statistically and practically significant [@murphy_statistical_2014]. Another benefit of minimum effect tests is that, especially in correlational studies in the social sciences, variables are often connected through causal structures that result in real but theoretically uninteresting nonzero correlations between variables, which has been labeled the 'crud factor' [@meehl_appraising_1990; @orben_crud_2020]. Because an effect of zero is unlikely to be true in large correlational datasets, rejecting a nil null hypothesis is not a severe test. Even if the hypothesis is incorrect, it is likely that an effect of 0 will be rejected due to ['crud'](#crud). For this reason, some researchers have suggested to test against a minimum effect of *r* = 0.1, as correlations below this threshold are quite common due to theoretically irrelevant correlations between variables [@ferguson_providing_2021].

与零假设检验相比，最小效应检验的一个好处是在统计显著性和实际显著性之间没有区别。由于检验值被选择来表示最小目标效应，无论何时被拒绝，这种影响在统计上和实际上都是显著的[@murphy_statistical_2014]。最小效应检验的另一个好处是，在社会科学中的相关研究中，变量通常通过因果结构相互联系，导致变量之间存在真实但理论上不感兴趣的非零相关性，这被称为“混杂因素”（crud factor）[@meehl_appraising_1990; @orben_crud_2020]。由于0效应在大型相关数据集中不太可能成立，因此拒绝效应量为0的虚无假设并不是一个严格的检验。即使假设不正确，0效应的假设也可能因“混杂”而被拒绝。因此一些研究人员建议针对*r* = 0.1的最小效应进行检验，由于变量之间在理论上不相关，低于该阈值的相关性非常常见[@ferguson_providing_2021]。

Figure \@ref(fig:intervaltest) illustrates two-sided tests, but it is often more intuitive and logical to perform one-sided tests. In that case, a minimum effect test would, for example, aim to reject effects smaller than 0.1, and an equivalence test would aim to reject effects larger than for example 0.1. Instead of specifying an upper and lower bound of a range, it is sufficient to specify a single value for one-sided tests. A final variation of a one-sided non-nil null hypothesis test is known as a test for **non-inferiority**, which examines if an effect is larger than the lower bound of an equivalence range. Such a test is for example performed when a novel intervention should not be noticeably worse than an existing intervention, but it can be a tiny bit worse. For example, if a difference between a novel and existing intervention is not smaller than -0.1, and effects smaller than -0.1 can be rejected, one can conclude an effect is non-inferior [@schumi_through_2011; @mazzolari_myths_2022]. We see that extending nil null hypothesis tests to non-nil null hypotheses allow researchers to ask questions that might be more interesting.

图\@ref(fig:intervaltest)说明了双侧检验，但做单侧检验通常更直观、更合乎逻辑。例如，最小效应检验的目标是拒绝小于0.1的效应，而等价检验的目标是拒绝大于0.1的效应。与其指定范围的上限和下限，不如为单侧检验指定一个值。单侧效应量非0的虚无假设的最后一种变体被称为**非劣效性检验**，它检查效应是否大于等效范围的下限。例如，当一种新的干预措施不应该明显比现有的干预措施差，但可能会差一点点时，就会进行这样的测试。例如，如果新的干预措施和现有的干预措施之间的差异不小于-0.1，并且小于-0.1的效应可以被拒绝，则可以得出结论，效果是非劣效的[@schumi_through_2011; @mazzolari_myths_2022]。我们发现，将效应量为0的虚无假设扩展到效应量非0的虚无假设可以让研究人员提出可能更有趣的问题。
 
## Equivalence tests
## 等价检验

Equivalence tests were first developed in pharmaceutical sciences [@hauck_new_1984; @westlake_use_1972] and later formalized as the **two one-sided tests (TOST)** approach to equivalence testing [@schuirmann_comparison_1987; @seaman_equivalence_1998; @wellek_testing_2010]. The TOST procedure entails performing two one-sided tests to examine whether the observed data is surprisingly larger than a lower equivalence boundary ($\Delta_{L}$), or surprisingly smaller than an upper equivalence boundary ($\Delta_{U}$):

等价检验最早是在药物科学中发展起来的[@hauck_new_1984; @westlake_use_1972]，后来正式成为等价检验的**双单侧检验(TOST)**方法[@schuirmann_comparison_1987; @seaman_equivalence_1998; @wellek_testing_2010]。TOST程序需要进行两次单侧检验，以检验观察到的数据是否出乎意料地大于等效下限($\Delta_{L}$), 或者出乎意料地小于等效上限($\Delta_{U}$)：

$$
t_{L} = \frac{{\overline{M}}_{1} - {\overline{M}}_{2} - \Delta_{L}}{\sigma\sqrt{\frac{1}{n_{1}} + \frac{1}{n_{2}}}}
$$

and 

$$
t_{U} = \frac{{\overline{M}}_{1} - {\overline{M}}_{2}{- \Delta}_{U}}{\sigma\sqrt{\frac{1}{n_{1}} + \frac{1}{n_{2}}}}
$$

where *M* indicates the means of each sample, *n* is the sample size, and σ is
the pooled standard deviation:

其中**M**表示每个样本的平均值，**n**是样本量，σ是合并的标准偏差：

$$
\sigma = \sqrt{\frac{\left( n_{1} - 1 \right)\text{sd}_{1}^{2} + \left( n_{2} - 1 \right)\text{sd}_{2}^{2}}{n_{1} + \ n_{2} - 2}}
$$

If both one-sided tests are significant, we can reject the presence of effects large enough to be meaningful. The formulas are highly similar to the normal formula for the *t*-statistic. The difference between a NHST *t*-test and the TOST procedure is that the lower equivalence boundary $\Delta_{L}$ and the upper equivalence boundary $\Delta_{U}$ are subtracted from the mean difference between groups (in a normal *t*-test, we compare the mean difference against 0, and thus the delta drops out of the formula because it is 0).

如果这两个单侧检验都是显著的，我们可以拒绝足够大而有意义的效应的存在。这些公式与*t*统计量的正态公式高度相似。NHST *t*检验和TOST程序之间的区别在于，NHST *t*检验从组别之间的平均差中减去了等效下限和等效上限（在正常的*t*检验中，我们将平均差与0进行比较，因此∆从公式中删除，因为它是0）。

To perform an equivalence test, you don't need to learn any new statistical tests, as it is just the well-known *t*-test against a different value than 0. It is somewhat surprising that the use of *t*tests to perform equivalence tests is not taught alongside their use in null hypothesis significance tests, as there is some indication that this could prevent common misunderstandings of *p*-values [@parkhurst_statistical_2001]. Let's look at an example of an equivalence test using the TOST procedure. 

要进行等价检验，你不需要学习任何新的统计检验，因为它只是针对不同于0的值进行的众所周知的t检验。令人有些惊讶的是，使用*t*检验进行等价检验并没有与在零假设显著性检验中使用*t*检验一起进行教学，因为有一些迹象表明，这可以防止对*p*值的常见误解[@parkhurst_statistical_2001]。让我们来看一个使用TOST程序进行等价检验的例子。

In a study where researchers are manipulating fatigue by asking participants to carry heavy boxes around, the researchers want to ensure the manipulation does not inadvertently alter participants’ moods. The researchers assess positive and negative emotions in both conditions, and want to claim there are no differences in positive mood. Let’s assume that positive mood in the experimental fatigue condition ($m_1$ = 4.55, $sd_1$ = 1.05, $n_1$ = 15) did not differ from the mood in the the control condition ($m_2$ = 4.87, $sd_2$ = 1.11, $n_2$ = 15). The researchers conclude: “Mood did not differ between conditions, *t* = -0.81, *p* = .42”. Of course, mood did differ between conditions, as 4.55 - 4.87 = -0.32. The claim is that there was no *meaningful* difference in mood, but to make such a claim in a correct manner, we first need to specify which difference in mood is large enough to be meaningful. For now, let's assume the researcher consider any effect less extreme half a scale point too small to be meaningful. We now test if the observed mean difference of -0.32 is small enough such that we can reject the presence of effects that are large enough to matter. 

在一项研究中，研究人员通过让被试随身携带沉重的盒子来操纵疲劳，研究人员希望确保这种操作不会无意中改变被试的情绪。研究人员评估了这两种情况下的积极情绪和消极情绪，并希望能够声称积极情绪在两种情况下没有差异。让我们假设实验性疲劳条件下的积极情绪($m_1$ = 4.55, $sd_1$ = 1.05, $n_1$ = 15)与控制条件下的情绪($m_2$ = 4.87, $sd_2$ = 1.11, $n_2$ = 15)没有差异。研究人员得出结论：“不同条件下的情绪没有差异，*t*=-0.81，*p*=.42”。当然，不同条件下的情绪确实不同，因为4.55-4.87=-0.32。这种说法是指两种情绪的差异“无意义”，但要以正确的方式得出这样的结论，我们首先需要指定什么程度的情绪差异才能被视为是有意义的。目前，我们假设，研究者认为小于半个刻度点（即0.5）的效应都太小，因而没有意义。我们现在检验观察到的-0.32的平均差异是否足够小，以便我们可以拒绝大到要去重视的效应的存在。

The TOSTER package (originally created by myself but recently redesigned by [Aaron Caldwell](https://aaroncaldwell.us/)) can be used to plot two *t*-distributions and their critical regions indicating when we can reject the presence of effects smaller than -0.5 and larger than 0.5. It can take some time to get used to the idea that we are rejecting values more extreme than the equivalence bounds. Try to consistently ask in any hypothesis test: Which values can the test reject? In a nil null hypothesis test, we can reject an effect of 0, and in the equivalence test in the Figure below, we can reject values lower than -0.5 and higher than 0.5. In Figure \@ref(fig:tdistequivalence) we see two *t*-distributions centered on the upper and lower bound of the specified equivalence range (-0.5 and 0.5). 

TOSTER软件包（最初由我创建，但最近由[Aaron Caldwell](https://aaroncaldwell.us/)重新设计）可用于绘制两个*t*分布及其临界区域的图表，指示我们何时可以拒绝小于-0.5和大于0.5的效应。我们可能需要一些时间来习惯这样一种想法，即我们拒绝的值比等效边界更极端。在任何假设检验中，试着始终提问：检验可以拒绝哪些值？在效应量为0的虚无假设中，我们可以拒绝效应为0的假设，在下图的等价检验中，可以拒绝低于-0.5和高于0.5的值。在图\@ref(fig:tdistequivalence)中，我们看到两个*t*分布集中在指定等效范围的上限和下限（-0.5和0.5）。

(ref:tdistequivalencelab) The mean difference and its confidence interval plotted below the *t*-distributions used to perform the two-one-sided tests against -0.5 and 0.5.

```{r, tdistequivalence, echo = FALSE, fig.cap="(ref:tdistequivalencelab)"}
res <- TOSTER::tsum_TOST(m1 = 4.55, m2 = 4.87, sd1 = 1.05, sd2 = 1.11,
                  n1 = 15, n2 = 15, low_eqbound = -0.5, high_eqbound = 0.5)

plot(res, type = "tnull")

```

Below the two curves we see a line that represents the confidence interval ranging from `r round(res$effsize$lower.ci[1], 2)` to `r round(res$effsize$upper.ci[1], 2)`, and a dot on the line that indicates the observed mean difference of `r round(res$effsize$estimate[1], 2)`. Let's first look at the left curve. We see the green highlighted area in the tails that highlights which observed mean differences would be extreme enough to statistically reject an effect of -0.5. Our observed mean difference of -0.32 lies very close to -0.5, and if we look at the left distribution, the mean is not far enough away from -0.5 to fall in the green area that indicates when observed differences would be statistically significant. We can also perform the equivalence test using the TOSTER package, and look at the results. 

在这两条曲线下面，我们看到一条线表示 `r round(res$effsize$lower.ci[1], 2)` 至`r round(res$effsize$upper.ci[1], 2)`的置信区间，线上的一个点表示观察到的 `r round(res$effsize$estimate[1], 2)`的平均差异。让我们先看看左边的曲线。我们在尾部看到绿色突出显示区域，突出显示观察到的平均差异将非常极端，足以在统计上拒绝-0.5的效应。我们观察到的-0.32的平均差异非常接近-0.5，如果我们看左边的分布，平均值离-0.5不远，不足以落在表明观察到的差异何时具有统计学意义的绿色区域。我们还可以使用TOSTER软件包进行等价检验，并查看结果。

```{r}
TOSTER::tsum_TOST(m1 = 4.55, 
                  m2 = 4.87, 
                  sd1 = 1.05, 
                  sd2 = 1.11,
                  n1 = 15, 
                  n2 = 15, 
                  low_eqbound = -0.5, 
                  high_eqbound = 0.5)
```

In the line 't-test' the output shows the traditional nil null hypothesis significance test (which we already knew was not statistically significant: *t* = `r round(res$TOST$t[2], 2)`, *p* = `r round(res$TOST$p[1],2)`. Just like the default *t*-test in R, the tsum_TOST function will by default calculate Welch’s *t*-test (instead of Student’s *t*-test), which is a better default [@delacre_why_2017], but you can request Student’s *t*-test by adding `var.equal = TRUE` as an argument to the function.

在“t检验”一行中，输出结果显示了传统上效应量为0的虚无假设的显著性检验（我们已经知道这在统计学上并不显著：*t*=`r round(res$TOST$t[2], 2)`，p=`r round(res$TOST$p[1],2)`。就像R中的默认*t*检验一样，tsum_TOST函数在默认情况下会计算Welch’s *t*检验（而不是Student’s *t*检验），这是一个更好的默认值[@delacre_why_2017]，但你可以通过添加`var.equal = TRUE`作为函数的参数来请求Student’s *t*检验。

We also see a test indicated by TOST Lower. This is the first one-sided test examining if we can reject effects lower than -0.5. From the test result, we see this is not the case: *t* = `r round(res$TOST$t[2], 2)`, *p* = `r round(res$TOST$p[2],2)`. This is an ordinary *t*-test, just against an effect of -0.5. Because we cannot reject differences more extreme than -0.5, it is possible that a difference we consider meaningful (e.g., a difference of -0.60) is present. When we look at the one-sided test against the upper bound of the equivalence range (0.5) we see that we can statistically reject the presence of mood effects larger than 0.5, as in the line TOST Upper we see *t* = `r round(res$TOST$t[3], 2)`, *p* = `r round(res$TOST$p[3],2)`. Our final conclusion is therefore that, even though we can reject effects more extreme than 0.5 based on the observed mean difference of `r round(res$effsize$estimate[1], 2)`, we cannot reject effects more extreme than -0.5. Therefore, we cannot completely reject the presence of meaningful mood effects. As the data does not allow us to claim the effect is different from 0, nor that the effect is, if anything, too small to matter (based on an equivalence range from -0.5 to 0.5), the data are **inconclusive**. We cannot distinguish between a Type 2 error (there is an effect, but in this study we just did not detect it) or a true negative (there really is no effect large enough to matter).

我们还看到TOST Lower指示的检验。这是第一次单侧检验，检验我们是否可以拒绝低于-0.5的效应。从检验结果来看，情况并非如此：*t*=`r round(res$TOST$t[2], 2)`，*p*=`r round(res$TOST$p[2],2)`。这是一个普通的*t*检验，只是针对-0.5的效应。因为我们不能拒绝比-0.5更极端的差异，所以可能存在我们认为有意义的差异（例如，-0.60的差异）。当我们观察等价范围上限（0.5）的单侧检验时，我们可以从统计学上拒绝大于0.5的情绪效应的存在，正如在TOST upper行中我们看到的*t*=`r round(res$TOST$t[3], 2)`，*p*=`r round(res$TOST$p[3],2)`。因此，我们的最终结论是，即使我们可以根据观察到的-0.32的平均差异来拒绝比0.5更极端的效应，我们也不能拒绝比-0.5更极端的效应。因此，我们不能完全拒绝有意义的情绪效应的存在。由于数据不允许我们声称效应与0有所不同，也不允许我们说效应太小而无关紧要（基于-0.5到0.5的等效范围），因此数据是**不确定**的。我们无法区分Ⅱ类错误（存在效应，但在这项研究中，我们只是没有检测到它）或真正的阴性（确实没有足够大到要去重视的效应）。

Note that because we fail to reject the one-sided test against the lower equivalence bound, the possibility remains that there is a true effect size that is large enough to be considered meaningful. This statement is true, even when the effect size we have observed (`r round(res$effsize$estimate[1], 2)`) is closer to zero than to the equivalence bound of -0.5. One might think the observed effect size needs to be more extreme (i.e., < -0.5 or > 0.5) than the equivalence bound to maintain the possibility that there is an effect that is large enough to be considered meaningful. But that is not required. The 90% CI indicates that some values below -0.5 cannot be rejected. As we can expect that 90% of confidence intervals in the long run capture the true population parameter, it is perfectly possible that the true effect size is more extreme than -0.5. And, the effect might even be more extreme than the values captured by this confidence interval, as 10% of the time, the computed confidence interval is expected to not contain the true effect size. Therefore, when we fail to reject the smallest effect size of interest, we retain the possibility that an effect of interest exists. If we can reject the nil null hypothesis, but fail to reject values more extreme than the equivalence bounds, then we can claim there is an effect, and it might be large enough to be meaningful. 

请注意，由于我们未能拒绝针对等效下限的单侧检验，因此仍有可能存在足够大以至于被认为是有意义的真实效应量。这种说法是正确的，即使我们观察到的效应大小（`r round(res$effsize$estimate[1], 2)`）比-0.5的等效边界更接近于零。有人可能认为，观察到的效应大小需要比等效边界更极端（即<-0.5或>0.5），以保持存在足够大的效应以至于被认为是有意义的可能性。但这并不是必须的。90%的置信区间不能拒绝低于-0.5的某些值。正如我们可以预期的那样，从长远来看，90%的置信区间捕捉到了真实的总体参数，真实的效应大小完全有可能比-0.5更极端。而且，这种效应甚至可能比这个置信区间捕获的值更极端，因为在10%的时间里，计算的置信区间预计不包含真实的效应量。因此，当我们不能拒绝最小目标效应量时，我们保留了存在关注效应的可能性。如果我们可以拒绝效应量为0的虚无假设，但不能拒绝比等效边界更极端的值，那么我们可以声称效应存在，并且它可能足够大，大到有意义。

```{r, ciequivalencetest1, echo = FALSE}
res <- TOSTER::tsum_TOST(m1 = 4.55, m2 = 4.87, sd1 = 1.05, sd2 = 1.11,
                  n1 = 200, n2 = 200, low_eqbound = -0.5, high_eqbound = 0.5)
```


One way to reduce the probability of an inconclusive effect is to collect sufficient data. Let's imagine the researchers had not collected 15 participants in each condition, but 200 participants. They otherwise observe exactly the same data. As explained in the chapter on [confidence intervals](#confint), as the sample size increases, the confidence interval becomes more narrow. For a TOST equivalence test to be able to reject both the upper and lower bound of the equivalence range, the confidence interval needs to fall completely within the equivalence range. In Figure \@ref(fig:ciequivalence1) we see the same result as in Figure \@ref(fig:tdistequivalence), but now if we had collected 200 observations. Because of the larger sample size, the confidence is more narrow than when we collected 15 participants. We see that the 90% confidence interval around the observed mean difference now excludes both the upper and lower equivalence bound. This means that we can now reject effects outside of the equivalence range (even though barely, with a *p* = `r round(res$TOST$p[2],3)` as the one-sided test against the lower equivalence bound is only just statistically significant).

降低不确定效应概率的一种方法是收集充分的数据。让我们想象一下，研究人员并没有在每种情况下只收集15名被试，而是收集了200名被试。除此之外，他们观察到的数据完全相同。正如[confidence intervals](#confint)一章中所解释的，随着样本量的增加，置信区间变得越来越窄。为了使TOST等价检验能够拒绝等效范围的上限和下限，置信区间需要完全落在等效范围内。在图\@ref(fig:ciequivalence1)中，我们看到了与图\@ref(fig:tdistequivalence)相同的结果，但现在如果我们收集了200个观测结果。由于样本量较大，置信度比我们收集15名被试时更窄。我们看到，观察到的平均差周围的90%置信区间现在排除了等效上限和等效下限。这意味着我们现在可以拒绝等效范围之外的效应（尽管几乎没有，因为对等效下限的单侧检验仅具有统计学意义，*p*=`r round(res$TOST$p[2],3)`）。

(ref:ciequivalence1lab) The mean difference and its confidence interval for an equivalence test with an equivalence range of -0.5 and 0.5.

(ref:ciequivalence1lab) 等效范围为-0.5和0.5的等价检验的平均差及其置信区间
```{r, ciequivalence1, echo = FALSE, fig.cap="(ref:ciequivalence1lab)"}

plot(res, type = "tnull", estimates = "raw")

print(res) 
```

In Figure \@ref(fig:ciequivalence2) we see the the same results, but now visualized as a confidence density plot [@schweder_confidence_2016], which is a graphical summary of the distribution of confidence. A confidence density plot allows you to see which effects can be rejected with difference confidence interval widths. We see the bounds of the green area (corresponding to a 90% confidence interval) fall inside the equivalence bounds. Thus, the equivalence test is statistically significant, and we can statistically reject the presence of effects outside the equivalence range. We can also see that the 95% confidence interval excludes 0, and therefore, a traditional null hypothesis significance test is also statistically significant. 

在图\@ref(fig:ciequivalence2)中，我们看到了相同的结果，但现在可视化为置信密度图[@schweder_confidence_2016]，这是置信度分布的图形总结。置信密度图允许你查看哪些效应可以通过不同的置信区间宽度来拒绝。我们看到绿色区域的边界（对应于90%的置信区间）落在等效边界内。因此，等价检验在统计学上是显著的，我们可以在统计学上拒绝存在等效范围之外的效应。我们还可以看到，95%的置信区间排除了0，因此，传统的零假设显著性检验也具有统计学意义。

(ref:ciequivalence2lab) The mean difference and its confidence interval for an equivalence test with an equivalence range of -0.5 and 0.5.

(ref:ciequivalence2lab) 等效范围为-0.5和0.5的等价检验的平均差及其置信区间

```{r, ciequivalence2, echo = FALSE, fig.cap="(ref:ciequivalence2lab)"}
res <- TOSTER::tsum_TOST(m1 = 4.55, m2 = 4.87, sd1 = 1.05, sd2 = 1.11,
                  n1 = 200, n2 = 200, low_eqbound = -0.5, high_eqbound = 0.5)

plot(res, type = "cd", estimates = "raw")
```

In other words, both the null hypothesis test and the equivalence test have yielded significant results. This means we can claim that the observed effect is statistically different from zero, and that the effect is statistically smaller than effects we deemed large enough to matter when we specified the equivalence range from -0.5 to 0.5. This illustrates how combining equivalence tests and nil null hypothesis tests can prevent us from mistaking statistically significant effects for practically significant effects. In this case, with 200 participants, we can reject an effect of 0, but the effect, if any, is not large enough to be meaningful.

换句话说，零假设检验和等价检验都产生了显著的结果。这意味着我们可以声称，观察到的效应在统计上与0不同，并且在统计上，当我们指定-0.5到0.5的等效范围时，该效应小于我们认为足够大的效应。这说明了将等价检验和效应量为0的虚无假设相结合可以防止我们误将具有统计学意义的效应当成实际上显著的效应。在这种情况下，有200名被试，我们可以拒绝一个为0的效应，但这个效应（如果有的话）没有大到是有意义的。

## Reporting Equivalence Tests 

It is common practice to only report the test yielding the higher *p*-value of the two one-sided tests when reporting an equivalence test. Because both one-sided tests need to be statistically significant to reject the null hypothesis in an equivalence test (i.e., the presence of effects large enough to matter), when the larger of the two hypothesis tests rejects the equivalence bound, so does the other test. Unlike in null hypothesis significance tests it is not common to report standardized effect sizes for equivalence tests, but there can be situations where researchers might want to discuss how far the effect is removed from the equivalence bounds on the raw scale. Prevent the erroneous interpretation to claim there is 'no effect', that an effect is 'absent', that the true effect size is 'zero', or vague verbal descriptions, such as that two groups yielded 'similar' or 'comparable' data. A significant equivalence test rejects effects more extreme that the equivalence bounds. Smaller true effects have not been rejected, and thus it remains possible that there is a true effect. Because a TOST procedure is a frequentist test based on a *p*-value, all other [misconceptions of *p*-values](#misconceptions) should be prevented as well. 

在报告等价检验时，通常只报告两个单侧检验中产生较高*p*值的检验。因为两个单侧检验都需要具有统计学意义，才能在等价检验中拒绝零假设（即存在足够大的效应），所以当两个假设检验中较大的一个拒绝等效边界时，另一个检验也是如此。与零假设显著性检验不同，报告等价检验的标准化效应量并不常见，但在某些情况下，研究人员可能想讨论在原始尺度上，效应与等效边界的差距有多大。防止例如声称‘没有效应’、效应‘不存在’、真实效应量为‘0’这样错误的解释，或例如两组得出的数据“相似”或“可比”这样模糊的口头描述。显著的等价检验拒绝比等效边界更极端的效应。较小的真实效应没有被拒绝，因此仍然有可能存在真实效应。因为TOST程序是一种基于*p*值的频率检验，所以也应该防止所有其他[对*p*值的误解](#misconceptions)。

When summarizing the main result of an equivalence test, for example in an abstract, always report the equivalence range that the data is tested against. Reading 'based on an equivalence test we concluded the absence of a meaningful effect' means something very different if the equivalence bounds were *d* =-0.9 to 0.9 than when the bounds were *d* =-0.2 to *d* =0.2. So instead, write 'based on an equivalence test with an equivalence range of *d* =-0.2 to 0.2, we conclude the absence of an effect we deemed meaningful'. Of course, whether peers agree you have correctly concluded the absence of a meaningful effect depends on whether they agree with your justification for a smallest effect of interest! A more neutral conclusion would be a statement such as: 'based on an equivalence test, we rejected the presence of effects more extreme than -0.2 to 0.2, so we can act (with an error rate of alpha) as if the effect, if any, is less extreme than our equivalence range'. Here, you do not use value-laden terms such as 'meaningful'. If both a null hypothesis test and an equivalence test are non-significant, the finding is best described as 'inconclusive': There is not enough data to reject the null, or the smallest effect size of interest. If both the null hypothesis test and the equivalence test are statistically significant, you can claim there is an effect, but at the same time claim the effect is too small to be of interest (given your justification for the equivalence range).

在总结等价检验的主要结果时，例如在摘要中，始终要报告数据所检验的等效范围。与边界为*d* = -0.2至*d*= 0.2时相比，如果等效边界为*d* = -0.9至0.9，读到‘基于等价检验，我们得出结论，有意义的效应不存在’这句话包含着非常不同的意义。反之，应当写下‘基于等效范围为*d*=-0.2至0.2的等价检验，我们得出结论，我们认为有意义的效应不存在’。当然，同行们是否同意你正确地得出了有意义效应不存在的结论，取决于他们是否同意你对最小目标效应的证明！一个更中性的陈述是这样的：“基于等价检验，我们拒绝了比-0.2到0.2更极端效应的存在，所以我们可以采取行动（错误率为α），就好像这种效应（如果有的话）没有我们的等效范围那么极端一样”。在这里，你不使用诸如‘有意义’之类的充满价值的术语。如果零假设检验和等价检验都是不显著的，那么这一发现最好被描述为‘不确定的’：没有足够的数据来拒绝零假设，或者最小目标效应量。如果零假设检验和等价检验都具有统计学意义，你可以声称效应存在，但同时声称效应太小，不值得关注（考虑到你对等效范围的证明）。

Equivalence bounds can be specified in raw effect sizes, or in standardized mean differences. It is better to specify the equivalence bounds in terms of raw effect sizes. Setting them in terms of Cohen's *d* leads to bias in the statistical test, as the observed standard deviation has to be used to translate the specified Cohen's *d* into a raw effect size for the equivalence test (and when you set equivalence bounds in standardized mean differences, TOSTER will warn: "Warning: setting bound type to SMD produces biased results!"). The bias is in practice not too problematic in any single equivalence test, and being able to specify the equivalence bounds in standardized mean differences lowers the threshold to perform an equivalence test when they do not know the standard deviation of their measure. But as equivalence testing becomes more popular, and fields establish smallest effect sizes of interest, they should do so in raw effect size differences, not in standardized effect size differences. 

等效边界可以在原始效应量中指定，也可以在标准化平均差中指定。最好根据原始效果量来指定等效边界。根据Cohen’s *d*设置它们会导致统计检验中的偏差，因为必须使用观察到的标准差将指定的Cohen‘s *d*转换为等价检验的原始效应量（当你在标准化平均差中设置等效边界时，TOSTER将警告：“警告：将边界类型设置为SMD会产生偏差结果！”）。在实践中，偏差在任何单一的等价检验中都不会有太大的问题，并且能够在标准化平均差中指定等效边界，这降低了当他们不知道其度量的标准差时进行等价检验的阈值。但是，随着等价检验变得越来越流行，并且领域建立了最小目标效应量，他们应该在原始效应量差异中这样做，而不是在标准化效应量差异这样做。

## Minimum Effect Tests{#MET}
## 最小等效检验

If a researcher has specified a smallest effect size of interest, and is interested in testing whether the effect in the population is larger than this smallest effect of interest, a minimum effect test can be performed. As with any hypothesis test, we can reject the smallest effect of interest whenever the confidence interval around the observed effect does not overlap with it. In the case of a minimum effect test, however, the confidence interval should be fall completely beyond the smallest effect size of interest. For example, let's assume a researcher performs a minimum effect test with 200 observations per condition against a smallest effect size of interest of a mean difference of 0.5.

如果研究人员指定了最小目标效应量，并且有兴趣检验群体中的效应是否大于该最小目标效应，则可以进行最小效应检验。与任何假设检验一样，只要观察到的效应与其周围的置信区间不重叠，我们就可以拒绝最小目标效应。然而，在最小效应检验的情况下，最小目标效应应该完全落在置信区间外。例如，让我们假设一名研究人员对平均差异为0.5的最小效应量进行最小效应检验，每个条件有200个观察结果。

(ref:tmetlab) The mean difference and its confidence interval plotted below the *t*-distributions used to perform the two-one-sided tests against -0.5 and 0.5 when performing a minimum effect test.
(ref:tmetlab) 在进行最小效应检验时，用于对-0.5和0.5进行两次单侧检验的*t*分布下方绘制的平均差及其置信区间

```{r, tmet, echo = FALSE, fig.cap="(ref:tmetlab)"}
res <- TOSTER::tsum_TOST(m1 = 5.73, m2 = 4.87, sd1 = 1.05, sd2 = 1.11,
                  n1 = 200, n2 = 200, low_eqbound = -0.5, high_eqbound = 0.5,
                  hypothesis = "MET")

plot(res, type = "tnull")
print(res)
```

Below the two curves we again see a line that represents the confidence interval ranging from `r round(res$effsize$lower.ci[1], 2)` to `r round(res$effsize$upper.ci[1], 2)`, and a dot on the line that indicates the observed mean difference of `r round(res$effsize$estimate[1], 2)`. The entire confidence interval lies well above the minimum effect of 0.5, and we can therefore not just reject the nil null hypothesis, but also effects smaller than the minimum effect of interest. Therefore, we can claim that the effect is large enough to be not just statistically significant, but also practically significant (as long as we have justified our smallest effect size of interest well). Because we have performed a two-sided minimum effect test, the minimum effect test would also have been significant if the confidence interval had been completely on the opposite side of -0.5. 

在这两条曲线下面，我们再次看到一条线，它代表的置信区间从`r round(res$effsize$lower.ci[1], 2)`至`r round(res$effsize$upper.ci[1], 2)`，以及表示观察到的`r round(res$effsize$estimate[1], 2)`的平均差的线上的点。整个置信区间远高于0.5的最小效应，因此我们不仅可以拒绝效应量为0的虚无假设，而且可以拒绝小于最小目标效应的效应。因此，我们可以声称这种效应足够大，不仅在统计上具有显著性，而且在实践中也具有显著性（只要我们很好地证明了我们最小目标效应量）。因为我们进行了双侧最小效应检验，如果置信区间完全在-0.5的相反侧，最小效应检验也会很显著。

Earlier we discussed how combining traditional NHST and an equivalence test could lead to more informative results. It is also possible to combine a minimum effect test and an equivalence test. One might even say that such a combination is the most informative test of a prediction whenever a smallest effect size of interest can be specified. In principle, this is true. As long as we are able to collect enough data, we will always get an informative and straightforward answer when we combine a minimum effect test with an equivalence test: Either we can reject all effects that are too small to be of interest, or we can reject all effects that are large enough to be of interest. As we will see below in the section on power analysis for interval hypotheses, whenever the true effect size is close to the smallest effect size of interest, a large amount of observations will need to be collected. And if the true effect size happens to be identical to the smallest effect size of interest, neither the minimum effect test nor the equivalence test can be correctly rejected (and any significant test would be a Type 1 error). If a researcher can collect sufficient data (so that the test has high statistical power), and is relatively confident that the true effect size will be larger or smaller than the smallest effect of interest, then the combination of a minimum effect test and an equivalence test can be attractive as such a hypothesis test is likely to yield an informative answer to the research question. 

早些时候，我们讨论了如何将传统的NHST和等价检验相结合，从而获得信息更加丰富的结果。我们也可以将最小效应检验和等价检验相结合。甚至可以说，无论何时，只要可以指定最小目标效应大小，这种组合都是让预测的信息最为丰富的检验。原则上，这是真的。只要我们能够收集到足够的数据，当我们将最小效应检验与等价检验相结合时，我们总是会得到一个信息丰富、直截了当的答案：要么我们可以拒绝所有太小而不关注的效应，要么我们可以拒绝所有足够大而关注的效应。正如我们将在下面关于区间假设的统计检验力分析一节中看到的那样，每当真实效应量接近最小目标效应量时，都需要收集大量的观测结果。如果真实效应量恰好与最小目标效应量相同，则最小效应检验和等价检验都不能被正确拒绝（任何显著的检验都将是Ⅰ型错误）。如果研究人员能够收集充分的数据（从而使检验具有很高的统计检验力），并且相对确信真实效应量将大于或小于最小目标效应，那么最小效应检验和等价检验的组合可能很有吸引力，因为这样的假设检验可能会为研究问题提供信息丰富的答案。

## Power Analysis for Interval Hypothesis Tests
## 区间假设检验的检验力分析

When designing a study it is a sensible strategy to always plan for both the presence and the absence of an effect. Several scientific journals require a sample size justification for Registered Reports where the statistical power to reject the null hypothesis is high, but where the study is also capable of demonstrating the absence of an effect, for example by also performing a power analysis for an equivalence test. As we saw in the chapter on [error control](#errorcontrol) and [likelihoods](#likelihoods) null results are to be expected, and if you only think about the possibility of observing a null effect when the data has been collected, it is often too late.

在设计研究时，一种明智的策略是始终同时计划效应存在与不存在两种情况。一些科学期刊要求注册报告提供样本量的合理性证明，对于这些注册报告，其拒绝零假设的统计检验力很高，但研究也能够证明其不存在影响效应。例如，同时对等效测试进行检验力分析。正如我们在误差控制和似然性的章节中看到的那样，零结果是意料之中的，如果您在收完数据后才考虑观察到零效应的可能性，通常为时已晚。

The statistical power for interval hypotheses depend on the alpha level, the sample size, the smallest effect of interest you decide to test against, and the true effect size. For an equivalence test, it is common to perform a power analysis assuming the true effect size is 0, but this might not always be realistic. The closer the expected effect size is to the smallest effect size of interest, the larger the sample size needed to reach a desired power. Don't be tempted to assume a true effect size of 0, if you have good reason to expect a small but non-zero true effect size. The sample size that the power analysis indicates you need to collect might be smaller, but in reality you also have a higher probability of an inconclusive result. Earlier versions of TOSTER only enabled researchers to perform power analyses for equivalence tests assuming a true effect size of 0, but a new power function by Aaron Caldwell allows users to specify `delta`, the expected effect size.

区间假设的统计检验力取决于alpha水平、样本量、您决定检验的最小目标效应以及真实效应大小。对于等价检验，通常假定真实效应大小为0来执行检验力分析，但这可能并不总是现实的。预期效应量越接近最小目标效应量，需要达到所需检验力的样本量就越大。如果您有充分的理由预期一个小但非零的真实效应量，请不要试图假定真实效应量为0。检验力分析可能会表明您需要收集的样本量较小，但实际上您也更有可能得到不确定的结果。早期版本的 TOSTER 仅允许研究人员在假设真实效应大小为 0 的情况下对等价检验执行检验力分析，但 Aaron Caldwell 的新检验力函数允许用户指定 `delta`，即预期的效应量。

Assume a researcher desired to achieve 90% power for an equivalence test with an equivalence range from -0.5 to 0.5, with an alpha level of 0.05, and assuming a population effect size of 0. A power analysis for an equivalence test can be performed to examine the required sample size. 

假设研究人员希望等价检验达到 90% 的检验力，等价范围为 -0.5 到 0.5，alpha 水平为 0.05，并假设总体效应量为 0。可以进行等价检验的检验力分析，从而确定所需的样本量。

```{r}
TOSTER::power_t_TOST(power = 0.9, delta = 0,
                     alpha = 0.05, type = "two.sample",
                     low_eqbound = -0.5, high_eqbound = 0.5)
```

We see that the required sample size is 88 participants in each condition for the independent *t*-test. Let's compare this power analysis to a situation where the researcher expects a true effect of *d* = 0.1, instead of a true effect of 0. To be able to reliably reject effects larger than 0.5, we will need a larger sample size, just as how we need a larger sample size for a null hypothesis test powered to detect *d* = 0.4 than a null hypothesis test powered to detect *d* = 0.5. 

我们看到，对于独立样本*t*检验，所需的样本量为每个条件88名被试。现在，让我们将这个检验力分析与研究人员预期真实效应为*d*= 0.1的情况进行比较，而不是真实效应为0。为了能够可靠地拒绝大于0.5的效应，我们将需要更大的样本量，就像我们需要更大的样本量去测出*d* = 0.4的零假设检验，而不是*d* = 0.5的零假设一样。

```{r}
TOSTER::power_t_TOST(power = 0.9, delta = 0.1,
                     alpha = 0.05, type = "two.sample",
                     low_eqbound = -0.5, high_eqbound = 0.5)
```

We see the sample size has now increased to 109 participants in each condition. As mentioned before, it is not necessary to perform a two-sided equivalence test. It is also possible to perform a one-sided equivalence test. An example of a situation where such a directional test is appropriate is a replication study. If a previous study observed an effect of *d* = 0.48, and you perform a replication study, you might decide to consider any effect smaller than *d* = 0.2 a failure to replicate - including any effect in the opposite direction, such as an effect of *d* = -0.3. Although most software for equivalence tests requires you to specify an upper and lower bound for an equivalence range, you can mimic a one-sided test by setting the equivalence bound in the direction you want to ignore to a low value so that the one-sided test against this value will always be statistically significant. This can also be used to perform a power analysis for a minimum effect test, where one bound is the minimum effect of interest, and the other bound is set to an extreme value on the other side of the expected effect size. 

我们看到，样本量现在增加到每个条件109名被试。如前所述，并不需要执行双侧等价检验。执行单侧等价检验也是有可能的。单侧等价检验适用的一个例子是重复性研究。如果之前的研究观察到*d* = 0.48的效应，并且您执行了一项重复性研究，您可能决定将任何小于*d* = 0.2的效应视为重复失败——包括任何相反方向的效应，例如*d* = -0.3的效应。虽然大多数等价检验软件需要您为等价范围指定一个上限和下限，但您可以通过将您想要忽略的方向的等价界限设置为一个低值来模拟单侧检验，使得对这个值的单侧检验始终具有统计学意义。这也可以用来执行最小效应检验的检验力分析，其中一个界限是最小目标效应，另一个界限则设置为预期效应量的另一侧的极端值。

In the power analysis for an equivalence test example below, the lower bound is set to -5 (it should be set low enough such that lowering it even further has no noticeable effect). We see that the new power function in the TOSTER package takes the directional prediction into account, and just as with directional predictions in a nil null hypothesis test, a directional prediction in an equivalence test is more efficient, and only 70 observations are needed to achieve 90% power. 

在下面的等价检验的检验力分析示例中，下限被设定为-5（应该将其设置得足够低，以便进一步降低它不会对结果有明显影响）。我们可以看到TOSTER软件包中的新检验力函数考虑了方向性预测，与在效应量为0的虚无假设中的方向性预测一样，等价检验中的方向性预测更有效，且只需要70个观测值即可达到90%的检验力。

```{r}
# New TOSTER power functions allows power for expected non-zero effect.
TOSTER::power_t_TOST(power = 0.9, delta = 0,
                     alpha = 0.05, type = "two.sample",
                     low_eqbound = -5, high_eqbound = 0.5)
```

Statistical software offers options for power analyses for some statistical tests, but not for all tests. Just as with power analysis for a nil null hypothesis test, it can be necessary to use a simulation-based approach to power analysis.

统计软件为某些统计检验提供了检验力分析选项，但并非对所有检验都具备此功能。正如在0零假设检验中进行检验力分析一样，有必要使用基于模拟的方法进行检验力分析。

## The Bayesian ROPE procedure{#ROPE}
## 贝叶斯ROPE程序{#ROPE}

In Bayesian estimation, one way to argue for the absence of a meaningful effect is the **region of practical equivalence** (ROPE) procedure (@kruschke_bayesian_2013), which is “somewhat analogous to frequentist equivalence testing” (@kruschke_bayesian_2017). In the ROPE procedure, an equivalence range is specified, just as in equivalence testing, but the Bayesian highest density interval based on a posterior distribution (as explained in the chapter on [Bayesian statistics](#bayes)) is used instead of the confidence interval. 

在贝叶斯估计中，一种论证缺乏有意义效应的方法是使用**实用等效区间**（ROPE）程序(@kruschke_bayesian_2013)，它“有点类似于频率学派的等价检验”(@kruschke_bayesian_2017)。在ROPE程序中，指定等价范围，就像在等价检验中一样，但是基于后验分布的贝叶斯最高密度区间（如在[贝叶斯统计](#贝叶斯)) 的章节中所解释的）被用来代替置信区间。

If the prior used by Kruschke was perfectly uniform, and the ROPE procedure and an equivalence test used the same confidence interval (e.g., 90%), the two tests would yield identical results. There would only be philosophical differences in how the numbers are interpreted. The `BEST` package in R that can be used to perform the ROPE procedure by default uses a ‘broad’ prior, and therefore results of the ROPE procedure and an equivalence test are not exactly the same, but they are very close. One might even argue the two tests are 'practically equivalent'. In the R code below, random normally distributed data for two conditions is generated (with means of 0 and a standard deviation of 1) and the ROPE procedure and a TOST equivalence test are performed. 

如果Kruschke使用的先验分布完全均匀，并且ROPE程序和等价检验使用相同的置信区间（例如90％），那么两个检验将产生相同的结果。在如何解释数字方面只会存在哲学上的差异。在R中可以使用`BEST` 软件包执行ROPE程序，该软件包默认使用“广泛”的先验分布，因此ROPE程序和等价检验的结果并不完全相同，但它们非常接近。有人甚至可能会提出这两个检验“实际上是等价的”这样的说法。下面的R代码生成了两个条件下的随机正态分布数据（均值为0，标准差为1），并执行了ROPE程序和TOST等价检验。

```{r, echo = FALSE, cache = TRUE}
set.seed(1)

x<-rnorm(100) #Generate 100 random normally distributed observations
y<-rnorm(100) #Generate 100 random normally distributed observations

#ROPE test
BESTout<-BEST::BESTmcmc(x,y)
plot(BESTout, ROPE = c(-0.5, 0.5), showCurve = TRUE, xlim = c(-0.5,0.6),
     credMass = 0.90)

#TOST test
TOSTout <- TOSTER::t_TOST(x = x, y = y, low_eqbound = -0.5, high_eqbound = 0.5, alpha = 0.05)

plot(TOSTout, estimates = "raw")

```

The 90% HDI ranges from -0.06 to 0.39, with an estimated mean based on the prior and the data of 0.164. The HDI falls completely between the upper and the lower bound of the equivalence range, and therefore values more extreme than -0.5 or 0.5 are deemed implausible. The 95% CI ranges from `r round(TOSTout$effsize$lower.ci[1], 2)` to `r round(TOSTout$effsize$upper.ci[1], 2)` with an observed mean difference of `r round(TOSTout$effsize$estimate[1], 2)`. We see that the numbers are not identical, because in Bayesian estimation the observed values are combined with a prior, and the mean estimate is not purely based on the data. But the results are very similar, and will in most cases lead to similar inferences. The BEST R package also enables researchers to perform simulation based power analyses, which take a long time but, when using a broad prior, yield a result that is basically identical to the sample size from a power analysis for an equivalence test. The biggest benefit of ROPE over TOST is that it allows you to incorporate prior information. If you have reliable prior information, ROPE can use this information, which is especially useful if you don’t have a lot of data. If you use informed priors, check the robustness of the posterior against reasonable changes in the prior in sensitivity analyses.

90% HDI范围为-0.06到0.39，基于先验和数据估计的平均值为0.164。HDI完全落在等效范围的上限和下限之间，因此超过-0.5或0.5的值被认为不可信的。95% CI范围为`r round(TOSTout$effsize$lower.ci[1], 2)`到`r round(TOSTout$effsize$estimate[1], 2)`，观察到的平均差为0.15。我们看到这些数字不是完全相同的，因为在贝叶斯估计中，观察到的值与先验结合，平均估计值不仅仅基于数据。但结果非常相似，并且在大多数情况下会导致相似的推论。BEST R软件包还使研究人员能够执行基于模拟的检验力分析，这需要很长时间，但是使用广泛的先验时，结果与等价检验的检验力分析的样本量基本相同。ROPE相对于TOST的最大优势在于它允许您纳入先验信息。如果您具有可靠的先验信息，ROPE可以使用此信息，这在您没有大量数据时尤其有用。如果您使用了知情先验，建议进行敏感性分析，检查后验在先验合理变化下的稳健性。

## Which interval width should be used?{#whichinterval}
## 应该使用哪个区间宽度？

Because the TOST procedure is based on two one-sided tests, a 90% confidence interval is used when the one-sided tests are performed at an alpha level of 5%. Because both the test against the upper bound and the test against the lower bound needs to be statistically significant to declare equivalence (which as explained in the chapter on [error control](#multiplecomparisons) is an intersection-union approach to multiple testing) it is not necessary to correct for the fact that two tests are performed. If the alpha level is adjusted for multiple comparisons, or if the alpha level is justified instead of relying on the default 5% level (or both), the corresponding confidence interval should be used, where CI = 100 - (2 * $\alpha$). Thus, the width of the confidence interval is directly related to the choice for the alpha level, as we are making decisions to reject the smallest effect size of interest, or not, based on whether the confidence interval excluded the effect that is tested against.

因为 TOST 程序基于两个单侧检验，所以当在 5% 的 alpha 水平下执行单侧检验时，将使用 90% 的置信区间。因为针对上限的检验和针对下限的检验都需要具有统计显著性才能声明等价（正如在误差控制一章中所解释的那样，等价是对多重检验的交集-并集方法），所以不必为进行了两次检验而校正。如果针对多重比较调整了 alpha 水平，或者如果 alpha 水平是合理的而不是依赖于默认的 5% 水平（或两者），则应使用相应的置信区间，其中CI = 100 - (2 * $\alpha$)。因此，置信区间的宽度与alpha水平的选择直接相关，因为我们基于置信区间是否排除所检验的效应来决定是否拒绝最小目标效应量。

When using a Highest Density Interval from a Bayesian perspective, such as the ROPE procedure, the choice for a width of a confidence interval does not follow logically from a desired error rate, or any other principle. Kruschke [-@kruschke_doing_2014] writes: “How should we define 'reasonably credible'? One way is by saying that any points within the 95% HDI are reasonably credible.” McElreath [-@mcelreath_statistical_2016] has recommended the use of 67%, 89%, and 97%, because "No reason. They are prime numbers, which makes them easy to remember.". Both these suggestions lack a solid justification. As Gosset (or Student), observed [-@gosset_application_1904]: 

当从贝叶斯角度使用最高密度区间时，比如ROPE程序，置信区间宽度的选择在逻辑上不符合所需的错误率或任何其他原则。Kruschke[-@kruschke_doing_2014]写道：“我们应该如何定义‘合理可信’？一种方法是说，任何在95% HDI内的点都是合理可信的。”McElreath [-@mcelreath_statistical_2016]推荐使用67%、89%和97%，因为“没有理由。它们是质数，因此很容易记住。” 这两种建议都缺乏坚实的依据。正如Gosset（或学生）观察到的[-@gosset_application_1904]：

>Results are only valuable when the amount by which they probably differ from the truth is so small as to be insignificant for the purposes of the experiment. What the odds selected should be depends-  
1. On the degree of accuracy which the nature of the experiment allows, and  
2. On the importance of the issues at stake.

>结果仅在它们可能与真相相差的程度足够小以至于在实验目的上可以忽略不计时才有价值。选定的机率应取决于以下两点：
1.实验允许的精度程度，
2.相关问题的重要性。

There are only two principled solutions. First, if a highest density interval width is used to make claims, these claims will be made with certain error rates, and researchers should quantify the risk of erroneous claims by computing frequentist error rates. This would make the ROPE procedure a Bayesian/Frequentist compromise procedure, where the computation of a posterior distribution allows for Bayesian interpretations of which parameters values are believed to be most probable, while decisions based on whether or not the HDI falls within an equivalence range have a formally controlled error rate. Note that when using an informative prior, an HDI does not match a CI, and the error rate when using an HDI can only be derived through simulations. The second solution is to not make any claims, present the full posterior distribution, and let readers draw their own conclusions.

有两种原则性的解决方案。首先，如果使用最高密度区间宽度来做出声明，这些声明将具有一定的错误率，且研究人员应该通过计算频率主义的错误率来量化错误声明的风险。这将使ROPE程序成为贝叶斯/频率主义的折中程序，其中后验分布的计算允许贝叶斯解释哪些参数值被认为是最有可能的，而基于 HDI 是否落在等价范围内的决策具有 一个规范控制的错误率。请注意，当使用信息先验时，HDI与CI不匹配，并且使用HDI时的错误率只能通过模拟来推导。第二种解决方案是不做任何声明，呈现完整的后验分布，并让读者自己得出结论。

## Setting the Smallest Effect Size of Interest{#sesoi}
## 设置最小目标效应量

To be able to falsify our predictions using an equivalence test is to specify which observed values would be too small to be predicted by our theory. We can never say that an effect is exactly zero, but we can examine whether observed effects are too small to be theoretically or practically interesting. This requires that we specify the **smallest effect size of interest** (SESOI). The same concept goes by many names, such as a minimal important difference, or clinically significant difference [@king_point_2011]. Take a moment to think about what the smallest effect size is that you would still consider theoretically or practically meaningful for the next study you are designing. It might be difficult to determine what the smallest effect size is that you would consider interesting, and the question what the smallest effect size of interest is might be something you have never really thought about to begin with. However, determining your smallest effect size of interest has important practical benefits. First, if researchers in a field are able to specify which effects would be too small to matter, it becomes very straightforward to power a study for the effects that are meaningful. The second benefit of specifying the smallest effect size of interest is that it makes your study falsifiable. Having your predictions falsified by someone else might not feel that great for you personally, but it is quite useful for science as a whole [@popper_logic_2002]. After all, if there is no way a prediction can be wrong, why would anyone be impressed if the prediction is right? 

能够使用等价检验来验证我们的预测是否正确，就需要明确规定哪些观察值太小而无法用我们的理论预测。我们永远无法说效应完全为零，但我们可以检查观察到的效应是否太小而不具备理论或实际上的重要性。这需要我们指定**最小目标效应量**（SESOI）。同样的概念有许多名称，比如最小重要差异或临床显著差异[@king_point_2011]。花些时间思考一下，对于您正在设计的下一项研究，最小效应量是多少才可以被认为是理论或实际上有意义的？确定您最小目标效应量可能很困难，并且最小目标效应量是多少，这个问题可能是您一开始从未真正想过的。然而，确定您最小目标效应量对于实践有重要的好处。首先，如果某个领域的研究人员能够确定哪些效应太小而不重要，那么就可以非常直接地为有意义的效应确定研究所需的检验力。其次，指定最小目标效应量的好处是可以使您的研究具有可证伪性。您的预测被别人证伪对您个人来说可能感觉不太好，但对整个科学来说却非常有用（Popper，2002）。毕竟，如果没有任何方法可以证明预测是错误的，谁会在意预测是否正确呢？

To start thinking about which effect sizes matter, ask yourself whether *any* effect in the predicted direction is actually support for the alternative hypothesis. For example, would an effect size of a Cohen's *d* of 10 be support for your hypothesis? In psychology, it should be rare that a theory prediucts such a huge effect, and if you observed a *d* = 10, you would probably check for either a computation error, or a confound in the study. On the other end of the scale, would an effect of *d* = 0.001 be in line with the theoretically proposed mechanism? Such an effect is incredibly small, and is well below what an individual would notice, as it would fall below the **just noticeable difference** given perceptual and cognitive limitations. Therefore, a *d* = 0.001 would in most cases lead researchers to conclude "Well, this is really too small to be something that my theory has predicted, and such a small effect is practically equivalent to the absence of an effect." However, when we make a directional prediction, we say that these types of effects are all part of our alternative hypothesis. Even though many researchers would agree such tiny effects are too small to matter, they still officially support for our alternative hypothesis if we have a directional prediction with a nil null hypothesis. Furthermore, researchers rarely have the resources to statistically reject the presence of effects this small, so the claim that such effects would still support a theoretical prediction makes the theory **practically unfalsifiable**: A researcher could simply respond to any replication study showing a non-significant small effect (e.g., *d* = 0.05) by saying: "That does not falsify my prediction. I suppose the effect is just a bit smaller than *d* = 0.05", without ever having to admit the prediction is falsified. This is problematic, because if we do not have a process of replication and falsification, a scientific discipline risks a slide towards the unfalsifiable [@ferguson_vast_2012]. So whenever possible, when you design an experiment or you have a theory and a theoretical prediction, carefully think about, and clearly state, what the smallest effect size of interest is. 

开始思考哪些效应量是重要的，可以问自己预测方向的任何效应是否实际上支持备择假设？例如，Cohen's *d* 为 10 的效应量是否支持您的假设？在心理学中，理论很少预构造如此巨大的效应量，如果您观察到 d = 10，您可能会检查一下计算错误或研究中的混淆变量。另一方面，*d* = 0.001 的效应量是否符合理论提出的机制？这样的效应量非常小，远低于个人能注意到的水平，因为它会低于感知和认知限制的*最小可觉差*。因此，在大多数情况下，*d* = 0.001 会导致研究人员得出结论：“嗯，这实在是太小了，根本不是我的理论所预测的，这么小的效果，几乎等同于没有效果。”然而，当我们做出方向性预测时，我们说这些类型的效应都是我们备择假设的一部分。尽管许多研究人员会同意这种微小的影响太小而不重要，但如果我们有一个效应量为0的虚无假设的方向性预测，它们仍然是支持我们备择假设的证据。此外，研究人员很少有资源从统计上拒绝如此小的效应的存在，因此声称这种效应仍然支持理论预测使得该理论实际上不可证伪：研究人员可以简单地回应任何显示出非显著小效应的重复性研究（例如 *d* = 0.05）：“这并没有证伪我的预测，我想效应只是比 *d* = 0.05 稍微小一些”，而无需承认预测已被证伪。这是有问题的，因为如果我们没有重复性和证伪的过程，科学学科就有滑向不可证伪的风险[@ferguson_vast_2012]。因此，只要有可能，当您设计实验或有理论和理论预测时，请仔细考虑并清楚地说明，最小目标效应量是多少。

## Specifying a SESOI based on theory 
## 根据理论来指定SESOI

One example of a theoretically predicted smallest effect size of interest can be found in the study by Burriss et al. [-@burriss_changes_2015], who examined whether women displayed increased redness in the face during the fertile phase of their ovulatory cycle. The hypothesis was that a slightly redder skin signals greater attractiveness and physical health, and that sending this signal to men yields an evolutionary advantage. This hypothesis presupposes that men can detect the increase in redness with the naked eye. Burriss et al. collected data from 22 women and showed that the redness of their facial skin indeed increased during their fertile period. However, this increase was not large enough for men to detect with the naked eye, so the hypothesis was falsified. Because the just-noticeable difference in redness of the skin can be measured, it was possible to establish a theoretically motivated SESOI. A theoretically motivated smallest effect size of interest can be derived from just-noticeable differences, which provide a lower bound on effect sizes that can influence individuals, or based on computational models, which can provide a lower bound on parameters in the model that will still be able to explain observed findings in the empirical literature.

一个理论预测的最小目标效应量的例子可以在Burriss等人[-@burriss_changes_2015]的研究中找到，他们研究了女性在排卵周期的育龄期间面部是否出现增加的红晕。他们的假设是，略微红润的皮肤可以传递更高的吸引力和身体健康性的信号，并且将这种信号发送给男性会产生进化优势。这个假设的前提是男性可以用肉眼检测出红晕的增加。Burriss等人从22名女性收集了数据，结果表明她们面部的红晕确实在育龄期间增加了。然而，这种增加对男性来说不足以用肉眼检测出来，因此假设被证伪。因为皮肤红润度的最小可觉差是可以被测量的，所以有可能建立一个理论驱动的 SESOI。理论上驱动的最小目标效应量可以从最小可觉差中推导出来，它提供了能够影响个体的效应量的下限，或者基于计算模型，它可以提供模型中参数的下限，该参数仍然能够解释实证文献中观察到的发现。

## Anchor based methods to set a SESOI
## 锚定法设置 SESOI

Building on the idea of a just-noticeable difference, psychologists are often interested in effects that are large enough to be noticed by single individuals. One procedure to estimate what constitutes a meaningful change on an individual level is the anchor-based method [@jaeschke_measurement_1989; @norman_truly_2004; @king_point_2011]. Measurements are collected at two time points (e.g., a quality of life measure before and after treatment). At the second time point, an independent measure (the anchor) is used to determine if individuals show no change compared to time point 1, or if they have improved, or worsened. Often, the patient is directly asked to answer the anchor question, and indicate if they subjectively feel the same, better, or worse at time point 2 compared to time point 1. @button_minimal_2015 used an anchor-based method to estimate that a minimal clinically important difference on the Beck Depression Inventory corresponded to a 17.5% reduction in scores from baseline.

基于最小可觉差的想法，心理学家通常对大到足以被单个个体注意到的效应感兴趣。锚定法是估计个体层面上何为有意义变化的一种程序[@jaeschke_measurement_1989; @norman_truly_2004; @king_point_2011]。该方法需要在两个时间点收集测量数据（例如，治疗前后的生活质量测量）。在第二个时间点，使用独立测量（锚点）来确定与时间点 1 相比个体是否有变化，或者他们是否有所改善或恶化。通常，患者会被直接问及锚定问题，并指出与时间点 1 相比，他们在时间点 2 的主观感觉是否相同、更好或更差。@button_minimal_2015等人使用锚定法估计出贝克抑郁量表的最小临床显著差异是对应于基线分数降低 17.5%。

Anvari and Lakens [-@anvari_using_2021] applied the anchor-based method to examine a smallest effect of interest as measured by the widely used Positive and Negative Affect Scale (PANAS). Participants completed the 20 item PANAS at two time points several days apart (using a Likert scale going from 1 = “very slightly or not at all”, to 5 = “extremely”). At the second time point they were also asked to indicate if their affect had changed a little, a lot, or not at all. When people indicated their affect had changed “a little”, the average change in Likert units was 0.26 scale points for positive affect and 0.28 scale points for negative affect. Thus, an intervention to improve people’s affective state that should lead to what individuals subjectively consider at least a little improvement might set the SESOI at 0.3 units on the PANAS. 

Anvari和Lakens[-@anvari_using_2021] 应用了锚定法来研究广泛使用的积极和消极情绪量表（PANAS）测量的最小目标效应量。参与者在相隔几天的两个时间点完成了 20 个项目的 PANAS调查（使用李克特量表，从1 =“非常轻微或根本没有”到5 =“极度”）。在第二个时间点，他们还被问及他们的情绪是否发生了一点、很多或根本没有变化。当人们表示他们的情绪“有一点”变化时，积极情绪的平均变化是0.26分，消极情绪的平均变化是0.28分。因此，用于改善人们情绪状态的干预措施，应该导致个体主观上认为至少有一点改善，可以将SESOI设置为PANAS量表上的0.3个单位。

## Specifying a SESOI based on a cost-benefit analysis 
## 根据成本效益分析确定SESOI

Another principled approach to justify a smallest effect size of interest is to perform a cost-benefit analysis. Research shows that cognitive training may improve mental abilities in older adults which might benefit older drivers [@ball_effects_2002]. Based on these findings, Viamonte, Ball, and Kilgore [-@viamonte_cost-benefit_2006] performed a cost-benefit analysis and concluded that based on the cost of the intervention (\$247.50), the probability of an accident for drivers older than 75 (*p* = 0.0710), and the cost of an accident (\$22,000), performing the intervention on all drivers aged 75 or older was more efficient than not intervening or only intervening after a screening test. Furthermore, sensitivity analyses revealed that intervening for all drivers would remain beneficial as long as the reduction in collision risk is 25%. Therefore, a 25% reduction in the probability of elderly above 75 getting into a car accident could be set as the smallest effect size of interest.

证明最小目标效应量合理的另一种原则性方法是执行成本效益分析。研究表明，认知训练可能改善老年人的心智能力，从而可能使老年驾驶员受益[@ball_effects_2002]。基于这些发现，Viamonte、Ball和Kilgore[-@viamonte_cost-benefit_2006]进行了成本效益分析并得出结论：根据干预措施的成本（247.50美元），75岁以上的老年驾驶员发生事故的概率（*p* = 0.0710）和一次事故的成本（22,000美元）相比，对所有75岁及以上的驾驶员进行干预比不干预或仅在筛查检验后进行干预更为有效。此外，敏感性分析表明，只要碰撞风险降低了25%，对所有驾驶员进行干预仍将是有益的。因此，可以将 75 岁以上老年人发生车祸的概率降低 25% 设置为最小目标效应量。

For another example, economists have examined the value of a statistical life, based on willingness to pay to reduce the risk of death, at \$1.5 - \$2.5 million (in the year 2000, in western countries, see Mrozek & Taylor [-@mrozek_what_2002]). Building on this work, Abelson [-@abelson_value_2003] calculated the willingness to pay to prevent acute health issues such as eye irritation at about \$40-\$50 per day. A researcher may be examining a psychological intervention that reduces the amount of times people touch their face close to their eyes, thereby reducing eye irritations caused by bacteria. If the intervention costs $20 per year to administer, it therefore should reduce the average number of days with eye irritation in the population by at least 0.5 days for the intervention to be worth the cost. A cost-benefit analysis can also be based on the resources required to empirically study a very small effect when weighed against the value this knowledge would have for the scientific community.

另一个例子，经济学家根据人们为降低死亡风险愿意支付的费用，计算出一个统计生命价值评估在150万到250万美元之间（2000 年，在西方国家，参见 Mrozek & Taylor  [-@mrozek_what_2002]）在这项工作的基础上，Abelson[-@abelson_value_2003]计算出为预防眼睛刺激等急性健康问题而支付的意愿约为每天 40-50 美元。 研究人员可能正在研究一种心理干预措施，可以减少人们将脸靠近眼睛的次数，从而减少细菌引起的眼睛刺激。 如果实施干预每年花费 20 美元，那么应该将人群中眼部刺激的平均天数减少至少 0.5 天，干预才值得花费。成本效益分析也可以基于研究非常小的效应所需的资源与这种知识对科学界的价值之间的权衡。

## Specifying the SESOI using the small telescopes approach
## 使用小型望远镜法确定 SESOI

Ideally, researchers who publish empirical claims would always specify which observations would falsify their claim. Regrettably, this is not yet common practice. This is particularly problematic when a researcher performs a close replication of earlier work. Because it is never possible to prove an effect is exactly zero, and the original authors seldom specify which range of effect sizes would falsify their hypotheses, it has proven to be very difficult to interpret the outcome of a replication study [@anderson_theres_2016]. When does the new data contradict the original finding?

理想情况下，发表经验声明的研究人员总是会指定哪些观察结果会证伪他们的声明。 遗憾的是，这还不是普遍做法。当研究人员对早期工作进行近似重复性研究时，这尤其成问题。因为永远无法证明一个效应确切等于零，而原作者很少指定哪种效应量的范围将推翻他们的假设，所以很难解释重复性研究的结果[@anderson_theres_2016]。新数据何时与原始发现相矛盾？

Consider a study in which you want to test the idea of the wisdom of crowds. You ask 20 people to estimate the number of coins in a jar, expecting the average to be very close to the true value. The research question is whether the people can on average correctly guess the number of coins, which is 500. The observed mean guess by 20 people is 550, with a standard deviation of 100. The observed difference from the true value is statistically significant, *t*(19)=2.37, *p* = 0.0375, with a Cohen’s *d* of 0.5. Can it really be that the group average is so far off? Is there no Wisdom of Crowds? Was there something special about the coins you used that make it especially difficult to guess their number? Or was it just a fluke? You set out to perform a close replication of this study.

考虑一项研究，您想在其中检验群体智慧这一概念。您让 20 个人估计一个罐子里的硬币数量，期望平均值非常接近真实值。 研究问题是人们是否能大体上猜对硬币数量，即 500。观察到的 20 人平均猜测为 550，标准差为 100。观察到的与真实值的差异具有统计显着性，*t* (19)=2.37，*p* = 0.0375，Cohen's *d* 为 0.5。 群体的平均水平真的相差如此之远吗？ 群体智慧不存在吗？ 您使用的硬币有什么特别之处使群体很难猜出它们的数量吗？ 还是只是侥幸？您用近似重复性方法来重新进行这项研究。

You want your study to be informative, regardless of whether there is an effect or not. This means you need to design a replication study that will allow you to draw an informative conclusion, regardless of whether the alternative hypothesis is true (the crowd will not estimate the true number of coins accurately) or whether the null hypothesis is true (the crowd will guess 500 coins, and the original study was a fluke). But since the original researcher did not specify a smallest effect size of interest, when would a replication study allow you to conclude the original study is contradicted by the new data? Observing a mean of exactly 500 would perhaps be considered by some to be quite convincing, but due to random variation you will (almost) never find a mean score of exactly 500. A non-significant result can’t be interpreted as the absence of an effect, because your study might have too small a sample size to detect meaningful effects, and the result might be a Type 2 error. So how can we move forward and define an effect size that is meaningful? How can you design a study that has the ability to falsify a previous finding?

您希望您的研究不论是否存在效应都能提供有意义的信息。这意味着您需要设计一项无论备择假设是否正确（群体无法准确猜测硬币的真正数量）或原假设是否正确（群体会猜测500枚硬币，原始的研究是巧合），都能够得出有意义的结论的重复性研究。但是，由于原始研究人员没有指定最小目标效应量，重复性研究什么时候可以让您得出原始研究与新数据相矛盾的结论？观察到恰好 500 的平均值可能会被某些人认为是非常有说服力的，但由于随机变化，您将（几乎）永远不会找到恰好 500 的平均值。一个不显著的结果不能解释为没有效应，因为您的研究可能样本量太小，无法检测到有意义的效应，且结果可能是第二类错误。那么我们应该如何继续研究并定义一个有意义的效应量呢？您如何设计一项能证伪先前发现的研究呢？

Uri Simonsohn [-@simonsohn_small_2015] defines a small effect as “one that would give 33% power to the original study”. In other words, the effect size that would give the original study odds of 2:1 *against* observing a statistically significant result if there was an effect. The idea is that if the original study had 33% power, the probability of observing a significant effect, if there was a true effect, is too low to reliably distinguish signal from noise (or situations where there is a true effect from situations where there is no true effect). Simonsohn (2015, p. 561) calls this the **small telescopes approach**, and writes: “Imagine an astronomer claiming to have found a new planet with a telescope. Another astronomer tries to replicate the discovery using a larger telescope and finds nothing. Although this does not prove that the planet does not exist, it does nevertheless contradict the original findings, because planets that are observable with the smaller telescope should also be observable with the larger one.”

Uri Simonsohn[-@simonsohn_small_2015]将小效应定义为“能够给原始研究提供33％的检验力的研究”。换句话说，如果效应存在，该效应量使得原始研究有1/3的可能观察到统计显著性。如果原始研究有33%的检验力，且效应真实存在的话，那么在该研究中观察到显著效应的概率太低，以至于不能准确区分信号和噪声（或者说不能准确区分效应存在和效应不存在的情况）。 Simonsohn（2015，第561页）称之为**小型望远镜法**，并写道：“想象一位天文学家声称通过使用望远镜发现了一个新行星。另一位天文学家试图使用更大的望远镜复制发现，但没有发现任何东西。尽管这并不能证明行星不存在，但它确实与原始发现相矛盾，因为使用较小望远镜可以观测到的行星也应该可以用更大望远镜观测到。”

Although this approach to setting a smallest effect size of interest (SESOI) is arbitrary (why not 30% power, or 35%?) it suffices for practical purposes (and you are free to choose a power level you think is too low). The nice thing about this definition of a SESOI is that if you know the sample size of the original study, you can always calculate the effect size that study had 33% power to detect. You can thus always use this approach to set a smallest effect size of interest. If you fail to find support for an effect size the original study has 33% power to detect, it does not mean there is no true effect, and not even that the effect is too small to be of any theoretical or practical interest. But using the small telescopes approach is a good first step, since it will get the conversation started about which effects are meaningful and allows researchers who want to replicate a study to specify when they would consider the original claim falsified.

虽然这种方法设定最小目标效应量是随意的（为什么不是30%或35%？），但它足以满足实际目的（您可以自由选择您认为过低的检验力水平）。SESOI 的这个定义的好处是，如果您知道原始研究的样本量，您总是可以计算出该研究具有 33%检验力检测到的效应量。 因此，您始终可以使用这种方法来设置最小目标效应量。如果您未能找到对原始研究具有 33%检验力检测到的效应量的支持，这并不意味着没有真正的效应，甚至也不意味着效应太小以至于没有任何理论或实践意义。但是使用小型望远镜法是很好的开端，因为它将开始讨论哪些效应是有意义的，并允许想要进行重复研究的研究人员指定他们何时会认为原始声明是伪造的。

With the small telescopes approach, the SESOI is based only on the sample size in the original study. A smallest effect size of interest is set only for effects in the same direction. All effects smaller than this effect (including large effects in the opposite direction) are interpreted as a failure to replicate the original results. We see that the small telescopes approach is a **one-sided equivalence test**, where only the upper bound is specified, and the smallest effect size of interest is determined based on the sample size of the original study. The test examines if we can reject effects as large or larger than the effect the original study has 33% power to detect. It is a simple one-sided test, not against 0, but against a SESOI.

使用小型望远镜法，SESOI 仅基于原始研究中的样本量。仅针对相同方向的效应设置最小目标效应量。 所有小于此效应的（包括相反方向的大效应）都被解释为无法重复原始结果。我们可以看到，小型望远镜法是一个单侧等价检验，只指定了上限，最小目标效应量是基于原始研究的样本量确定的。该检验检查我们是否可以拒绝与原始研究有33％检验力检测到的效应一样大或更大的效应。它是一个简单的单侧检验，不是针对0，而是针对SESOI。

For example, consider our study above in which 20 guessers tried to estimate the number of coins. The results were analyzed with a two-sided one-sample *t*-test, using an alpha level of 0.05. To determine the effect size that this study had 33% power for, we can perform a sensitivity analysis. In a sensitivity analysis we compute the required effect size given the alpha, sample size, and desired statistical power. Note that Simonsohn uses a two-sided test in his power analyses, which we will follow here – if the original study reported a pre-registered directional prediction, the power analysis should be based on a one-sided test. In this case, the alpha level is 0.05, the total sample size is 20, and the desired power is 33%. We compute the effect size that gives us 33% power and see that it is a Cohen’s *d* of 0.358. This means we can set our smallest effect size of interest for the replication study to *d* = 0.358. If we can reject effects as large or larger than *d* = 0.358, we can conclude that the effect is smaller than anything the original study had 33% power for. The screenshot below illustrates the correct settings in G\*Power, and the code in R is:

例如，考虑我们上面的研究，其中 20 位猜测者试图估计硬币的数量。 使用 0.05 的 alpha 水平，使用双侧单样本 *t* 检验分析结果。为了确定本研究具有 33% 检验力的效应量，我们可以进行敏感性分析。 在敏感性分析中，我们根据 alpha、样本量和所需的统计检验力计算所需的效应量。 请注意，Simonsohn 在他的检验力分析中使用了双侧检验，我们将在此处遵循——如果原始研究报告了预先登记的方向预测，则检验力分析应基于单侧检验。 在本例中，alpha 水平为 0.05，总样本量为 20，所需检验力为 33%。 我们计算给我们 33% 检验力的效应量，发现它是 Cohen’*d*值 为0.358。 这意味着我们可以将重复研究最小目标效应量设置为 *d* = 0.358。 如果我们可以拒绝等于或大于 *d* = 0.358 的效应，我们可以得出结论，该效应小于具有33%检验力的原始研究给出的效应。 下面的屏幕截图说明了 G\*Power 中的正确设置，R 中的代码是：

```{r}
library("pwr")

pwr::pwr.t.test(
  n = 20, 
  sig.level = 0.05, 
  power = 0.33, 
  type = "one.sample",
  alternative = "two.sided"
)
```



(ref:smalltelpowerlab) Screenshot illustrating a sensitivity power analysis in G*Power to compute the effect size an original study had 33% power to detect.
(ref:smalltelpowerlab) G*Power中演示敏感性检验力分析的截图，用于计算原始研究能够检测到33%检验力的效应量。
```{r smalltelpower, echo = FALSE, fig.cap="(ref:smalltelpowerlab)"}
knitr::include_graphics("~/Desktop/statistical_inferences-master/statistical_inferences-master/images/0deabffd850f7b63c16e41e0af9ae0b6.png")
```

Determining the SESOI based on the effect size the original study had 33% power to detect has an additional convenient property. Imagine the true effect size is actually 0, and you perform a statistical test to see if the data is statistically smaller than the SESOI based on the small telescopes approach (which is called an inferiority test). If you increase the sample size by 2.5 times, you will have approximately 80% power for this one-sided equivalence test, assuming the true effect size is exactly 0 (e.g., *d* = 0). People who do a replication study can follow the small telescope recommendations, and very easily determine both the smallest effect size of interest, and the sample size needed to design an informative replication study, assuming the true effect size is 0 (but see the section above for a-priori power analyses where you want to test for equivalence, but do not expect a true effect size of 0).

基于具有33%检验力的研究所给出的效应量来确定最小目标效应还有一个好处，假设真实的效应量为0，基于小型望远镜法，您最执行统计检验来看数据是否小于最小目标效应（这也被称作劣势检验）。在真实效应量确实为0（例如，*d*=0）的情况下，将样本量增加2.5倍，该单侧等价检验的检验力大概是80%。进行重复研究的人可以遵循小型望远镜法的建议，并且可以很容易地确定最小目标效应量和设计有意义的重复研究所需的样本量，假设真实效应量为 0（但请参阅上面的先验检验力分析部分，在其中您希望检验等价，但不期望真正的效果量为0。

The figure below, from Simonsohn (2015) illustrates the small telescopes approach using a real-life example. The original study by Zhong and Liljenquist (2006) had a tiny sample size of 30 participants in each condition and observed an effect size of *d* = 0.53, which was barely statistically different from zero. Given a sample size of 30 per condition, the study had 33% power to detect effects larger than *d* = 0.401. This “small effect” is indicated by the green dashed line. In R, the smallest effect size of interest is calculated using:

下图来自 Simonsohn（2015），使用现实生活中的例子说明了小型望远镜法。 Zhong 和 Liljenquist（2006 ）的最初研究在每种情况下的样本量很小，只有 30 名被试，观察到的效应量为 *d* = 0.53，这与零几乎没有统计学差异。 假设每个条件的样本量为 30，则该研究有 33% 的检验力来检测大于 *d* = 0.401 的效应。 这种“小效应”由绿色虚线表示。 在 R 中，最小目标效应量是使用以下方法计算的：

```{r}
pwr::pwr.t.test(
  n = 30, 
  sig.level = 0.05, 
  power = 1/3, 
  type = "two.sample",
  alternative = "two.sided"
)
```

Note that 33% power is a rounded value, and the calculation uses 1/3 (or 0.3333333…).
请注意，33%的统计检验力是一个取整的值，计算时使用了1/3（或0.3333333 ...）。


(ref:simonsohnexamplelab) Example used in Simonsohn (2015) of an original study and two replication studies.
(ref:simonsohnexamplelab) Simonsohn (2015) 在一项原始研究和两项重复研究中使用的示例。
```{r simonsohnexample, echo = FALSE, fig.cap="(ref:simonsohnexamplelab)"}
knitr::include_graphics("~/Desktop/statistical_inferences-master/statistical_inferences-master/images/a4aa20a6e2dadfbaa82bc614d40693c7.png")
```

We can see that the first replication by Gámez and colleagues also had a relatively small sample size (N = 47, compared to N = 60 in the original study), and was not designed to yield informative results when interpreted with a small telescopes approach. The confidence interval is very wide and includes the null effect (*d* = 0) and the smallest effect size of interest (*d* = 0.401). Thus, this study is inconclusive. We can’t reject the null, but we can also not reject effect sizes of 0.401 or larger that are still considered to be in line with the original result. The second replication has a much larger sample size, and tells us that we can’t reject the null, but we can reject the smallest effect size of interest, suggesting that the effect is smaller than what is considered an interesting effect based on the small telescopes approach.

我们可以看到，Gámez及其同事进行的第一次重复研究也具有相对较小的样本量（N = 47，相对于原始研究中的N = 60），并且不是为了通过小型望远镜法产生有意义的结果而设计的。置信区间非常宽，且包括零效应（*d* = 0）和最小目标效应量（*d* = 0.401）。因此，这项研究是没有确切结果的。我们不能否认零值，但我们也不能否认大于或等于0.401的效应量，因为这仍然符合原始结果。第二次重复研究具有更大的样本量，并告诉我们不能否认零值，但我们可以拒绝最小目标效应量，这表明该效应小于根据小型望远镜法认为应当关注的效应。

Although the *small telescope* recommendations are easy to use, one should take care not to turn any statistical procedure into a heuristic. In our example above with the 20 referees, a Cohen’s *d* of 0.358 would be used as a smallest effect size of interest, and a sample size of 50 would be collected (2.5 times the original 20), but if someone would make the effort to perform a replication study, it would be relatively easy to collect a larger sample size. Alternatively, had the original study been extremely large, it would have had high power for effects that might not be practically significant, and we would not want to collect 2.5 times as many observations in a replication study. Indeed, as Simonsohn writes: “whether we need 2.5 times the original sample size or not depends on the question we wish to answer. If we are interested in testing whether the effect size is smaller than d33%, then, yes, we need about 2.5 times the original sample size no matter how big that original sample was. When samples are very large, however, that may not be the question of interest.” Always think about the question you want to ask, and design the study so that it provides an informative answer for a question of interest. Do not automatically follow a 2.5 times n heuristic, and always reflect on whether the use of a suggested procedure is appropriate in your situation.

虽然*小望远镜法*的建议易于使用，但应注意不要将任何统计程序变成启发式程序。 在我们上面关于20名被试的例子中，Cohen's *d*为0.358将用作最小目标效应量，并且将收集50个样本量（原始20个的2.5倍），但如果有人付出努力进行重复性研究，则收集更大的样本量将相对容易。或者，如果原始研究非常大，则对于可能不太实际的效应具有很高的检验力，我们将不希望在重复研究中收集2.5倍于原始研究的观测值。事实上，正如Simonsohn所写：“我们是否需要原始样本量的2.5倍取决于我们希望回答的问题。如果我们想检验效应量是否小于33％，那么，无论原始样本量如何，我们都需要大约2.5倍的原始样本量。但是，当样本非常大时，这可能不是我们感兴趣的问题。”始终考虑您想要问的问题，并设计能为感兴趣的问题提供信息丰富的答案的研究。不要自动遵循2.5倍n的启发式方法，并且始终反思建议的程序在您的情况下是否合适。


## Setting the Smallest Effect Size of Interest to the Minimal Statistically Detectable Effect
## 把最小目标效应量设置为统计可检验的最小效应

Given a sample size and alpha level, every test has a [minimal statistically detectable effect](#minimaldetectable). For example, given a test with 86 participants in each group, and an alpha level of 5%, only *t*-tests which yield a *t* ≥ 1.974 will be statistically significant. In other words, *t* = 1.974 is the **critical *t*-value**. Given a sample size and alpha level, the critical *t*-value can be transformed into a **critical *d*-value**. As visualized in Figure \@ref(fig:distpowerplot1), with n = 50 in each group and an alpha level of 5% the critical *d*-value is 0.4. This means that only effects larger than 0.4 will yield a *p* \< α. The critical *d*-value is influenced by the sample size per group, and the alpha level, but does not depend on the the true effect size.

给定样本量和 alpha 水平，每个检验都具有最小的统计可检验效应。例如，给定每组有 86 名参与者的检验，且 alpha 水平为 5%，只有 t ≥ 1.974 的 t 检验才具有统计显著性。 换句话说，t = 1.974 是临界 t 值。 给定样本量和 alpha 水平，可以将临界 t 值转换为临界 d 值。如图 9.8 所示，每组 n = 50，alpha水平为 5%，临界 d 值为 0.4。这意味着只有大于 0.4 的效应才会产生 p < α。 临界 d 值受每组样本量和 alpha 水平的影响，但不取决于真实效应量。


(ref:distpowerplot1lab) Null and alternative distribution with Type 1 and Type 2 error indicating the smallest effect size that will be statistically significant with n = 50 per condition.
(ref:distpowerplot1lab) 具有一类和二类错误的零分布和备择分布表明，在每个条件为 n = 50时，最小的效应量将具有统计显著性。

```{r distpowerplot1, echo = FALSE, fig.cap="(ref:distpowerplot1lab)"}
knitr::include_graphics("~/Desktop/statistical_inferences-master/statistical_inferences-master/images/dpplot50.png")
```

It is possible to observe a statistically significant test result if the true effect size is *smaller* than the critical effect size. Due to random variation, it is possible to observe a larger value in a *sample* than is the true value in the population. This is the reason the statistical power of a test is never 0 in a null hypothesis significance test. As illustrated in Figure \@ref(fig:distpowerplot2), even if the true effect size is smaller than the critical value (i.e., if the true effect size is 0.2) we see from the distribution that we can expect some *observed effect sizes* to be larger than 0.4 when the *true population effect size* is *d* = 0.2 – if we compute the statistical power for this test, it turns out we can expect 16.77% of the *observed effect sizes* will be larger than 0.4, in the long run. That is not a lot, but it is something. This is also the reason why publication bias combined with underpowered research is problematic: It leads to a large **overestimation of the true effect size** when only observed effect sizes from statistically significant findings in underpowered studies end up in the scientific literature.

如果真实效应量小于临界效应量，则有可能观察到统计上显著的检验结果。由于随机变异，有可能在样本中观察到比总体中的真实值更大的值。这就是为什么在零假设显著性检验中，检验的统计检验力永远不为零的原因。正如图9.9所示，即使真实效应量小于临界值（例如，真实效应量为0.2），我们从分布中可以看出，当真实的总体效应量为 d = 0.2，我们可以预期一些大于0.4的效应量——如果我们计算此检验的统计检验力，结果表明从长远来看，我们可以预期观察到的效应量中有 16.77% 会大于 0.4。 这不是很多，但很重要。 因此，若检验力不足的研究由于发表偏倚更多地出现，将是很有问题的，如果文献中观察到显著结果的效应量只来自检验力不足的研究，这将导致我们对真实效应量的大幅高估。

(ref:distpowerplot2lab) Null and alternative distribution with Type 1 and Type 2 error indicating the smallest effect size that will be statistically significant with n = 50 per condition.
(ref:distpowerplot2lab) 具有一类和二类错误的零分布和备择分布表明，，在每个条件为 n = 50时，最小效应量将具有统计显著性。

```{r distpowerplot2, echo = FALSE, fig.cap="(ref:distpowerplot2lab)"}
knitr::include_graphics("~/Desktop/statistical_inferences-master/statistical_inferences-master/images/dpplot502.png")
```

We can use the minimal statistically detectable effect to set the SESOI for replication studies. If you attempt to replicate a study, one justifiable option when choosing the smallest effect size of interest (SESOI) is to use the smallest observed effect size that could have been statistically significant in the study you are replicating. In other words, you decide that effects that could not have yielded a *p*-value less than α in an original study will not be considered meaningful in the replication study. The assumption here is that the original authors were interested in observing a significant effect, and thus were not interested in observed effect sizes that could not have yielded a significant result. It might be likely that the original authors did not consider which effect sizes their study had good statistical power to detect, or that they were interested in smaller effects but gambled on observing an especially large effect in the sample purely as a result of random variation. Even then, when building on earlier research that does not specify a SESOI, a justifiable starting point might be to set the SESOI to the smallest effect size that, when observed in the original study, **could have been statistically significant**. Not all researchers might agree with this (e.g., the original authors might say they actually cared just as much about an effect of *d* =0.001). However, as we try to change the field from the current situation where no one specifies what would falsify their hypothesis, or what their smallest effect size of interest is, this approach is one way to get started. In practice, as explained in the section on [post-hoc power](#posthoc), due to the relation between *p* = 0.05 and 50% power for the observed effect size, this justification for a SESOI will mean that the SESOI is set to the effect size the original study had 50% power to detect for an independent *t*test. This approach is in some ways similar to the small telescopes approach by Simonsohn (2015), except that it will lead to a somewhat larger SESOI.

我们可以使用最小的统计意义上可检测的效应来设置重复研究的SESOI。如果您试图重复一项研究，选择最小目标效应量（SESOI）的一个合理选项是使用在您正在重复的研究中可能具有统计显著性的最小的观察效应量。换句话说，你认为在重复研究中，那些在原始研究中大于α水平的p值所对应的效应不是有意义的。这里的假设是原作者希望观察到显著效应，因此对观察到的无法产生显著结果的效应量不感兴趣。原作者可能没有考虑他们的研究对哪些效应具有良好的统计检验力，或者他们对更小的效应感兴趣，但仍冒着观察到的大效应只是来源于随机变异的风险。即使那样，当建立在未指定 SESOI 的早期研究的基础上时，合理的起点可能是将 SESOI 设置为最小效应量，当在原始研究中观察到时，该效应量可能具有统计显著性。并非所有研究人员都会同意这一点（例如，原始作者可能会说他们实际上也关心d = 0.001的效应）。然而，当我们试图改变目前没有人指定什么会证伪他们的假设，或者他们最小目标效应量是什么的情况时，这种方法是一种开始的方式。实际上，如事后检验力部分所述，对于观察到的效应量而言, 由于在p = 0.05 和 50% 检验力之间的关系，这种对 SESOI 的合理性证明将意味着 SESOI 被设置为原始研究在独立t检验中有 50% 的检验力来探测的效应量。这种方法在某些方面类似于 Simonsohn (2015) 的小型望远镜法，只是它会导致更大的 SESOI。

Setting a smallest effect size of interest for a replication study is a bit like a tennis match. Original authors serve and hit the ball across the net, saying ‘look, something is going on’. The approach to set the SESOI to the effect size that could have been significant in the original study is a return volley which allows you to say ‘there does not seem to be anything large enough that could have been significant in your own original study’ after performing a well-designed replication study with high statistical power to reject the SESOI. This is never the end of the match – the original authors can attempt to return the ball with a more specific statement about effects their theory predicts, and demonstrate such a smaller effect size is present. But the ball is back in their court, and if they want to continue to claim there is an effect, they will have to support their claim by new data.

为重复研究设置最小目标效应量有点像网球比赛。原始作者发球并把球打过网，说“看，有些事情正在发生”。将SESOI设置为在原始研究中可能会显著的效应量的方法是回球，这样在通过一项设计良好、统计检验力高的重复研究拒绝了SESOI后，您可以说“您的原始研究似乎没有足够大的显著效应”，这并不是比赛的终点——原始作者可以尝试以一种更具体的有关他们的理论预测的效应的说明击回球，并证明存在这样一个更小的效应量。但球回到他们这边了，如果他们想继续声称存在效应，他们将不得不通过新数据支持自己的主张。

Beyond replication studies, the amount of data that is collected limits the inferences one can make. It is also possible to compute a minimal statistically detectable effect based on the sample sizes that are typically used in a research field. For example, imagine a line of research in which a hypothesis has almost always
been tested by performing a one-sample *t*-test, and where the sample sizes that are collected are always smaller than 100 observations. A one-sample *t*-test on 100 observations, using an alpha of .05 (two sided), has 80% power to detect an effect of *d* = 0.28 (as can be calculated in a sensitivity power analysis). In a new study, concluding that one can reliably reject the presence of effects more extreme than *d* = 0.28 suggests that sample sizes of 100 might not be enough to detect effects in such research lines. Rejecting the presence of effects more extreme than *d* = 0.28 does not test a theoretical prediction, but it contributes to the literature by answering a **resource question**. It suggests that future studies in this research line will need to change the design of their studies by substantially increasing the sample size. Setting the smallest effect size of interest based on this approach does not answer any theoretical question (after all, the SESOI is not based on any theoretical prediction). But informing peers that given the sample size commonly collected in a field, the effect is not large enough so that it can be reliably studied is a useful contribution to the literature. It does not mean that the effect is not interesting per se, and a field might decide that it is time to examine the research question collaboratively, by coordinating research lines, and collecting enough data to reliably study whether a smaller effect is present.

除了重复研究之外，收集的数据量限制了人们能够做出的推论。根据研究领域通常使用的样本量，也可以计算出最小的统计意义上可检测的效应。例如，假设一个研究领域中的假设几乎总是通过执行单样本 t 检验来检验，并且收集的样本大小始终小于 100个观测值。对 100 个观察值的单样本 t 检验，使用 0.05 的 alpha（双侧），具有 80% 的检验力来检测一个 d = 0.28 的效应（可以在灵敏度检验力分析中计算）。在一项新研究中，得出结论认为可以可靠地拒绝比 d = 0.28 更极端效应的存在，这表明 100 的样本量可能不足以检测此类研究系列中的效应。拒绝比 d = 0.28 更极端效应的存在并不能检验理论预测，但它通过回答资源问题对文献做出贡献。这表明该研究领域的未来研究将需要通过大幅增加样本量来改变研究设计。基于这种方法设置最小目标效应量并不能回答任何理论问题（毕竟，SESOI 不基于任何理论预测）。但是，告知同行，在考虑到在一个领域里通常收集的样本量，效应不够大因此无法进行可靠地进行研究，这是对文献的有益贡献。这并不意味着该效应本身并不值得探索，并且一个领域可能会决定是时候通过协调研究路线并收集足够的数据来可靠地研究是否存在较小的效应来协同检验研究问题。

## Test Yourself
## 自我测验
### Questions about equivalence tests
### 关于等效检验的问题
**Q1**: When the 90% CI around a mean difference falls just within the equivalence range from -0.4 to 0.4, we can reject the smallest effect size of interest. Based on your knowledge about confidence intervals, when the equivalence range is changed to -0.3 – 0.3, what is needed for the equivalence test to be significant (assuming the effect size estimate and standard deviation remains the same)?

A) A larger effect size.
B) A lower alpha level.
C) A larger sample size.
D) Lower statistical power.
**问题1**：当均值差异的90%置信区间落入等效范围-0.4到0.4之间时，我们可以拒绝最小目标效应量。根据你对于置信区间的了解，当等效范围改变为变化为-0.3到0.3时，什么情况下等效检验才能显著（假定估计效应量和标准差不变）？
A）更大的效应量。
B）更低的α水平。
C）更大的样本量。
D）更低的统计效力。
**Q2**: Why is it incorrect to conclude that there is no effect, when an equivalence test is statistically significant?

A) An equivalence test is a statement about the data, not about the presence or absence of an effect.
B) The result of an equivalence test could be a Type 1 error, and therefore, one should conclude that there is no effect, or a Type 1 error has been observed.
C) An equivalence test rejects values as large or larger than the smallest effect size of interest, so the possibility that there is a small non-zero effect cannot be rejected.
D) We conclude there is no effect when the equivalence test is non-significant, not when the equivalence test is significant.
**问题2**：为什么在等效检验统计显著时，得出没有效应的结论是错误的？

A）等效检验只针对数据，而非效应的存在与否。
B）等效检验可能伴随着一类错误，因此，我们应该认为不存在效应，或者存在一类错误。
C）等效检验会拒绝与最小目标效应同样大或者更大的数值，所以不能拒绝存在一个小的非零效应的可能性。
D）当等价检验不显著而非显著时，我们才可以得出效应不存在的结论。
**Q3**: Researchers are interested in showing that students who use an online textbook perform just as well as students who use a paper textbook. If so, they can recommend teachers to allow students to choose their preferred medium, but if there is a benefit, they would recommend the medium that leads to better student performance. They randomly assign students to use an online textbook or a paper textbook, and compare their grades on the exam for the course (from the worst possible grade, 1, to the best possible grade, 10). They find that the both groups of students perform similarly, with for the paper textbook condition *m* = 7.35, *sd* = 1.15, *n* = 50, and the online textbook *m* = 7.13, *sd* = 1.21, *n* = 50). Let’s assume we consider any effect as large or larger than half a grade point (0.5) worthwhile, but any difference smaller than 0.5 too small to matter, and the alpha level is set at 0.05. What would the authors conclude? Copy the code below into R, replacing all zeroes with the correct numbers. Type `?tsum_TOST` for help with the function.

**问题3**：研究者想知道使用电子书的学生是否与使用纸质书的学生得表现一样好。如果一样好，他们就会建议教师允许学生自由使用媒介；但如果这两者有差异，他们则会推荐使用导致更好表现的那种媒介。他们随机分配学生使用电子书或者教科书，比较他们在考试中的成绩
课程(从最差的1分到最好的10分)。他们发现两组学生的表现相似，对于纸质教科书条件均值为 7.35，标准差为1.15,样本量为50，电子书均值为7.13,标准差为1.21,样本量为50)。假设我们认为任何大于或大于半个绩点(0.5)的影响都是值得的，但任何差异小于0.5，因为太小而无关紧要，alpha水平被设置为0.05。作者会得出什么结论?将下面的代码复制到R中，用正确的数字替换所有的0。输入?tsum_TOST以获取该函数的帮助。


<!-- ```{r eval = FALSE} -->

<!-- TOSTER::tsum_TOST(m1 = 7.35, -->
<!--                   sd1 = 1.15, -->
<!--                   n1 = 50, -->
<!--                   m2 = 7.13, -->
<!--                   sd2 = 1.21, -->
<!--                   n2 = 50, -->
<!--                   low_eqbound = -0.5, -->
<!--                   high_eqbound = 0.5, -->
<!--                   eqbound_type = "raw", -->
<!--                   alpha = 0.05) -->

<!-- ``` -->




```{r eval = FALSE}
TOSTER::tsum_TOST(
  m1 = 7.35,
  sd1 = 1.15,
  n1 = 50,
  m2 = 7.13,
  sd2 = 1.21,
  n2 = 50,
  low_eqbound = -0.5,
  high_eqbound = 0.5,
  eqbound_type = "raw",
  alpha = 0.05
)

```

A) We can **reject** an effect size of zero, and we can **reject** the presence of effects as large or larger than the smallest effect size of interest.
B) We can **not reject** an effect size of zero, and we can **reject** the presence of effects as large or larger than the smallest effect size of interest.
C) We can **reject** an effect size of zero, and we can **not reject** the presence of effects as large or larger than the smallest effect size of interest.
D) We can **not reject** an effect size of zero, and we can **not reject** the presence of effects as large or larger than the smallest effect size of interest.
A）我们可以**拒绝**效应值为零，也可以**拒绝**效应值大于或大于最小效应值的存在。
B）我们不能**拒绝**效应值为零，我们可以**拒绝**效应值大于或大于最小效应值的存在。
C）我们可以**拒绝**效应值为零，也不能**拒绝**效应值大于或大于最小效应值的存在。
D）我们不能**拒绝**效应值为零，也不能**拒绝**效应值大于或大于最小效应值的存在。
**Q4**: If we increase the sample size in question Q3 to 150 participants in each condition, and assuming the observed means and standard deviations would be exactly the same, which conclusion would we draw?

A) We can **reject** an effect size of zero, and we can **reject** the presence of effects as large or larger than the smallest effect size of interest.
B) We can **not reject** an effect size of zero, and we can **reject** the presence of effects as large or larger than the smallest effect size of interest.
C) We can **reject** an effect size of zero, and we can **not reject** the presence of effects as large or larger than the smallest effect size of interest.
D) We can **not reject** an effect size of zero, and we can **not reject** the presence of effects as large or larger than the smallest effect size of interest.
**问题4**：如果我们将问题3中的样本量增加到每个条件下150名参与者，并且假设观察到的平均值和标准差完全相同，我们会得出什么结论?

A）我们可以**拒绝**效应值为零，也可以**拒绝**效应值大于或大于最小效应值的存在。
B）我们不能**拒绝**效应值为零，我们可以**拒绝**效应值大于或大于最小效应值的存在。
C）我们可以**拒绝**效应值为零，也可以**不拒绝**效应值大于或大于最小效应值的存在。
D）我们不能**拒绝**效应值为零，也不能**拒绝**效应值大于或大于最小效应值的存在。
**Q5**: If we increase the sample size in question Q3 to 500 participants in each condition, and assuming the observed means and standard deviations would be exactly the same, which conclusion would we draw?

A) We can **reject** an effect size of zero, and we can **reject** the presence of effects as large or larger than the smallest effect size of interest.
B) We can **not reject** an effect size of zero, and we can **reject** the presence of effects as large or larger than the smallest effect size of interest.
C) We can **reject** an effect size of zero, and we can **not reject** the presence of effects as large or larger than the smallest effect size of interest.
D) We can **not reject** an effect size of zero, and we can **not reject** the presence of effects as large or larger than the smallest effect size of interest.
Sometimes the result of a test is **inconclusive**, as both the null hypothesis test, and the equivalence test, of not statistically significant. The only solution in such a case is to collect additional data. Sometimes both the null hypothesis test and the equivalence test are statistically significant, in which case the effect is **statistically different from zero, but practically insignificant** (based on the justification for the SESOI).
**问题5**：如果我们将问题3中的样本量增加到每个条件下500名参与者，并且假设观察到的平均值和标准差完全相同，我们会得出什么结论?
A）我们可以**拒绝**效应值为零，也可以**拒绝**效应值大于或大于最小效应值的存在。
B）我们不能**拒绝**效应值为零，我们可以**拒绝**效应值大于或大于最小效应值的存在。
C）我们可以**拒绝**效应值为零，也可以**不拒绝**效应值大于或大于最小效应值的存在。
D）我们不能**拒绝**效应值为零，也不能**拒绝**效应值大于或大于最小效应值的存在。
有时检验的结果是**不确定的**，如零假设检验和等效检验在统计上不显著。在这种情况下，唯一的解决方案是收集额外的数据。有时，零假设检验和等效检验在统计上都是显著的，在这种情况下，效果**在统计上不同于零，但实际上不显著**(基于SESOI的证明)。
**Q6**: We might wonder what the statistical power was for the test in Q3, assuming there was no true difference between the two groups (so a true effect size of 0). Using the new and improved `power_t_TOST` function in the TOSTER R package, we can compute the power using a sensitivity power analysis (i.e., entering the sample size per group of 50, the assumed true effect size of 0, the equivalence bounds, and the alpha level. Note that because the equivalence bounds were specified on a raw scale in Q3, we will also need to specify an estimate for the true standard deviation in the population. Let's assume this true standard deviation is 1.2. Round the answer to two digits after the decimal. Type `?power_t_TOST` for help with the function. What was the power in Q3?

**问题6**：我们可能想知道问题3中检验的统计效力是多少，假设两组之间没有真正的差异(因此真实效应大小为0)。使用R包TOSTER中新改进的'power_t_TOST'函数，我们可以使用灵敏度效力分析(即输入每组50个样本量，假设真实效应大小为0，等效边界和alpha水平)计算效力。请注意，由于等效边界是在问题3的原始尺度上指定的，因此我们还需要指定总体中真实标准偏差的估计值。假设真实标准差是1.2。把答案四舍五入到小数点后两位。输入?power_t_TOST '获取函数的帮助。问题3的效力是多少?
```{r eval = FALSE}
TOSTER::power_t_TOST(
  n = 15,
  delta = 0.0,
  sd = 1.2,
  low_eqbound = -0.5,
  high_eqbound = 0.5,
  alpha = 0.05,
  type = "two.sample"
)
```

```{r eval = FALSE}
TOSTER::power_t_TOST(
  n = 00,
  delta = 0.0,
  sd = 0.0,
  low_eqbound = -0.0,
  high_eqbound = 0.0,
  alpha = 0.05,
  type = "two.sample"
)
```

A) 0.00
B) 0.05
C) 0.33
D) 0.40

**Q7**: Assume we would only have had 15 participants in each group in Q3, instead of 50. What would be the statistical power of the test with this smaller sample size (keeping all other settings as in Q6)? Round the answer to 2 digits.
**问题7**：假设在问题3每组只有15名参与者而不是50名。在这个较小的样本量下(其他条件如问题6中所示)，检验的统计效力是多少?答案四舍五入到两位小数。

A) 0.00
B) 0.05
C) 0.33
D) 0.40

**Q8**: You might remember from discussions on statistical power for a null hypothesis significance test that the statistical power is never smaller than 5% (if the true effect size is 0, power is formally undefined, but we will observe at least 5% Type 1 errors, and the power increases when introducing a true effect). In a two-sided equivalence tests, power can be lower than the alpha level. Why?

A) Because in an equivalence test the Type 1 error rate is not bounded at 5%.
B) Because in an equivalence test the null hypothesis and alternative hypothesis are reversed, and therefore the Type 2 error rate does not have a lower bound (just as the Type 1 error rate in NHST has no lower bound).
C) Because the confidence interval needs to fall between the lower and upper bound of the equivalence interval, and with small sample sizes, this probability can be close to one (because the confidence interval is very wide).
D) Because the equivalence test is based on a confidence interval, and not on a *p*-value, and therefore power is not limited by the alpha level.
**问题8**：你可能还记得关于零假设显著性检验的统计效力的讨论，统计效力从不小于5%(如果真实效应大小为0，效力在形式上未定义，但我们将观察到至少5%的一类错误，并且在引入真实效应时效力增加)。在双尾等效检验中，效力可以低于α水平。为什么?

A）因为在等效检验中，一类错误率没有限定在5%。
B）因为在等效检验中，原假设和备择假设是相反的，因此二类错误率没有下界(就像零假设检验中的一类错误率没有下界一样)。
C）由于置信区间需要落在等效区间的下界和上界之间，并且样本量较小，因此该概率可以接近于1(因为置信区间非常宽)。
D）因为等效检验是基于置信区间，而不是基于**p**值，因此效力不受alpha水平的限制。
**Q9**: A well designed study has high power to detect an effect of interest, but also to reject the smallest effect size of interest. Perform an a-priori power analysis for the situation described in Q3. Which sample size in **each group** needs to be collected to achieved a desired statistical power of 90% (or 0.9), assuming the true effect size is 0, and we still assume the true standard deviation is 1.2? Use the code below, and round up the sample size (as we cannot collect a partial observation).
**问题9**：一项设计良好的研究能够很好地检测目标效应，但也能拒绝最小的目标效应。对问题3中描述的情况进行先验效力分析。假设真实效应量为0，我们仍然假设真实标准差为1.2，**每个组**中需要收集多少样本量才能达到期望的统计效力为90%(或0.9)?使用下面的代码，并将样本大小四舍五入(因为我们无法获得非整数的观测)。  

```{r eval = FALSE}
TOSTER::power_t_TOST(
  power = 0.90,
  delta = 0.0,
  sd = 1.2,
  low_eqbound = -0.5,
  high_eqbound = 0.5,
  alpha = 0.05,
  type = "two.sample"
)
```


```{r eval = FALSE}
TOSTER::power_t_TOST(
  power = 0.00,
  delta = 0.0,
  sd = 0.0,
  low_eqbound = -0.0,
  high_eqbound = 0.0,
  alpha = 0.05,
  type = "two.sample"
)
```

A) 100
B) 126
C) 200
D) 252

**Q10**: Assume that when performing the power analysis for Q9 we did not expect the true effect size to be 0, but we actually expected a mean difference of 0.1 grade point. Which sample size in **each group** would we need to collect for the equivalence test, now that we expect a true effect size of 0.1? Change the variable `delta` in `power_t_TOST` to answer this question.
**问题10**：假设在对问题9进行效力分析时，我们并不期望真正的效应大小为0，但我们实际期望的平均差值为0.1分。在**每个组**中，当我们期望真正的效应量为0.1时，我们需要收集多少样本量来进行等效检验?调整' power_t_TOST '中的变量' delta '来回答这个问题。
```{r eval = FALSE}
TOSTER::power_t_TOST(
  power = 0.90,
  delta = 0.1,
  sd = 1.2,
  low_eqbound = -0.5,
  high_eqbound = 0.5,
  alpha = 0.05,
  type = "two.sample"
)
```

A) 117
B) 157
C) 314
D) 3118

**Q11**: Change the equivalence range to -0.1 and 0.1 for Q9 (and set the expected effect size of `delta` to 0). To be able to reject effects outside such a very narrow equivalence range, you’ll need a large sample size. With an alpha of 0.05, and a desired power of 0.9 (or 90%), how many participants would you need in **each group**?
**问题11**：将问题9的等效范围更改为-0.1和0.1(并将'delta'的预期效应大小设置为0)。为了能够拒绝这个小等效范围之外的效应，你将需要大样本量。如果alpha值为0.05，期望效力为0.9(或90%)，那么**每个组**需要多少被试?
```{r eval = FALSE}
TOSTER::power_t_TOST(
  power = 0.90,
  delta = 0.0,
  sd = 1.2,
  low_eqbound = -0.1,
  high_eqbound = 0.1,
  alpha = 0.05,
  type = "two.sample"
)
```

A) 1107
B) 1157
C) 2468
D) 3118

You can see it takes a very large sample size to have high power to reliably reject very small effects. This should not be surprising. After all, it also requires a very large sample size to *detect* small effects! This is why we typically leave it to a future meta-analysis to detect, or reject, the presence of small effects.
你可以看到我们需要一个非常大的样本量才能有高的效力来可靠地拒绝非常小的效应。这不足为奇。毕竟，我们也需要一个非常大的样本量来*检测*到小效应!这就是为什么我们通常把它留给未来的荟萃分析来检测或拒绝小效应的存在。

**Q12**: You can do equivalence tests for all tests. The TOSTER package has functions for *t*-tests, correlations, differences between proportions, and meta-analyses. If the test you want to perform is not included in any software, remember that you can just use a 90% confidence interval, and test whether you can reject the smallest effect size of interest. Let’s perform an equivalence test for a meta-analysis. Hyde, Lindberg, Linn, Ellis, and Williams [-@hyde_gender_2008] report that effect sizes for gender differences
in mathematics tests across the 7 million students in the US represent trivial differences, where a trivial difference is specified as an effect size smaller then *d* =0.1. The table with Cohen’s d and se is reproduced below:

| **Grades**  | **d + se**       |
|-------------|------------------|
| Grade 2     | 0.06 +/- 0.003   |
| Grade 3     | 0.04 +/- 0.002   |
| Grade 4     | \-0.01 +/- 0.002 |
| Grade 5     | \-0.01 +/- 0.002 |
| Grade 6     | \-0.01 +/- 0.002 |
| Grade 7     | \-0.02 +/- 0.002 |
| Grade 8     | \-0.02 +/- 0.002 |
| Grade 9     | \-0.01 +/- 0.003 |
| Grade 10    | 0.04 +/- 0.003   |
| Grade 11    | 0.06 +/- 0.003   |
**问题12**：你可以对所有检验进行等效检验。TOSTER包具有进行*t*检验，相关性，比例差异和元分析的函数。如果你想要进行的检验没有包含在任何软件中，请记住，你可以只使用90%的置信区间，并检验你是否可以拒绝最小目标效应值。让我们对meta分析进行等效检验。Hyde, Lindberg, Linn, Ellis, and Williams [-@hyde_gender_2008]报告了在美国700万学生的数学测试中，性别差异的效应大小可以忽略不计，这个性别差异的效应被定义为小于*d* =0.1。科恩d效应量和标准误se表如下:

| **年级**   | **d + se**       |
|------------|------------------|
| 二年级     | 0.06 +/- 0.003   |
| 三年级     | 0.04 +/- 0.002   |
| 四年级     | \-0.01 +/- 0.002 |
| 五年级     | \-0.01 +/- 0.002 |
| 六年级     | \-0.01 +/- 0.002 |
| 七年级     | \-0.02 +/- 0.002 |
| 八年级     | \-0.02 +/- 0.002 |
| 九年级     | \-0.01 +/- 0.003 |
| 十年级     | 0.04 +/- 0.003   |
| 十一年级   | 0.06 +/- 0.003   |

For grade 2, when we perform an equivalence test with boundaries of *d* =-0.1 and *d* =0.1, using an alpha of 0.01, which conclusion can we draw? Use the TOSTER function TOSTmeta, and enter the alpha, effect size (ES), standard error (se), and equivalence bounds.
对于二年级，当我们在*d*=-0.1和*d*=0.1的边界下，使用alpha为0.01进行等效检验时，我们可以得出什么结论?使用TOSTER函数TOSTmeta，并输入alpha、效应大小(ES)、标准误差(se)和等效边界。

```{r eval = FALSE}
TOSTER::TOSTmeta(
  ES = 0.00,
  se = 0.000,
  low_eqbound_d = -0.0,
  high_eqbound_d = 0.0,
  alpha = 0.05
)
```

A) We can **reject** an effect size of zero, and we can **reject** the presence of effects as large or larger than the smallest effect size of interest.
B) We can **not reject** an effect size of zero, and we can **reject** the presence of effects as large or larger than the smallest effect size of interest.
C) We can **reject** an effect size of zero, and we can **not reject** the presence of effects as large or larger than the smallest effect size of interest.
D) We can **not reject** an effect size of zero, and we can **not reject** the presence of effects as large or larger than the smallest effect size of interest.
A）我们可以**拒绝**效应值为零，也可以**拒绝**效应值大于或大于最小效应值的存在。
B）我们不能**拒绝**效应值为零，我们可以**拒绝**效应值大于或大于最小效应值的存在。
C）我们可以**拒绝**效应值为零，也可以**不拒绝**效应值大于或大于最小效应值的存在。
D）我们不能**拒绝**效应值为零，也不能**拒绝**效应值大于或大于最小效应值的存在。
### Questions about the small telescopes approach

**Q13**: What is the smallest effect size of interest based on the small telescopes approach, when the original study collected 20 participants in each condition of an independent *t*-test, with an **alpha level of 0.05**. Note that for this answer, it happens to depend on whether you enter the power as 0.33 or 1/3 (or 0.333). You can use the code below, which relies on the `pwr` package. 
**问题13**：当原始研究在每个条件下收集20名参与者进行独立样本**t**检验，**α=0.05时**，基于小型望远镜方法的最小效应量大小是多少?请注意，答案将取决于你输入的效力是0.33还是1/3(或0.333)。你可以使用下面的代码，它依赖于' pwr '包。
```{r}
pwr::pwr.t.test(
  n = 20,
  sig.level = 0.05,
  power = 1/3,
  type = "two.sample",
  alternative = "two.sided"
)
```

```{r, eval = FALSE}
pwr::pwr.t.test(
  n = 0, 
  sig.level = 0.00, 
  power = 0, 
  type = "two.sample",
  alternative = "two.sided"
)
```

A) *d* =0.25 (setting power to 0.33) or 0.26 (setting power to 1/3)
B) *d* =0.33 (setting power to 0.33) or 0.34 (setting power to 1/3)
C) *d* =0.49 (setting power to 0.33) or 0.50 (setting power to 1/3)
D) *d* =0.71 (setting power to 0.33) or 0.72 (setting power to 1/3)
A）*d* =0.25（将效力设为0.33）或0.26（将效力设为1/3）
B）*d* =0.33（将效力设为0.33）或0.34（将效力设为1/3）
C）*d* =0.49（将效力设为0.33）或0.50（将效力设为1/3）
D）*d* =0.71（将效力设为0.33）或0.72（将效力设为1/3）
**Q14**: Let’s assume you are trying to replicate a previous result based on a correlation in a two-sided test. The study had 150 participants. Calculate the SESOI using a small telescopes justification for a replication of this study that will use an alpha level of 0.05. Note that for this answer, it happens to depend on whether you enter the power as 0.33 or 1/3 (or 0.333). You can use the code below.

**问题14**：假设你正在尝试基于双尾检验中的相关分析来复现先前的结果。这项研究有150名被试。使用小型望远镜计算SESOI，并使用0.05的alpha水平。请注意，答案将取决于你输入的效力是0.33还是1/3(或0.333)。你可以使用下面的代码。

```{r, eval = FALSE}
pwr::pwr.r.test(
  n = 150,
  sig.level = 0.05,
  power = 1/3,
  alternative = "two.sided")
```

```{r, eval = FALSE}
pwr::pwr.r.test(
  n = 0, 
  sig.level = 0, 
  power = 0, 
  alternative = "two.sided")
```

A) *r* = 0.124 (setting power to 0.33) or 0.125 (setting power to 1/3)
B) *r* = 0.224 (setting power to 0.33) or 0.225 (setting power to 1/3)
C) *r* = 0.226 (setting power to 0.33) or 0.227 (setting power to 1/3)
D) *r* = 0.402 (setting power to 0.33) or 0.403 (setting power to 1/3)
A）*r* =0.124（将效力设为0.33）或0.125（将效力设为1/3）
B）*r* =0.224（将效力设为0.33）或0.225（将效力设为1/3）
C）*r* =0.226（将效力设为0.33）或0.227（将效力设为1/3）
D）*r* =0.402（将效力设为0.33）或0.403（将效力设为1/3）
**Q15**: In the age of big data researchers often have access to large databases, and can run correlations on samples of thousands of observations. Let’s assume the original study in the previous question did not have 150 observations, but 15000 observations. We still use an alpha level of 0.05. Note that for this answer, it happens to depend on whether you enter the power as 0.33 or 1/3 (or 0.333). What is the SESOI based on the small telescopes approach?
**问题15**：在大数据时代，研究人员通常可以访问大型数据库，并可以对数千个样本进行相关性分析。假设上一个问题中的原始研究不是150个样本，而是15000个样本。我们仍然使用0.05的alpha水平。请注意，对于这个答案，答案将取决于你输入的效力是0.33还是1/3(或0.333)。基于小型望远镜方法的SESOI是多少?

A）*r* =0.124（将效力设为0.33）或0.125（将效力设为1/3）
B）*r* =0.224（将效力设为0.33）或0.225（将效力设为1/3）
C）*r* =0.226（将效力设为0.33）或0.227（将效力设为1/3）
D）*r* =0.402（将效力设为0.33）或0.403（将效力设为1/3）
A) *r* = 0.0124 (setting power to 0.33) or 0.0125 (setting power to 1/3)
B) *r* = 0.0224 (setting power to 0.33) or 0.0225 (setting power to 1/3)
C) *r* = 0.0226 (setting power to 0.33) or 0.0227 (setting power to 1/3)
D) *r* = 0.0402 (setting power to 0.33) or 0.0403 (setting power to 1/3)

Is this effect likely to be practically or theoretically significant? Probably not. This would be a situation where the small telescopes approach is not a very useful procedure to determine a smallest effect size of interest.
这种影响可能在实践上或理论上是显著的吗?可能不会。在这种情况下，小型望远镜并不是一个非常有用的方法来确定最小效应的大小。
**Q16**: Using the small telescopes approach, you set the SESOI in a replication study to *d* = 0.35, and set the alpha level to 0.05. After collecting the data in a well-powered replication study that was as close to the original study as practically possible, you find no significant effect, and you can reject effects as large or larger than *d* = 0.35. What is the correct interpretation of this result?
**问题16**：使用小型望远镜方法，并将复制研究中的SESOI设置为*d*=0.35，将alpha水平设置为0.05。在尽可能接近原始研究的有力复现研究中收集数据后，你发现没有显著的效应，并且你可以拒绝大于或大于*d* = 0.35的效应。这个结果的正确解释是什么?

A）没有效应存在。
B）我们可以在统计上拒绝(使用0.05的alpha值)任何理论上有意义的效应。
C）我们可以在统计上拒绝(使用0.05的alpha值)任何实际上有意义的效应。
D）我们可以在统计上拒绝(使用0.05的alpha值)原始研究中有33%的效力检测到的效应。
A) There is no effect.
B) We can statistically reject (using an alpha of 0.05) effects anyone would find theoretically meaningful.
C) We can statistically reject (using an alpha of 0.05) effects anyone would find practically relevant.
D) We can statistically reject (using an alpha of 0.05) effects the original study had 33% power to detect.

### Questions about specifying the SESOI as the Minimal Statistically Detectable Effect
###关于将SESOI指定为最小统计可检测效应的问题

**Q17**: Open the online Shiny app that can be used to compute the minimal statistically detectable effect for two independent groups: https://shiny.ieis.tue.nl/d_p_power/. Three sliders influence what the figure looks like: The sample size per condition, the true effect size, and the alpha level. Which statement is true?
**问题17**：打开在线的Shiny应用程序，它可以用来计算两个独立总体的最小统计可检测效应:https://shiny.ieis.tue.nl/d_p_power/。 三个滑块影响图形的外观:每个条件的样本量、真实效应大小和alpha水平。下列哪个说法是正确的?

A）临界*d*值受每组样本量，即真实效应大小的影响，但**不**受α水平的影响。
B）临界*d*值受每组样本量，即α水平的影响，但**不**受真实效应大小的影响。
C）临界*d*值受α水平，即真实效应大小的影响，但**不**受样本量的影响。
D）临界*d*值受每组样本量，即α水平的影响，且受真实效应大小的影响。
A) The critical *d*-value is influenced by the sample size per group, the true effect size, but **not** by the alpha level.
B) The critical *d*-value is influenced by the sample size per group, the alpha level, but **not** by the true effect size.
C) The critical *d*-value is influenced by the alpha level, the true effect size, but **not** by the sample size per group.
D) The critical *d*-value is influenced by the sample size per group, the alpha level, and by the true effect size.


**Q18**: Imagine researchers performed a study with 18 participants in each condition, and performed a *t*-test using an alpha level of 0.01. Using the Shiny app, what is the smallest effect size that could have been statistically significant in this study?
**问题18**：假设研究人员对每种情况下的18名参与者进行了一项研究，并使用0.01的α水平进行了*t*检验。使用Shiny应用程序，在这项研究中可能具有统计意义的最小效应大小是多少?

A) *d* = 0.47
B) *d* = 0.56
C) *d* = 0.91
D) *d* = 1

**Q19**: You expect the true effect size in your next study to be *d* = 0.5, and you plan to use an alpha level of 0.05. You collect 30 participants in each group for an independent *t*-test. Which statement is true?
**问题19**：你预期你的下一个研究中真实效应大小为*d*=0.5，并且你计划使用0.05的alpha水平。每组收集30名被试进行独立的*t*检验。下列哪个说法是正确的?

A）所有可能效应量的统计效力都很低。
B）对于你目标效应大小，你有足够的统计效力（大于80%）。
C）观察到的效应量*d* = 0.5永远不会有统计学意义。
D）观察到的效应量*d* = 0.5具有统计学意义。
A) You have low power for all possible effect sizes.
B) You have sufficient (i.e., \> 80%) power for all effect sizes you are interested in.
C) Observed effect sizes of *d* = 0.5 will never be statistically significant.
D) Observed effect sizes of *d* = 0.5 will be statistically significant.

The example we have used so far was based on performing an independent *t*-test, but the idea can be generalized. A shiny app for an *F*-test is available here: <https://shiny.ieis.tue.nl/f_p_power/>. The effect size associated to the power of an *F*-test is partial eta squared ($\eta_{p}^{2})$, which for a One-Way ANOVA (visualized in the Shiny app) equals eta-squared.
到目前为止，我们使用的例子是基于执行独立的*t*检验，但这个想法可以推广。这里有一个用于*F*测试的shiny应用程序:<https://shiny.ieis.tue.nl/f_p_power/>。与*F*检验的效力相关的效应大小是偏eta平方($\eta_{p}^{2})$，对于单因素方差分析(在Shiny应用程序中可视化)是eta平方。

The distribution for eta-squared looks slightly different from the distribution of Cohen’s *d*, primarily because an *F*-test is a one-directional test (and because of this, eta-squared values are all positive, while Cohen’s *d* can be positive or negative). The light grey line plots the expected distribution of eta-squared when the null is true, with the red area under the curve indicating Type 1 errors, and the black line plots the expected distribution of eta-squared when the true effect size is η = 0.059. The blue area indicates the expected effect sizes smaller that the critical η of 0.04, which will not be statistically significant, and thus will be Type 2 errors.
偏eta平方的分布看起来与科恩的*d*的分布略有不同，主要是因为*F*检验是单向检验(正因为如此，平方的值都是正的，而科恩的*d*可以是正的或负的)。浅灰线表示零值为真时的期望分布，曲线下的红色区域表示一类误差，黑线表示真效应大小η=0.059时的期望分布。蓝色区域表示预期效应值小于临界η值0.04，不具有统计学意义，因此属于二类误差。


(ref:critflab) Illustration of the criticial *F*-value for two groups, 50 observations per group, and an alpha level of 0.05.
(参考:critflab)两组的临界*F*值示意图，每组50个样本，α水平为0.05。
```{r critf, echo = FALSE, fig.cap="(ref:critflab)"}
knitr::include_graphics("~/Desktop/statistical_inferences-master/statistical_inferences-master/images/7f6d17dc07bdc9e95ea8944d78b16d7c.png")
```

**Q20**: Set the number of participants (per condition) to 14, and the number of groups to 3. Using the Shiny app at <https://shiny.ieis.tue.nl/f_p_power/> which effect sizes (expressed in partial eta-squared, as indicated on the vertical axis) can be statistically significant with n = 14 per group, and 3 groups?
**问题20**：将参与者的数量(每个条件)设置为14，组的数量设置为3。使用Shiny应用程序<https://shiny.ieis.tue.nl/f_p_power/>看哪些效应量(以偏eta平方表示，如纵轴所示)在每组和三组n = 14时具有统计学显著性？
A) Only effects larger than 0.11
B) Only effects larger than 0.13
C) Only effects larger than 0.14
D) Only effects larger than 0.16

A）仅当效应量大于0.11
B）仅当效应量大于0.13
C）仅当效应量大于0.14
D）仅当效应量大于0.16
Every sample size and alpha level implies a minimal statistically detectable effect that can be statistically significant in your study. Looking at which observed effects you can detect is a useful way to make sure you could actually detect the smallest effect size you are interested in.
每个样本量和α水平都意味着在你的研究中具有统计显著性的最小统计可检测效应。查看你可以检测到哪些观察到的效应是一种有用的方法，可以确保你实际上可以检测到您最小目标效应大小。



**Q21**: Using the minimal statistically detectable effect, you set the SESOI in a replication study to *d* = 0.35, and set the alpha level to 0.05. After collecting the data in a well-powered replication study that was as close to the original study as practically possible, you find no significant effect, and you can reject effects as large or larger than *d* = 0.35. What is the correct interpretation of this result?
**问题21**：使用最小可检测统计效应，将复现研究中的SESOI设置为*d*=0.35，并将alpha水平设置为0.05。在尽可能接近原始研究的有力复现研究中收集数据后，你发现没有显著的效应，并且你可以拒绝大于或大于*d* = 0.35的影响。这个结果的正确解释是什么?

A）没有效应存在。
B）我们可以在统计上拒绝(使用0.05的alpha值)任何理论上有意义的效应。
C）我们可以在统计上拒绝(使用0.05的alpha值)任何实际上有意义的效应。
D）我们可以在统计上拒绝(使用0.05的alpha值)原始研究中有33%的效力检测到的效应。
<!-- <!-- A) There is no effect. --> -->
<!-- <!-- B) We can statistically reject (using an alpha of 0.05) effects anyone would find theoretically meaningful. --> -->
<!-- <!-- C) We can statistically reject (using an alpha of 0.05) effects anyone would find practically relevant. --> -->
<!-- <!-- D) We can statistically reject (using an alpha of 0.05) effects that could have been statistically significant in the original study. --> -->

<!-- <!-- ### Open Questions --> -->
<!-- ### 开放性问题 -->

1. What is meant with the statement ‘Absence of evidence is not evidence of absence’?
1. “没有发现证据不等于证据不存在”这句话是什么意思?
2. What is the goal of an equivalence test?
2. 等效检验的目的是什么？
3. What is the difference between a nil null hypothesis and a non-nil null hypothesis?
3. 效应量为0的虚无假设和效应量非0的虚无假设的区别是什么?
4. What is a minimal effect test?
4. 最小效应检验是什么？
5. What conclusion can we draw if a null-hypothesis significance test and equivalence test
are performed for the same data, and neither test is statistically significant?
5. 如果对同一批数据进行零假设显著性检验和等价检验，并且都不是显著，可以得出什么结论?
6. When designing equivalence tests to have a desired statistical power, why do you need a larger sample size, the narrower the equivalence range is?
6. 当设计等效检验以获得期望的统计效力时，为什么需要更大的样本量，等效范围越窄?

7. Why is it incorrect to say there is ‘no effect’ when the equivalence test is statistically significant?
7. 当等效检验显著时，为什么不能说不存在效应？
8. Specify one way in which the Bayesian ROPE procedure and an equivalence test are similar, and specify one way in which they are different.
8. 设计一种方法使得贝叶斯ROPE程序和等效检验相同，并设计另一种方法使二者不同。
9. What are two approaches to specify a smallest effect size of interest?
9. 有哪两种方法可以使得目标效应量最小？
10. What is the idea behind the ‘small telescopes’ approach to equivalence testing?
10. 等效检验中“小望远镜”背后的思想是什么？